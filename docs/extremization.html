<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Extremization | 6.7480 Notes</title>
  <meta name="description" content="6 Extremization | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Extremization | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Extremization | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-10-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="variational-measures-of-information.html"/>
<link rel="next" href="tensorization.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#todo-list"><i class="fa fa-check"></i>Todo list</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics and geometry</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="divergence.html"><a href="divergence.html"><i class="fa fa-check"></i><b>3</b> Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#kl-divergence"><i class="fa fa-check"></i>KL-Divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#sufficient-statistic"><i class="fa fa-check"></i>Sufficient statistic</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#entropy-power-inequality"><i class="fa fa-check"></i>Entropy-power Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html"><i class="fa fa-check"></i><b>5</b> Variational Measures of Information</a>
<ul>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#geometric-interpretations-of-mi"><i class="fa fa-check"></i>Geometric interpretations of MI</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#gaussian-saddle-point"><i class="fa fa-check"></i>Gaussian saddle point</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#empirical-distribution-and-χ²"><i class="fa fa-check"></i>Empirical distribution and χ²</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-compression.html"><a href="data-compression.html"><i class="fa fa-check"></i>Data compression</a></li>
<li class="chapter" data-level="9" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>9</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="extremization" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Extremization<a href="extremization.html#extremization" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Key perspectives:</p>
<ol style="list-style-type: decimal">
<li><em>Capacity:</em> Given channel <span class="math inline">\(P_{Y|X}\)</span>, maximize <span class="math inline">\(I(X; Y)\)</span>
over a convex set of inputs <span class="math inline">\(P_X\)</span>.</li>
<li><em>Rate-distortion:</em> Given <span class="math inline">\(P_X\)</span>, minimize <span class="math inline">\(I(X; Y)\)</span>
over a convex set of <span class="math inline">\(P_{Y|X}\)</span>.</li>
<li><em>Maximum likelihood:</em> Given <span class="math inline">\(P\)</span>, minimize <span class="math inline">\(D(P\|Q)\)</span>
over a class of <span class="math inline">\(Q\)</span>.</li>
<li><em>Information projection:</em> Given <span class="math inline">\(Q\)</span>, minimize <span class="math inline">\(D(P\|Q)\)</span>
over a convex class of <span class="math inline">\(P\)</span>.</li>
</ol>
<p>The first two minimax objectives correspond to the
concavity of <span class="math inline">\(I(P_X; P_{Y|X})\)</span> in the first argument and
convexity in the second argument. The last two objectives
correspond to the convexity of <span class="math inline">\(D(P\|Q)\)</span> in both arguments.</p>
<p>Key mathematical ideas:</p>
<ol style="list-style-type: decimal">
<li>Convexity of <span class="math inline">\(D(P\|Q)\)</span> is equivalent to “conditioning increases
divergence” (proposition <a href="divergence.html#prp:conditionIncDiv">3.4</a>)
<span class="math inline">\(\iff\)</span> “mixing decreases divergence.”</li>
<li>For convexity proofs, introduce a Bernoulli latent variable,
then use chain rule to decompose the quantity (MI, KL, entropy, etc)
into the two compared components.</li>
<li>The saddle point of mutual information yields a game-theoretic
perspective to the duality between achieving capacity and
rate distortion: corollary <a href="extremization.html#cor:MIMinimax">6.2</a>.</li>
<li>The capacity of a channel is the radius of posteriors under
divergence.</li>
</ol>
<div id="extremizationConvexity" class="section level2 unnumbered hasAnchor">
<h2>Convexity<a href="extremization.html#extremizationConvexity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-42" class="theorem"><strong>Theorem 6.1  (KL convexity) </strong></span>The map <span class="math inline">\((P, Q)\mapsto D(P\|Q)\)</span> is convex:
<span class="math display">\[
    \forall \lambda \in [0, 1]:
    \lambda D(P_0 \| Q_0) + \bar \lambda D(P_1\|Q_1)
    \geq D(\lambda P_0 + \bar \lambda P_1 \|
    \lambda Q_0 + \bar \lambda Q_1)
\]</span></p>
</div>
<p><em>Proof:</em> The first quantity can be seen as an expected divergence
over <span class="math inline">\(R\sim \mathrm{Ber}(\lambda)\)</span>:
<span class="math display">\[
    \lambda D(P_0 \| Q_0) + \bar \lambda D(P_1\|Q_1)
    = D(P\|Q | R), \quad (P, Q)_{R=j} = (P_j, Q_j)
\]</span>
The second quantity is simply the divergence of the marginal,
then by chain rule we have
<span class="math display">\[\begin{align}
    D(P\|Q | R)
    = D(PR \|QR) \geq D(P\|Q)
\end{align}\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-43" class="theorem"><strong>Theorem 6.2  (concavity of entropy) </strong></span><span class="math inline">\(P_X\mapsto H(X)\)</span> is concave. Fixing channel
<span class="math inline">\(P_{Y|X}\)</span>, <span class="math inline">\(P_X\mapsto H(X|Y)\)</span> is concave
and continuous if <span class="math inline">\(\mathcal X\)</span> finite.</p>
</div>
<p><em>Proof:</em> The first proof is complete by the
KL-entropy relation <span class="math inline">\(H(X) = \log |\mathcal X| - D(P_X\|U_X)\)</span>.
The second proof follows by a similarl latent
variable argument: consider <span class="math inline">\(U\sim \mathrm{Ber}(\lambda)\)</span> and
<span class="math inline">\(U\to X\to Y\)</span>; let <span class="math inline">\(f(P_X) = H(X|Y)\)</span>, then
<span class="math display">\[\begin{align}
    f(\lambda P_0 + \bar \lambda P_1) = H(X|Y), \quad
    \lambda f(P_0) + \bar \lambda f(P_1) = H(X|Y, U)
\end{align}\]</span>
concavity follows from <span class="math inline">\(H(X|Y) \leq H(X|Y, U)\)</span>. Continuity
follows <span class="math inline">\(H(Y|X) = H(Y) - I(X; Y)\)</span> both continuous.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-44" class="theorem"><strong>Theorem 6.3  (extremality of MI) </strong></span></p>
<ol style="list-style-type: decimal">
<li>Fixing channel <span class="math inline">\(P_{Y|X}\)</span>, <span class="math inline">\(P_X\mapsto I(P_X, P_{Y|X})\)</span> is concave.</li>
<li>Fixing input distribution <span class="math inline">\(P_X\)</span>, <span class="math inline">\(P_{Y|X} \mapsto I(P_X, P_{Y|X})\)</span>
is convex.</li>
</ol>
</div>
<p><em>Proof</em> Consider 3 proofs for the first statement:
<span class="math display">\[
    \lambda I(P^0_X; P_{Y|X} \circ P^0_X) +
    \bar \lambda I(P^1_X; P_{Y|X} \circ P^1_X)
    \geq
    I(P_X; P_{Y|X}\circ P_X), \quad
    P_X = \lambda P^0_X + \bar \lambda P^1_X
\]</span></p>
<ol style="list-style-type: decimal">
<li><em>Standard latent variable proof:</em> Consider the standard
latent variable proof <span class="math inline">\(Z\to X\to Y\)</span> with
<span class="math inline">\(Z\sim \mathrm{Ber}(\lambda)\perp\!\!\!\perp Y\)</span>,
then the RHS is <span class="math inline">\(I(X; Y)\)</span> while the LHS is
<span class="math display">\[
     I(Y; X, Z) = I(Y; X | Z) + I(Y; Z)_{=0}
\]</span>
the result follows from the fact that mutual information increases
with more deta.</li>
<li><em>Center of gravity formula:</em>
Use corollary <a href="variational-measures-of-information.html#cor:centerOfGravityMI">5.1</a>:
<span class="math inline">\(I(X; Y) = \min_{Q_Y} D(P_{Y|X} \|Q_Y | P_X)\)</span>; this is
the pointwise minimum of affine functions in <span class="math inline">\(P_X\)</span>
hence concave.</li>
<li><em>Golden formula:</em> Since
<span class="math inline">\(I(X; Y) = D(P_{Y|X} \|Q_Y|P_X) - D(P_Y\|Q_Y)\)</span>,
the map <span class="math inline">\(P_X\mapsto D(P_{Y|X} \circ P_X \|Q_Y)\)</span>
is convex and the first term is affine, so the
combination is concave.</li>
</ol>
<p>To prove the second argument, note that
<span class="math inline">\(I(X; Y) = D(P_{Y|X} \| P_Y|P_X)\)</span>; here <span class="math inline">\(D(P_{Y|X=x} \| P_Y)\)</span>
is jointly convex, and <span class="math inline">\(P_Y\)</span> is linear function of <span class="math inline">\(P_{Y|X}\)</span>.</p>
</div>
<div id="minimax-and-saddle-point" class="section level2 unnumbered hasAnchor">
<h2>Minimax and saddle-point<a href="extremization.html#minimax-and-saddle-point" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="proposition">
<p><span id="prp:unlabeled-div-45" class="proposition"><strong>Proposition 6.1  (minimax inequality) </strong></span><span class="math display">\[
    \inf_y \sup_x f(x, y) \geq \sup_x \inf_y f(x, y)
\]</span>
Whichever operation acts first strictly dominates.</p>
</div>
<p><em>Proof:</em> Fixing <span class="math inline">\(y=y_0\)</span> on the LHS,
<span class="math inline">\(\sup_x f(x, y_0) \geq \sup_x \inf_y f(x, y)\)</span> by
<span class="math inline">\(f(x, y_0)\geq \inf_y f(x, y)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Minimax equality is implied by the existence of a saddle
point <span class="math inline">\((x^*, y^*)\)</span> such that
<span class="math display">\[
     f(x, y^*) \leq f(x^*, y^*) \leq f(x^*, y), \quad \forall x, y
\]</span>
here <span class="math inline">\(x^*\)</span> is the dominant strategy given <span class="math inline">\(y^*\)</span>, and
<span class="math inline">\(y^*\)</span> is the dominant strategy given <span class="math inline">\(x^*\)</span>.</li>
<li>If <span class="math inline">\(\inf, \sup\mapsto \min, \max\)</span>, then equality implies
the existence of a saddle point.</li>
<li><em>von Neumann</em>: Given <span class="math inline">\(A, B\)</span> with finite alphabets and <span class="math inline">\(g(A, B)\)</span>
arbitrary, then
<span class="math display">\[
     \min_{P_A} \max_{P_B} \mathbb E[g(A, B)]
     = \max_{P_B} \min_{P_A} \mathbb E[g(A, B)]
\]</span>
this is a special case of minimax with
<span class="math inline">\(f(x, y) = \sum_{ab} P_A(a)P_B(b) g(a, b)\)</span>.</li>
</ol>
<div class="theorem">
<p><span id="thm:minimaxEquality" class="theorem"><strong>Theorem 6.4  (minimax theorem) </strong></span>If <span class="math inline">\(\mathcal X, \mathcal Y\)</span> are compact domains in <span class="math inline">\(\mathbb R^n\)</span>, and
<span class="math inline">\(f(x, y)\)</span> is continuous in <span class="math inline">\((x, y)\)</span>, concave in <span class="math inline">\(x\)</span> and
convex in <span class="math inline">\(y\)</span>, then
<span class="math display">\[
    \max_x \min_y f(x, y) = \min_y \max_x f(x, y)
\]</span>
In particular, this implies the existence of a saddle point.</p>
</div>
</div>
<div id="capacity-saddle-point-of-mi" class="section level2 unnumbered hasAnchor">
<h2>Capacity, Saddle point of MI<a href="extremization.html#capacity-saddle-point-of-mi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-46" class="definition"><strong>Definition 6.1  (capacity, caid, caod) </strong></span>The capacity of a channel <span class="math inline">\(P_{Y|X}\)</span> over a set <span class="math inline">\(\mathcal P\)</span>
of (usually convex) input distributions is
<span class="math display">\[
    C = \sup_{P_X\in \mathcal P} I(P_X, P_{Y|X})
    = \sup_{P_X} D(P_{Y|X}P_X \| (P_{Y|X}\circ P_X)P_X)
\]</span>
If equality is saturated by <span class="math inline">\(P_X^*\in\mathcal P\)</span>, then
<span class="math inline">\(P_X^*\)</span> is a capacity-achieving input distribution
(caid) and <span class="math inline">\(P_Y^* = P_{Y|X}\circ P_X^*\)</span> is the capacity-achieving
output distribution (caod).</p>
</div>
<div class="theorem">
<p><span id="thm:MISaddleInequality" class="theorem"><strong>Theorem 6.5  (implications of MI saddle point) </strong></span>Fixing a convex <span class="math inline">\(\mathcal P\)</span> on <span class="math inline">\(\mathcal X\)</span>. The existence of a caid
implies, for every <span class="math inline">\(P_X\in \mathcal P, P_Y\in P_{Y|X} \circ \mathcal P\)</span>,
<span class="math display">\[
    D(P_{Y|X} \| P_Y^* |P_X) \leq D(P_{Y|X} \| P_Y^* | P_X^*)
    \leq D(P_{Y|X} \| Q_Y | P_X^*)
\]</span></p>
</div>
<p><em>Proof:</em> The second inequality is simply center of gravity
formula <a href="variational-measures-of-information.html#cor:centerOfGravityMI">5.1</a> applied to the
middle quantity <span class="math inline">\(C=I(X^*, Y^*) = D(P_{Y|X} \| P_Y^* | P_X^*)\)</span>.
For the first inequality, let <span class="math inline">\(C&lt;\infty\)</span> and let
<span class="math display">\[
    P_{X_\lambda} = \lambda P_X + \bar \lambda P_X^*, \quad
    P_{Y_\lambda} = P_{Y|X} \circ P_{X_\lambda} \implies
    P_{Y_\lambda} = \lambda P_Y + \bar \lambda P_Y^*
\]</span>
The following chain yields (the third line applies the
center of gravity characterization again)
<span class="math display">\[\begin{align}
    C
    &amp;\geq I(X_\lambda; Y_\lambda)
    = D(P_{Y|X} \| P_{Y_\lambda} | P_{X_\lambda}) \\
    &amp;= \lambda D(P_{Y|X} \| P_{Y_\lambda} | P_X) +
    \bar \lambda D(P_{Y|X} \| P_{Y_\lambda} | P_{X^*}) \\
    &amp;\geq \lambda D(P_{Y|X} \| P_{Y_\lambda} | P_X) + \bar \lambda C \\
    C &amp;\geq D(P_{Y|X} \| P_{Y_\lambda} | P_X)
    = D(P_{Y|X} P_X \| P_{Y_\lambda} P_X)
\end{align}\]</span>
Apply lower semi-continuity <a href="variational-measures-of-information.html#thm:semiContinuity">5.6</a>
by taking <span class="math inline">\(\liminf_{\lambda\to 0}\)</span> to obtain the desired quantity
<span class="math display">\[
    \liminf_{\lambda\to 0} P_{Y_\lambda} P_X = P_{Y^*}P_X \implies
    D(P_{Y|X} \| P_{Y^*} | P_X) \leq
    \liminf_{\lambda\to 0} D(P_{Y|X} P_X \| P_{Y_\lambda} P_X)  \leq C
\]</span></p>
<div class="corollary">
<p><span id="cor:caodUniqueness" class="corollary"><strong>Corollary 6.1  (uniqueness of caod) </strong></span>In addition to the assumptions in theorem <a href="extremization.html#thm:MISaddleInequality">6.5</a>,
if capacity is finite, then the caod <span class="math inline">\(P_Y^*\)</span> is unique and satisfies
<span class="math display">\[
    D(P_{Y|X} \circ P_X \| P_Y^*) \leq C &lt; \infty,
    \quad \forall P_X\in \mathcal P
\]</span>
In particular, KL-finite implies <span class="math inline">\(P_Y \ll P_Y^*\)</span>.</p>
</div>
<p><em>Proof:</em> Recognize the capacity as a decomposed component
in the divergence chain rule <a href="divergence.html#prp:divergenceChainRule">3.2</a>:
<span class="math display">\[\begin{align}
    C = D(P_{X^*Y^*} \| P_{Y^*}P_{X^*})
    &amp;= D(P_{X^*Y^*} \| P_{Y^*} P_X) - D(P_{Y^*} \| P_Y) \\
    &amp;= D(P_{X|Y} \| P_X | P_{Y^*}) - D(P_{Y^*} \| P_Y) \\
    &amp;\geq D(P_{X|Y} \| P_{X^*} | P_{Y^*})_{=C} - D(P_{Y^*} \| P_Y)
\end{align}\]</span>
Saturated equality implies <span class="math inline">\(D(P_{Y^*} \| P_Y)=0\iff P_Y=P_{Y^*}\)</span>.</p>
<p>Note that the caid need not be unique.</p>
<div class="corollary">
<p><span id="cor:MIMinimax" class="corollary"><strong>Corollary 6.2  (minimax MI) </strong></span>Under saddle point assumptions, we additionally have
<span class="math display">\[\begin{align}
    C = \max_{P_X\in \mathcal P} I(X; Y)
    &amp;= \max_{P_X\in \mathcal P} \min_{Q_Y} D(P_{Y|X} \| Q_Y | P_X) \\
    &amp;= \min_{Q_Y} \sup_{P_X\in \mathcal P} D(P_{Y|X} \| Q_Y|P_X)
\end{align}\]</span></p>
</div>
<p><em>Proof:</em> The first <span class="math inline">\(\max\min\)</span> equation comes from the
center of gravity characterization <a href="variational-measures-of-information.html#cor:centerOfGravityMI">5.1</a>.
The second equation comes from applying center of gravity
to the left inequality of theorem <a href="extremization.html#thm:MISaddleInequality">6.5</a>
<span class="math display">\[
    C = D(P_{Y|X} \| P_Y^* | P_X^*)
    = \max_{P_X\in \mathcal P} D(P_{Y|X} \| P_Y^* | P_X)
    = \min_{Q_Y} \sup_{P_X\in \mathcal P} D(P_{Y|X} \| P_Y^* | P_X)
\]</span></p>
</div>
<div id="capacity-as-information-radius" class="section level2 unnumbered hasAnchor">
<h2>Capacity as information radius<a href="extremization.html#capacity-as-information-radius" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some review: given metric space <span class="math inline">\((X, d)\)</span> and bounded set <span class="math inline">\(A\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-47" class="definition"><strong>Definition 6.2  (radius, diameter) </strong></span>The (Chebyshev) radius is the radius of the smallest ball
that covers <span class="math inline">\(A\)</span>.
<span class="math display">\[
    \mathrm{rad}(A) = \inf_{y\in X}\sup_{x\in A} d(x, y)
\]</span>
The diameter of <span class="math inline">\(A\)</span> is the least upper bound on
two distances in the set
<span class="math display">\[
    \mathrm{diam}(A) = \sup_{x, y\in A} d(x, y)
\]</span></p>
</div>
<p><span style="color:green">
Compare the radius to the second equation in corollary
<a href="extremization.html#cor:MIMinimax">6.2</a>: channel capacity is the information
radius of the conditionals.
</span></p>
<div class="corollary">
<p><span id="cor:unlabeled-div-48" class="corollary"><strong>Corollary 6.3  </strong></span>Given finite <span class="math inline">\(\mathcal X\)</span> and channel <span class="math inline">\(P_{Y|X}\)</span>, the
maximal mutual information over all input distributions
satisfies
<span class="math display">\[
    \max_{P_X} I(P_X; P_{Y|X})
    = \max_{x\in \mathcal X} D(P_{Y|x=x} \| P_{Y^*})
\]</span></p>
</div>
<p><em>Proof:</em> This follows from the left equality
<span class="math inline">\(C=\max_{P_X\in \mathcal P} D(P_{Y|X} \| P_Y^* | P_X)\)</span>: for finite
alphabets, concentrate weight in the single <span class="math inline">\(x\in \mathcal X\)</span>
which maximizes <span class="math inline">\(D(P_{Y|X=x} \| P_Y^*)\)</span>.</p>
</div>
<div id="gaussian-saddle-point" class="section level2 unnumbered hasAnchor">
<h2>Gaussian saddle point<a href="extremization.html#gaussian-saddle-point" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-49" class="theorem"><strong>Theorem 6.6  (extremality of Gaussian channels) </strong></span>Let <span class="math inline">\(X_g\sim \mathcal N(0, \sigma_X^2), N_g\sim \mathcal N(0, \sigma_N^2)\perp\!\!\!\perp X_g\)</span>, then</p>
<ol style="list-style-type: decimal">
<li><em>Gaussian capacity:</em>
<span class="math display">\[
C = I(X_g; X_g + N_g) = \dfrac 1 2 \log \left(
     1 + \dfrac{\sigma_X^2}{\sigma_N^2}
\right)
\]</span></li>
<li><em>Gaussian input is the caid for Gaussian noise:</em> under
the power constraint <span class="math inline">\(\mathrm{Var}(X) \leq \sigma_X^2\)</span> and <span class="math inline">\(X\perp\!\!\!\perp N_g\)</span>
<span class="math display">\[
I(X; X+N_g) \leq I(X_g; X_g+N_g)
\]</span>
with equality saturated iff <span class="math inline">\(X=X_g\)</span> (in distribution).</li>
<li><em>Gaussian noise is the worst for Gaussian input:</em> under
the power constraint <span class="math inline">\(\mathbb E[N^2] \leq \sigma_N^2\)</span> and <span class="math inline">\(\mathbb E[X_gN] = 0\)</span>
<span class="math display">\[
I(X_g; X_g+N) \geq I(X_g; X_g+N_g)
\]</span>
with equality iff <span class="math inline">\(N=N_g\)</span> (in distribution and <span class="math inline">\(N\perp\!\!\!\perp X_g\)</span>.</li>
</ol>
</div>
<p><em>Proof:</em> Placeholder.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="variational-measures-of-information.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tensorization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
