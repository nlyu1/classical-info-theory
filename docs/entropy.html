<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Entropy | 6.7480 Notes</title>
  <meta name="description" content="1 Entropy | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Entropy | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Entropy | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-10-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="entropy-method-in-combinatorics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#todo-list"><i class="fa fa-check"></i>Todo list</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#entropy-power-inequality"><i class="fa fa-check"></i>Entropy-power Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#source-coding-theorems"><i class="fa fa-check"></i>Source coding theorems</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="fixed-length-compression.html"><a href="fixed-length-compression.html"><i class="fa fa-check"></i><b>11</b> Fixed-length compression</a>
<ul>
<li class="chapter" data-level="" data-path="fixed-length-compression.html"><a href="fixed-length-compression.html#source-coding-theorems-1"><i class="fa fa-check"></i>Source coding theorems</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="entropy-of-ergodic-processes.html"><a href="entropy-of-ergodic-processes.html"><i class="fa fa-check"></i><b>12</b> Entropy of Ergodic Processes</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-of-ergodic-processes.html"><a href="entropy-of-ergodic-processes.html#stochasticProcessPrelim"><i class="fa fa-check"></i>Preliminaries</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="universal-compression.html"><a href="universal-compression.html"><i class="fa fa-check"></i><b>13</b> Universal compression</a>
<ul>
<li class="chapter" data-level="" data-path="universal-compression.html"><a href="universal-compression.html#arithmetic-encoding"><i class="fa fa-check"></i>Arithmetic encoding</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>14</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="entropy" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Entropy<a href="entropy.html#entropy" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In my interpretation, the fundamental properties of entropy are:</p>
<ol style="list-style-type: decimal">
<li>Additive under independent joint: <span class="math inline">\(H(X, Y) = H(X)+H(Y)\)</span> when <span class="math inline">\(X\perp\!\!\!\perp Y\)</span>,
with subadditivity when <span class="math inline">\(X, Y\)</span> are not independent.</li>
<li>Subtractive under conditioning: <span class="math inline">\(H(X|Y) = H(X, Y) - H(X)\)</span> (chain rule).</li>
<li>Conditioning decreases entropy (implication of 1, 2)</li>
</ol>
<p>Strong subadditivity is the mathematical equivalent of
“conditioning decreases entropy,” which is further equivalent to
submodularity.</p>
<p>The two tricks in the proofs below are:</p>
<ol style="list-style-type: decimal">
<li>The chain <span class="math inline">\(H_n/n \leq \cdots \leq H_k/k \leq \cdots H_1\)</span> is equivalent to
<span class="math inline">\(H_{k+1}-H_k \leq H_k - H_{k-1}\)</span>. (Han’s inequality).</li>
<li>Expand “probability” to empirical lists and take appropriate limit (Shearer’s lemma).</li>
<li>When reduction inequalities are present (e.g. submodularity), reason about an easy case
(e.g. nested chain) then reduce using the reduction inequality (Shearer’s lemma).</li>
</ol>
<div id="basics" class="section level2 unnumbered hasAnchor">
<h2>Basics<a href="entropy.html#basics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1.1  (Entropy) </strong></span>The shannon entropy of a discrete random variable with
probability mass function <span class="math inline">\(P_x(x\in \mathcal X)\)</span> is
<span class="math display">\[
    H(X) = \mathbb E\left[\log \dfrac 1 {P_X(X)}\right]
\]</span>
The conditional entropy is
<span class="math display">\[
    H(X|Y) = \mathbb E_{y\sim P_Y}\left[
        H(P_{X|Y=y})
    \right]
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 1.2  (convexity) </strong></span>A subset <span class="math inline">\(S\)</span> of a vector space is convex if
<span class="math display">\[
    x, y\in S\implies \alpha x + \bar \alpha y \in S, \quad \alpha \in [0, 1], \quad \bar \alpha = 1 - \alpha
\]</span>
A function <span class="math inline">\(f:S\to \mathbb R\)</span> is convex if
<span class="math display">\[
    f(\alpha x + \bar \alpha y) \leq \alpha f(x) + \bar \alpha f(y)
\]</span>
For any <span class="math inline">\(S\)</span>-valued r.v. <span class="math inline">\(X\)</span>, <span class="math inline">\(f\)</span> is convex implies <span class="math inline">\(f(\mathbb E[X]) \leq \mathbb E[f(X)]\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-3" class="theorem"><strong>Theorem 1.1  (properties of entropy) </strong></span></p>
<ol style="list-style-type: lower-alpha">
<li>Positivity <span class="math inline">\(H(X)\geq 0\)</span> with equality iff <span class="math inline">\(X\)</span> is constant.</li>
<li>Uniform distribution maximizes entropy: for finite <span class="math inline">\(\mathcal X, H(X)\leq \log |\mathcal X|\)</span>
with equality iff <span class="math inline">\(X\)</span> is uniform on <span class="math inline">\(X\)</span>.</li>
<li>Chain rule: <span class="math inline">\(H(X, Y)=H(X)+H(Y|X) \leq H(X)+H(Y)\)</span>.</li>
<li>Conditioning decreases entropy: <span class="math inline">\(H(X|Y)\leq H(X)\)</span> with equality iff <span class="math inline">\(X, Y\)</span> are independent.</li>
<li>Deterministic transformation: <span class="math inline">\(H(X)=H(X, f(X))\geq H(f(X))\)</span> with equality iff <span class="math inline">\(f\)</span>
is injective on the support of <span class="math inline">\(P_X\)</span>.</li>
<li>Chain rule: equality holds iff <span class="math inline">\(X_1, \cdots, X_n\)</span> are mutually independent.<br />
<span class="math display">\[
H(X_1, \cdots, X_n) = \sum_{j=1}^n H(X_j|X^{j-1}) \leq \sum_{j=1}^n H(X_j)
\]</span></li>
</ol>
</div>
<p><em>Proof:</em>
(b) <span class="math inline">\(x\mapsto \log x\)</span> is concave, so
<span class="math display">\[
    H(X) = \mathbb E\log \dfrac 1 {P_X(X)} \leq
    \log \mathbb E\left[\dfrac 1 {P_X(X)}\right]
    = \log |\mathcal X|
\]</span>
(c) Telescoping
<span class="math display">\[
    H(X, Y) = \mathbb E\left[\log \dfrac 1 {P_{X, Y}(X, Y)}\right]
    = \mathbb E\left[\log \dfrac 1 {P_X(X)}\right] + \mathbb E\left[\log \dfrac 1 {P_{Y|X}(Y|X)}\right]
\]</span>
(d) Apply Jensen’s inequality to the strictly concave (over <span class="math inline">\([0, 1]\)</span>)
function <span class="math inline">\(-x\log x\)</span>.
<span class="math display">\[
    H(X|Y) = \sum_x \mathbb E_Y\left[P(x|Y) \log \dfrac 1 {P(x|Y)}\right]
    \leq \sum_x P(x)\log \dfrac 1 {P(x)} = H(X)
\]</span>
Jenson’s equality is saturated when <span class="math inline">\(\forall x: \mathbb E[P(x|Y)] = P(x) \iff X \perp\!\!\!\perp Y\)</span>.
(e) <span class="math inline">\(H(X, f(X)) = H(f(X)) + H(f(X)|X) = H(X) + H(f(X)|X)\)</span>, and <span class="math inline">\(H(f(X)|X)=0\iff f(X)\)</span> is constant
given <span class="math inline">\(X\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-4" class="theorem"><strong>Theorem 1.2  (axiomatic characterization of entropy) </strong></span>Denote a distribution <span class="math inline">\(P=(p_1, \cdots, p_m)\)</span> on <span class="math inline">\(m\)</span> letters and a functional <span class="math inline">\(H_m(p_1, \cdots, p_m)\)</span>.
<span class="math inline">\(H_m(P) = H(P)\)</span> has to be the Shannon entropy if it obeys the following axioms:</p>
<ol style="list-style-type: lower-alpha">
<li>Permutation invariance: <span class="math inline">\(H(P\circ \pi) = H(P)\)</span></li>
<li>Expandibility: <span class="math inline">\(H_{m+1}(P\oplus [0]) = H_m(P)\)</span></li>
<li>Subadditivity: <span class="math inline">\(H(X, Y)\leq H(X)+H(Y)\)</span>, with equality if <span class="math inline">\(X\perp\!\!\!\perp Y\)</span>. The saturation constraint
is equivalently
<span class="math display">\[
H_{mn}(p_1q_1, \cdots, p_mq_n) = H_m(P) + H_n(Q)
\]</span></li>
<li>Normalization: <span class="math inline">\(H_2(1/2, 1/2)=\log 2)\)</span></li>
<li>Continuity: <span class="math inline">\(H_2(p, 1-p)\to 0\)</span> as <span class="math inline">\(p\to 0\)</span>.</li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 1.3  (Renyi entropy) </strong></span>The Renyi entropy of order <span class="math inline">\(\alpha\)</span> is
<span class="math display">\[
    H_\alpha(P) = \begin{cases}
        \dfrac 1 {1-\alpha} \log \sum_j p_j^\alpha &amp; \alpha \in (0, \infty)-\{1\} \\
        \min_j \log(1/p_j) &amp; \alpha = \infty
    \end{cases}
\]</span>
Renyi entropy is non-increasing in <span class="math inline">\(\alpha\)</span> and tends to the Shannon entropy as <span class="math inline">\(\alpha\to 1\)</span>.</p>
</div>
<p>The “type” of a sequence refers to the empirical distribution of symbols in the sequence
(physical “macrostate”, and the exact sequence corresponds to the “microstate”).<br />
The following proposition shows that the multinomial coefficient can be approximated up to
a polynomial term by <span class="math inline">\(\exp(nH(P))\)</span>.</p>
<div class="proposition">
<p><span id="prp:methodOfTypes" class="proposition"><strong>Proposition 1.1  (method of types) </strong></span>Given non-negative integers <span class="math inline">\((n_j)_{1\leq j\leq k}\)</span> with <span class="math inline">\(\sum n_j=n\)</span> and
denote the distribution <span class="math inline">\((p_j) = (n_j/n)\)</span>, then the multinomial coefficient satisfies
<span class="math display">\[
    \dfrac 1 {(1+n)^{k-1}} e^{nH(P)} \leq \binom{n}{(n_j)} \leq e^{nH(P)},
    \quad \binom{n}{(n_j)} = \dfrac{n!}{\prod n_j!}
\]</span></p>
</div>
<p><em>Proof:</em> Fix <span class="math inline">\(P\)</span>, let <span class="math inline">\((X_j)\sim P\)</span> i.i.d and let <span class="math inline">\(N_j\)</span> denote the number of occurences of <span class="math inline">\(j\)</span>, then
<span class="math inline">\((N_j)\)</span> has a multinomial distribution
<span class="math display">\[
    \mathrm{Pr}((N_j)) = \binom{n}{(n_j)} \prod_j p_j^{n_j} = \binom{n}{(n_j)} e^{-nH(P)} \leq 1
\]</span>
The upper bound follows from <span class="math inline">\(\mathrm{Pr}((N_j))\leq 1\)</span>. For the lower-bound, note that
<span class="math inline">\((N_j)\)</span> takes at most <span class="math inline">\((n+1)^{k-1}\)</span> values (each entry takes values in <span class="math inline">\(\{0, \cdots, n\}\)</span>, and
there are <span class="math inline">\(k-1\)</span> degrees of freedom). Now
<span class="math display">\[
    1 = \sum_{(N_j)} \mathrm{Pr}((N_j)) \leq (n+1)^{k-1} \max \mathrm{Pr}((N_j))
    = (n+1)^{k-1} \binom{n}{(N_j)} e^{-nH(P)}
\]</span></p>
</div>
<div id="combinatorial-properties" class="section level2 unnumbered hasAnchor">
<h2>Combinatorial properties<a href="entropy.html#combinatorial-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-6" class="definition"><strong>Definition 1.4  (submodularity, monotone) </strong></span>Define <span class="math inline">\([n]=\{1, \cdots, n\}\)</span> and let <span class="math inline">\(\binom{S}{k}\)</span> denote subsets of <span class="math inline">\(S\)</span> of size <span class="math inline">\(k\)</span>, <span class="math inline">\(2^S\)</span>
all subsets of <span class="math inline">\(S\)</span>. A set function <span class="math inline">\(f:2^S\to \mathbb R\)</span> is submodular if <span class="math inline">\(\forall T_1, T_2\subset S\)</span>
<span class="math display">\[
    f(T_1\cup T_2) + f(T_1\cap T_2) \leq f(T_1) + f(T_2)
\]</span>
This captures the sense that “adding elements yield diminishing returns” by rearranging the equation
<span class="math display">\[
    f(T_1\cup T_2) - f(T_2)\leq f(T_1) - f(T_1\cap T_2)
\]</span>
The set function <span class="math inline">\(f\)</span> is monotone if <span class="math inline">\(T_1\subset T_2\implies f(T_1)\leq f(T_2)\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-7" class="theorem"><strong>Theorem 1.3  (strong subadditivity of entropy) </strong></span>Let <span class="math inline">\(X^n\)</span> be a discete RV, then <span class="math inline">\(T\mapsto H(X_T)\)</span> is monotone and submodular.</p>
</div>
<p><em>Proof:</em> Let <span class="math inline">\(A=X_{T_1-T_2}, B=X_{T_1\cap T_2}, C-X_{T_2-T_1}\)</span>, we need to show that
<span class="math display">\[
    H(A, B, C) + H(B) \leq H(A, B) + H(B, C)
\]</span>
Monotonicity is apparant. Submodularity follows
from the fact that conditioning decreases entropy
<span class="math display">\[\begin{align}
    H(A, B, C) - H(A, B) &amp;\leq H(B, C) - H(B) \\
    H(C|A, B) &amp;\leq H(C|B)
\end{align}\]</span></p>
<div class="remark">
<p><span id="submodularityInterpretation" class="remark"><em>Remark</em> (interpretation on submodularity). </span>Entropy subadditivity is the foralization of the fact that
adding information decreases entropy. Consider an example:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Z\)</span> is the result of a fair coin flip (<span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>).</li>
<li><span class="math inline">\(X\)</span> is the result of an independent fair coin flip.</li>
<li><span class="math inline">\(Y=X\oplus Z\)</span> denotes whether <span class="math inline">\(Z=X\)</span> (<span class="math inline">\(0\)</span> if this is true, else <span class="math inline">\(1\)</span>).</li>
</ol>
<p>Then <span class="math inline">\(H(X) = H(Y) = H(Z) = \log 2\)</span>;
also <span class="math inline">\(H(X, Z)=H(Y, Z)=H(X, Y, Z)=2\log 2\)</span>.
Then</p>
<span class="math display">\[\begin{aligned}
    H(Z|X, Y) &amp;= H(X, Y, Z) - H(X, Y) = 2\log 2 - 2\log 2 = 0 \\
    H(Z|Y) &amp;= H(Y, Z) - H(Y) = 2\log 2 - \log 2 = \log 2 = H(Z)
\end{aligned}\]</span>
<p>The first equation makes precise that “<span class="math inline">\(X, Y\)</span> tells us everything we need to know
about <span class="math inline">\(Z\)</span>”, while the last two that “<span class="math inline">\(X\)</span> alone tells us
nothing about <span class="math inline">\(Z\)</span>.” The mathematical implication of this statement is that
<span class="math display">\[
    H(Z|Y) &gt; H(Z|X, Y) \iff H(Y, Z) - H(Y) \geq H(X, Y, Z) - H(X, Y)
\]</span>
which is exactly the strong subadditivity of entropy:
adding information about <span class="math inline">\(X\)</span> increases <span class="math inline">\(H(X)\mapsto H(X, Y)\)</span> more than
it increases <span class="math inline">\(H(X, Z)\mapsto H(X, Y, Z)\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-8" class="theorem"><strong>Theorem 1.4  (Han's inequality) </strong></span>Let <span class="math inline">\(X^n\)</span> be a discrete <span class="math inline">\(n\)</span>-dimensional RV. Let
<span class="math display">\[
    \bar H_k(X^n) = \binom{n}{k}^{-1} \sum_{T\in \binom{[n]}{k}} H(X_T)
\]</span>
denote the average entropy of a <span class="math inline">\(k\)</span>-subset of coordinates. Then <span class="math inline">\(\bar H_k/k\)</span>
is decreasing in <span class="math inline">\(k\)</span>:
<span class="math display">\[
    \dfrac 1 n \bar H_n \leq \cdots \leq \dfrac 1 k \bar H_k \leq \cdots \leq \bar H_1
\]</span>
Furthermore, <span class="math inline">\(\bar H_k\)</span> is monotonically increasing but concave
<span class="math display">\[
    \bar H_{k+1} - \bar H_k \leq \bar H_k - \bar H_{k-1}
\]</span></p>
</div>
<p><em>Proof:</em> Let <span class="math inline">\(\bar H_0=0\)</span>.
The second claim implies the first since each term in the first
equation is an average of the (diminishing) terms in the second:
<span class="math display">\[
    \dfrac 1 m \bar H_m = \dfrac 1 m \sum_{k=1}^m (\bar H_k - \bar H_{k-1})
\]</span>
To prove the second claim, use submodularity
<span class="math display">\[
    H(X_1, \cdots, X_{k+1}) - H(X_1, \cdots, X_{k}) \leq H(X_2, \cdots, X_{k+1}) - H(X_2, \cdots, H_{k})
\]</span>
Average this over all <span class="math inline">\(n!\)</span> permutations of <span class="math inline">\([n]\)</span> to get
<span class="math display">\[
    \bar H_{k+1} - \bar H_k \leq \bar H_k - \bar H_{k-1}
\]</span>
Equivalently, use the fact that conditioning decreases entropy,
average this expression over <span class="math inline">\(n!\)</span> permutations
<span class="math display">\[
    H(X_{k+1}|X_1, \cdots, X_k) \leq H(X_{k+1}|X_2, \cdots, X_k)
\]</span></p>
<div class="corollary">
<p><span id="cor:pairwiseBound" class="corollary"><strong>Corollary 1.1  (joint entropy pairwise bound) </strong></span><span class="math inline">\(H(X) \leq \dfrac 1 {n-1} \sum_{i&lt;j} H(X_i, X_j)\)</span></p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-9" class="corollary"><strong>Corollary 1.2  (a cute geometric result) </strong></span>Consider placing <span class="math inline">\(N\)</span> points in <span class="math inline">\(\mathbb R^3\)</span> arbitrarily. Let<br />
<span class="math inline">\(N_1, N_2, N_3\)</span> denote the number of distinct points projected
onto the <span class="math inline">\(xy, xz, yz\)</span>-planes, then <span class="math inline">\(N_1N_2N_3\geq N^2\)</span>.</p>
</div>
<p><em>Proof</em>: Let <span class="math inline">\(\mathcal C\subset (\mathbb R^3)^N\)</span> denote the set of coordinates
of the <span class="math inline">\(N\)</span> points, and let <span class="math inline">\((A, B, C)\sim \mathcal C\)</span> denote the three
components of the points drawn from <span class="math inline">\(\mathcal C\)</span>, then using Han’s inequality:
<span class="math display">\[
    \log N = H(A, B, C) \leq \dfrac 1 2(\log N_1 + \log N_2 + \log N_3)
\]</span></p>
<div class="remark">
<p><span id="unlabeled-div-10" class="remark"><em>Remark</em>. </span>The core to the proof above is that combinatorics (size-counting)
related to components are subject to the submodularity of entropy.</p>
</div>
<div class="theorem">
<p><span id="thm:shearer" class="theorem"><strong>Theorem 1.5  (Shearer's lemma) </strong></span>Let <span class="math inline">\(X^n\)</span> be discrete <span class="math inline">\(n\)</span>-dimensional RV and <span class="math inline">\(S\subset [n]\)</span>
be a RV independent of <span class="math inline">\(X^n\)</span>, then
<span class="math display">\[
    H(X_S|S) \geq H(X^n) \min_{i\in [n]} \mathrm{Pr}(i\in S)
\]</span>
This holds for any submodular set-function <span class="math inline">\(H\)</span> with <span class="math inline">\(H(\emptyset) = 0\)</span>.</p>
</div>
<p><em>Proof:</em> Consider the equivalent statement by “unrolling the probability”:
if <span class="math inline">\(\mathcal C=(S_1, \cdots, S_M)\)</span>
is a list of subsets of <span class="math inline">\([n]\)</span> then
<span class="math display">\[
    \sum_j H(S_j) \geq H(X^n) \min_j \deg(j), \quad \deg(j) = |\{j: i\in S_j\}|
\]</span>
Call <span class="math inline">\(\mathcal C\)</span> a chain if all subsets are such that
<span class="math inline">\(S_1\subset S_2\cdots \subset S_M\)</span>. For a chain, the proposition
is trivial since <span class="math inline">\(\min_j \deg(j)\)</span> is zero of <span class="math inline">\(S_M\neq [n]\)</span> or equals
the multiplicity of <span class="math inline">\(S_M\)</span> in <span class="math inline">\(\mathcal C\)</span>, which equals <span class="math inline">\(\min_j \deg(j)\)</span>, in which case
<span class="math display">\[
    \sum_j H(X_{S_j}) \geq H(X_{S_M})\min_j \deg(j)
\]</span>
When <span class="math inline">\(\mathcal C\)</span> is not a chain, consider a pair of sets <span class="math inline">\(S_1, S_2\)</span>
incomparable by inclusion and replace with <span class="math inline">\(S_1\cap S_2, S_1\cup S_2\)</span>.
Submodularity implies that <span class="math inline">\(\sum_j H(X_{S_j})\)</span> does not increase under this
transform, and <span class="math inline">\(\deg j\)</span> are not changed. Repeat this until we have a chain.</p>
<p><span style="color:green">
Main idea of the proof:
(1) expand “probability” to empirical lists.
(2) Reason about an easy case (chain),
then reduce to easy case by submodularity.
</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="entropy-method-in-combinatorics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
