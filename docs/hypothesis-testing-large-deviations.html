<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Hypothesis Testing, Large Deviations | 6.7480 Notes</title>
  <meta name="description" content="11 Hypothesis Testing, Large Deviations | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Hypothesis Testing, Large Deviations | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Hypothesis Testing, Large Deviations | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-11-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lossless-compression.html"/>
<link rel="next" href="lecture-notes.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>12</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-testing-large-deviations" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">11</span> Hypothesis Testing, Large Deviations<a href="hypothesis-testing-large-deviations.html#hypothesis-testing-large-deviations" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li>The Neyman-Pearson region is reducible to the
closure of convex hull of deterministic tests (thm:detReduction).
Use the layer-cake representation to argue convexity.</li>
<li>The Neyman-Pearson lemma fundamentally follows from the
convexity of <span class="math inline">\(\mathcal R(P, Q)\)</span> and that the supporting
lines of are given by likelihood tests.</li>
<li>Stein’s and Chernoff’s asymptotic regimes.</li>
<li>The cumulant-generating function shows up naturally as the
normalization factor in tilting, and extremizing the tilting
formula leads naturally to the rate function (Legendre
conjugate of the CGF).
<ul>
<li>The one-parameter family of tilted families form the natural
exponential families.</li>
</ul></li>
<li>A sharper version of Chernoff’s bound for mean
deviations (equation <a href="hypothesis-testing-large-deviations.html#eq:largeSamplePrecChernoff">(11.1)</a>).</li>
<li>Crucial bridge between large-deviation estimate and
information projection (theorem <a href="hypothesis-testing-large-deviations.html#thm:ldeInfoProj">11.9</a>).</li>
<li>For any event <span class="math inline">\(E\)</span> with <span class="math inline">\(P_X[E]&gt;0\)</span>, we have
<span class="math inline">\(\log \frac 1 {P_X(E)} = D(P_{X|X\in E} \| P_X)\)</span></li>
</ol>
<p><span style="color:red">
Justification of the infimum in 15.13
</span></p>
<div id="npFormulation" class="section level2 unnumbered hasAnchor">
<h2>Neyman-Pearson<a href="hypothesis-testing-large-deviations.html#npFormulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This part corresponds to chapter 14 of the textbook.</p>
<div class="definition">
<p><span id="def:unlabeled-div-85" class="definition"><strong>Definition 11.1  (NP formulation of BHT) </strong></span>Consider two distributions <span class="math inline">\(P, Q\)</span> on <span class="math inline">\(\Omega\)</span>, one
of which generated our observation <span class="math inline">\(X\)</span>.
The two hypotheses are <span class="math inline">\(H_0:X\sim P\)</span> and <span class="math inline">\(H&gt;_1:X\sim Q\)</span>.
Let <span class="math inline">\(Z=\{0, 1\}\)</span> denote accepting and rejecting
the null, respectively.</p>
<ul>
<li>A <strong>test</strong> is a (possibly random) decision rule.
<ul>
<li>It is deterministic if of form <span class="math inline">\(f:\mathcal X\to \{0, 1\}\)</span>;
which partitions the event space.</li>
<li>A randomized test is specified by a
kernel <span class="math inline">\(P_{Z|X}:\mathcal X\to \{0, 1\}\)</span> so that <span class="math inline">\(P_{Z|X}(1|x)\in [0, 1]\)</span>
is the probability of rejecting the null upon observing <span class="math inline">\(X=x\)</span>.</li>
</ul></li>
<li><span class="math inline">\(\alpha = \pi_{0|0} = P[Z=0]\)</span>: probability of success given <span class="math inline">\(H_0\)</span> is true.
<ul>
<li><span class="math inline">\(1 - \alpha\)</span> is the false positive rate.</li>
</ul></li>
<li><span class="math inline">\(\beta = \pi_{0|1} = Q[Z=0]\)</span>: probability of error given <span class="math inline">\(H_1\)</span> is true.
<ul>
<li><span class="math inline">\(1-\beta\)</span> is the power.</li>
</ul></li>
</ul>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-86" class="definition"><strong>Definition 11.2  (Neyman-Pearson region) </strong></span>Given <span class="math inline">\((P, Q)\)</span>, the Neyman-Pearson region consists of achievable
points for all randomized tests
<span class="math display">\[
    \mathcal R(P, Q) = \{(P[Z=0], Q[Z=0]):\forall P_{Z|X}:
    \mathcal X\to \{0, 1\}\}\subset [0, 1]^2.
\]</span>
Fixing <span class="math inline">\(\alpha\)</span>, we wish to obtain the lowest <span class="math inline">\(\beta\)</span>, so the
lower boundary is defined by
<span class="math display">\[
    \beta_\alpha(P, Q) = \inf_{P[Z=0]\geq \alpha} Q[Z=0]
\]</span>
Each deterministic test corresponds to a measurable subset <span class="math inline">\(E\)</span>,
so the deterministic Neyman-Pearson region is specified by
<span class="math display">\[
    \mathcal R_{\mathrm{det}}(P, Q) = \{(P[E], Q[E]): E\text{ measurable}\}.
\]</span></p>
</div>
<p>Note that <span class="math inline">\(\mathcal R(P, Q)=[0, 1]^2\)</span> if <span class="math inline">\(P\perp Q\)</span>, while it’s the
zero-measure diagonal if <span class="math inline">\(P=Q\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-87" class="proposition"><strong>Proposition 11.1  (properties of the NP region) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathcal R(P, Q)\)</span> is closed and convex.</li>
<li><span class="math inline">\(\mathcal R(P, Q)\)</span> contains the diagonal.</li>
<li><span class="math inline">\(\mathcal R(P, Q)\)</span> is symmetric about <span class="math inline">\(\alpha=\beta\)</span>.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
Given <span class="math inline">\((\alpha_0, \beta_0), (\alpha_1, \beta_1)\in \mathcal R(P, Q)\)</span>
corresponding to tests <span class="math inline">\(P_{Z_0|X}, P_{Z_1|X}\)</span>, and randomizing between
the two tests yield the desired convexity. Closeness is established later.
Testing by random guessing <span class="math inline">\(Z\sim \mathrm{Ber}(1-\alpha)\perp X\)</span> achieves
<span class="math inline">\((\alpha, \alpha)\)</span>. To establish symmetry, <span class="math inline">\(P_{1-Z|X}\)</span>
achieves <span class="math inline">\((1-\alpha, 1-\beta)\)</span>.
</details>
<div class="theorem">
<p><span id="thm:detReduction" class="theorem"><strong>Theorem 11.1  (reduction to deterministic tests) </strong></span><span class="math inline">\(\mathcal R(P, Q) = \overline{\mathrm{co}(\mathcal R_{\mathrm{det}}(P, Q))}\)</span>.
Consequently, if <span class="math inline">\(P, Q\)</span> are on a finite alphabet <span class="math inline">\(\mathcal X\)</span>, then
<span class="math inline">\(\mathcal R(P, Q)\)</span> is a polygon of at most <span class="math inline">\(2^{|X|}\)</span> vertices.</p>
</div>
<details>
<summary>
Proof
</summary>
To prove the nontrivial direction <span class="math inline">\(\subset\)</span>, given any
randomized <span class="math inline">\(P_{Z|X}\)</span>, define <span class="math inline">\(g:\mathcal X\to [0, 1]\)</span>
by <span class="math inline">\(g(x) = P_{Z|X}(0|X)\)</span>, the
<span class="math display">\[\begin{align}
    P[Z=0] &amp;= \sum_x g(x)P(x) = \mathbb E_P[g] = \int_0^1 P[g\geq t ]\, dt \\
    Q[Z=0] &amp;= \mathbb E_Q[g] = \int_0^1 Q[g\geq t]\, dt
\end{align}\]</span>
The last inequality holds because for any nonnegative r.v,
it has an integral representation in terms of indicator r.v’s <span class="math inline">\(1_{U\geq t}\)</span>
by <span class="math inline">\(U = \int_0^\infty 1_{U\geq t}\, dt\)</span> so
<span class="math display">\[\begin{align}
    \mathbb E[U] = \mathbb E\left[\int_0^\infty 1_{U\geq t}\, dt\right]
    = \int_0^\infty \mathbb E[1_{U\geq t}]\, dt
\end{align}\]</span>
Thus <span class="math inline">\((P[Z=0], Q[Z=0])\)</span> is a mixture of points
<span class="math inline">\((P[g\geq t], Q[g\geq t])\in \mathcal R_{\mathrm{det}}\)</span> thus in the closure of
the convex hull.
</details>
<p>We define the extended log-likelihood ratio (LLR) <span class="math inline">\(T(x)\)</span>
by extending <span class="math inline">\(\log p(x)/q(x)\)</span> in the usual
manner to <span class="math inline">\(\pm \infty\)</span> if <span class="math inline">\(q, p=0\)</span> respectively and <span class="math inline">\(0\)</span> if both are <span class="math inline">\(0\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-88" class="definition"><strong>Definition 11.3  ((extended) log-likelihood ratio (LLR)) </strong></span>Given <span class="math inline">\(P, Q\)</span>, define LLR by
<span class="math display">\[
    T(x) = \begin{cases}
        0 &amp; q(x) = p(x) = 0 \\
        \infty &amp; q(x) = 0 \\
        -\infty &amp; p(x) = 0 \\
        \log \dfrac{p(x)}{q(x)}
    \end{cases}
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:llrProperties" class="theorem"><strong>Theorem 11.2  (properties of LLR, change of measures) </strong></span></p>
<ol style="list-style-type: decimal">
<li>Fixing <span class="math inline">\(t\in \mathbb R\)</span>, <span class="math inline">\(Q[T=t] = e^{-t} P[T=t]\)</span>. In other words,
on the common support of <span class="math inline">\(P, Q\)</span> we obtain
<span class="math inline">\(dQ = e^{-T} dP\)</span>.</li>
<li>The expectation value of <span class="math inline">\(h:\mathcal X\to \mathbb R\)</span> on the common
support of <span class="math inline">\(P, Q\)</span> under the two distributions are related by
<span class="math display">\[\begin{align}
\mathbb E_Q\left[h \cdot 1_{T&gt; -\infty}\right]
= \mathbb E_P[h\cdot e^{-T}], \quad
\mathbb E_P[h \cdot 1_{T&lt;+\infty}] = \mathbb E_Q[h \cdot e^T]
\end{align}\]</span></li>
<li>For any <span class="math inline">\(f\geq 0\)</span> and <span class="math inline">\(\tau\in \mathbb R\)</span> we have
<span class="math display">\[\begin{align}
\mathbb E_Q[f \cdot 1_{T\geq \tau}] \leq
e^{-\tau} \mathbb E_P[f\cdot 1_{T\geq \tau}], \quad
e^{-\tau} \mathbb E_P[f\cdot 1_{T\leq  \tau}] \leq
\mathbb E_Q[f\cdot 1_{T\leq \tau}]
\end{align}\]</span></li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
To prove (1), compute
<span class="math display">\[\begin{align}
    Q[T=t] &amp;= \sum Q(x) 1\left[\log \dfrac{P(x)}{Q(x)} = t\right]
    = \sum Q(x) 1\left[e^t Q(x) = P(x)\right] \\
    &amp;= e^{-t} \sum P(x) 1[e^t Q(x)=P(x)] = e^{-t}P[T=t]
\end{align}\]</span>
From the definition, note that <span class="math inline">\(Q[T=\infty]=P[T=-\infty]=0\)</span>.
Then we can w.l.o.g. operate on the common support <span class="math inline">\(R\)</span> of
<span class="math inline">\(P, Q\)</span>, on which <span class="math inline">\(dQ = e^{-T}\, dP\)</span>
<span class="math display">\[
    \mathbb E_Q[h\cdot 1_{T&gt;-\infty}]
    = \int_R h\, dQ = \int_R e^{-T} h\, dP = \mathbb E_P [h\cdot e^{-T}]
\]</span>
The second equation is proved similarly.
Onto (3), for the first inequality,
substitute <span class="math inline">\(h\mapsto f\cdot 1_{T\geq \tau}\)</span>
and <span class="math inline">\(h\mapsto f\cdot 1_{T\leq \tau}\)</span>
to part (2) to obtain
<span class="math display">\[\begin{align}
    \mathbb E_Q[f\cdot 1_{T\geq \tau}]
    &amp;= \mathbb E_P[f\cdot 1_{T\geq \tau} e^{-T}]
    \leq e^{-\tau} \, \mathbb E_P[f\cdot 1_{T\geq \tau}] \\
    \mathbb E_P[f\cdot 1_{T\leq \tau}]
    &amp;= \mathbb E_Q[f\cdot 1_{T\leq \tau} e^T]
    \leq e^\tau \mathbb E_Q[f\cdot 1_{T\leq \tau}]
\end{align}\]</span>
</details>
<div class="definition">
<p><span id="def:unlabeled-div-89" class="definition"><strong>Definition 11.4  (likelihood ratio test) </strong></span>The LRT with threshold <span class="math inline">\(\tau\in \mathbb R\cup {\pm \infty}\)</span> is defined by
<span class="math inline">\(\mathrm{LRT}_\tau(x) = 1_{T(x)&gt;\tau}\)</span>.</p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-90" class="corollary"><strong>Corollary 11.1  (sufficiency) </strong></span><span class="math inline">\(T\)</span> is sufficient statistic for testing <span class="math inline">\(P\)</span> versus <span class="math inline">\(Q\)</span>.</p>
</div>
<p>It suffices to prove that <span class="math inline">\(P_{X|T}=Q_{X|T}\)</span>.
Note that <span class="math inline">\(P_{T|X}=Q_{T|X}\)</span>, in which case
<span class="math display">\[\begin{align}
    P_{X|T}(x|t)
    &amp;= \dfrac{P_X(x) P_{T|X}(t|x)}{P_T(t)}
    = \dfrac{P_X(x) 1[dP/dQ = e^t]}{P_T(t)} \\
    &amp;= \dfrac{e^t Q(x) 1[dP/dQ = e^t]}{P_T(t)}
    = \dfrac{Q_{XT}(x, t)}{e^{-t}P_T(t)} = Q_{X|T}(x|t)
\end{align}\]</span></p>
<p>We proceed to providing two bounds on <span class="math inline">\(\mathcal R(P, Q)\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Converse (outer) bounds: any point in <span class="math inline">\(\mathcal R(P, Q)\)</span>
must satisfy certain constraints.</li>
<li>Achievability (innder) bounds: points satisfying certain
constraints belong to <span class="math inline">\(\mathcal R(P, Q)\)</span>.</li>
</ol>
<p>The following result provides an “envelope” for <span class="math inline">\(\mathcal R(P, Q)\)</span>
and, in fact, applies to any <span class="math inline">\(f\)</span>-divergence. It follows
from simply applying data-processor <span class="math inline">\(P_{Z|X}\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-91" class="theorem"><strong>Theorem 11.3  (weak converse) </strong></span><span class="math inline">\(\forall (\alpha, \beta)\in \mathcal R(P, Q)\)</span>, we obtain
<span class="math display">\[
    d(\alpha \| \beta) \leq D(P\|Q), \quad
    d(\beta \| \alpha) \leq D(Q\|P)
\]</span></p>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-92" class="lemma"><strong>Lemma 11.1  </strong></span>Given any test <span class="math inline">\(Z\)</span> and <span class="math inline">\(\gamma&gt;0\)</span>, we obtain
<span class="math display">\[
    P[Z=0] - \gamma Q[Z=0] \leq P[T&gt;\log \gamma]
    \iff \alpha - \gamma \beta \leq P[T&gt;\log \gamma]
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Let <span class="math inline">\(\tau = \log \gamma\)</span>, apply theorem <a href="hypothesis-testing-large-deviations.html#thm:llrProperties">11.2</a>
to obtain
<span class="math display">\[
    P[Z=0, T\leq \tau] \leq \gamma Q[Z=0, T\leq \tau]
\]</span>
Decomposing the marginal <span class="math inline">\(P[Z=0]=P[Z=0, T\leq \tau] + P[Z=0, T&gt;\tau]\)</span> yields
<span class="math display">\[
    P[Z=0] - \gamma Q[Z=0]
    \leq P[Z=0, T&gt;\tau] - Q[Z=0, T&gt;\tau]  \leq P[T&gt;\tau]
\]</span>
</details>
<p>Applying this lemma to <span class="math inline">\((P, Q, \gamma)\)</span> and <span class="math inline">\((P, Q, 1/\gamma)\)</span> yields
the strong converse. It effectively states that <span class="math inline">\(\mathcal R(P, Q)\)</span>
is contained in the intersection of an infinite collection of
halfplanes indexed by <span class="math inline">\(\gamma\)</span>.
Compared to the weak converse, we need to know the CDF of <span class="math inline">\(T\)</span>
compared to just the expectation (given by the divergence).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-93" class="theorem"><strong>Theorem 11.4  (strong converse) </strong></span><span class="math inline">\(\gamma&gt;0, \alpha - \gamma \beta \leq P[T&gt;\log \gamma]\)</span> and
<span class="math inline">\(\beta - \alpha/\gamma \leq Q[T&lt;\log \gamma]\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Proceeding to achievability, a convex set can be efficiently
dscribed by its supporting hyperplanes. Characte&gt;rizing <span class="math inline">\(\mathcal R(P, Q)\)</span>
is thus equivalent to solving, for each <span class="math inline">\(t&gt;0\)</span>,
<span class="math display">\[
    \min_{(\alpha, \beta)\in \mathcal R(P, Q)} t\beta - \alpha.
\]</span>
To see this, this is looking for the minimal <span class="math inline">\(y\)</span>-intercept <span class="math inline">\(c\)</span>
of <span class="math inline">\((1, -t)\)</span> such that <span class="math inline">\(t\beta = \alpha + c\)</span> for
some <span class="math inline">\((\alpha, \beta) \in \mathcal R(P, Q)\)</span>. This is equivalent to
minimizing weighted probability of error with <span class="math inline">\((1-\alpha, \beta)\)</span>
weighted by <span class="math inline">\((1, t)\)</span>. To solve this,
<span class="math display">\[\begin{align}
    \alpha^* - t\beta^*
    = \max_{P_{Z|X}} \sum_{x\in \mathcal X} \left[
        P(x) - t Q(x)
    \right] P_{Z|X}(0|X)
    = \sum_{x\in \mathcal X} \max[0, P(X) - tQ(X)]
\end{align}\]</span>
The last equality follows from the obvious choice
<span class="math display">\[
    P_{Z|X}^*(0|x) = 1\left\{
        P(x) \geq tQ(x)
    \right\} = 1_{T\geq \log t}.
\]</span>
</details>
<div class="theorem">
<p><span id="thm:npLemma" class="theorem"><strong>Theorem 11.5  (Neyman-Pearson lemma) </strong></span>For each <span class="math inline">\(\alpha, \beta_\alpha\)</span> is attained by the test
<span class="math display">\[
    P_{Z|X}(0|x) = \begin{cases}
        1 &amp; T&gt;\tau \\
        \lambda &amp; T=\tau \\
        0 &amp; T &lt; \tau
    \end{cases}, \quad \alpha = P[T&gt;\tau] + \lambda P[T=\tau].
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Fixing <span class="math inline">\(\tau\in \mathbb R\)</span>, let <span class="math inline">\(t=e^\tau\)</span>. Given any
test <span class="math inline">\(P_{Z|X}\)</span>, write <span class="math inline">\(g(x) = P_{Z|X}(0|x)\)</span> be the claimed
probability of the null. We wish to show that
<span class="math display">\[
    \alpha = E_P[g] = P[T&gt;\tau] + \lambda P[T=\tau]
    \implies \beta = \mathbb E_Q[g] \geq Q[T&gt;\tau] + \lambda Q[T=\tau].
\]</span>
Apply theorem <a href="hypothesis-testing-large-deviations.html#thm:llrProperties">11.2</a> twice yields
<span class="math display">\[\begin{align}
    \beta
    &amp;= \mathbb E_Q[g\cdot 1_{T&gt;\tau}] + \mathbb E_Q[g\cdot 1_{T\leq \tau}]
    \geq \mathbb E_Q[g\cdot 1_{T&gt;\tau}] +
    \dfrac 1 t \mathbb E_P[g\cdot 1_{T\leq \tau}] \\
    &amp;= \mathbb E_Q[g\cdot 1_{T&gt;\tau}] + \dfrac 1 t \left(
        \mathbb E_P\left[(1-g) 1_{T&gt;\tau}\right]
        + \lambda P[T=\tau]
    \right) \\
    &amp;\geq
    \mathbb E_Q[g\cdot 1_{T&gt;\tau}] + \mathbb E_Q[(1-g) 1_{T&gt;\tau}]
    + \lambda Q[T=\tau]  \\
    &amp;= Q[T&gt;\tau] + \lambda Q[T=\tau]
\end{align}\]</span>
Inspecting the saturation conditions yield the LLR
test as claimed.
</details>
<div class="definition">
<p><span id="def:unlabeled-div-94" class="definition"><strong>Definition 11.5  (asymptotic regimes) </strong></span>For large-sample i.i.d asymptotics, we are interested
in the following two questions:</p>
<ol style="list-style-type: decimal">
<li><em>Stein’s regime:</em> conditioning on <span class="math inline">\(\pi_{1|0}\leq \epsilon\)</span>,
what is the best decay rate for <span class="math inline">\(\pi_{0|1}\)</span>?</li>
<li><em>Chernoff’s regime:</em> when <span class="math inline">\(\pi_{1|0}, \pi_{0|1}\)</span> both
vanish exponentially, what is the optimal tradeoff between
their exponents?</li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-95" class="definition"><strong>Definition 11.6  (Stein's exponent) </strong></span>The <span class="math inline">\(\epsilon\)</span>-optimal exponent in Stein’s regime is
<span class="math display">\[
    V_\epsilon = \sup[
        E: \exists n_0:\forall n\geq n_0, \exists
        P_{Z|X^n} \text{ such that } \alpha &gt; 1 - \epsilon,
        \beta &lt; e^{-nE}
    ]
\]</span> &gt;
Stein’s exponent is <span class="math inline">\(V=\lim_{\epsilon\to 0}V_\epsilon\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-96" class="proposition"><strong>Proposition 11.2  (equivalent definition of Stein's exponent) </strong></span><span class="math display">\[
    V_\epsilon = \liminf_{n\to \infty} \dfrac 1 n \log \dfrac 1 {
        \beta_{1-\epsilon}(P_{X^n}, Q_{X^n})
    }
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-97" class="theorem"><strong>Theorem 11.6  (Stein's lemma) </strong></span><span class="math inline">\(V_\epsilon = D(P\|Q)\)</span> for all <span class="math inline">\(\epsilon \in (0, 1)\)</span>.</p>
</div>
</div>
<div id="large-deviations-theory" class="section level2 unnumbered hasAnchor">
<h2>Large deviations theory<a href="hypothesis-testing-large-deviations.html#large-deviations-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This part corresponds to chapter 15 of the textbook.</p>
<p>Consider an i.i.d sequence <span class="math inline">\(X_1, \dots, X_n\sim P\)</span> and
<span class="math inline">\(\hat P_n\)</span> their empirical distribution.
We define the <strong>cumulant-generating function</strong>
<span class="math display">\[
    \psi_X(\lambda) = \log \mathbb E[e^{\lambda X}]
\]</span>
The Legendre transform <span class="math inline">\(\psi_X^*=E\)</span> is also known as
the <strong>rate function</strong>:
<span class="math display">\[
    \psi_X^*(\lambda) = \sup_\lambda \lambda \gamma - \psi_X(\lambda)
\]</span>
We assume for regularity that <span class="math inline">\(\psi_X\)</span> is finite everywhere.
This is known as <strong>Cramer’s condition</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-98" class="definition"><strong>Definition 11.7  (CGF properties) </strong></span>Under Cramer’s condition, we obtain</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\psi_X\)</span> is convex.</li>
<li><span class="math inline">\(\psi_X\)</span> continuous.</li>
<li><span class="math inline">\(\psi_X\)</span> smooth (infinitely differentiable) and
<span class="math display">\[
\psi&#39;_X(\lambda) = \dfrac{\mathbb E[X e^{\lambda X}]}{\mathbb E[\exp(\lambda X)]}
= e^{-\psi_X(\lambda)}\mathbb E[X e^{\lambda X}]
\]</span>
In particular, <span class="math inline">\(\psi&#39;_X(0) = \mathbb E[X]\)</span>.</li>
<li>If <span class="math inline">\(a\leq X\leq b\)</span> a.s. then <span class="math inline">\(a\leq \psi&#39;_X\leq b\)</span>.
Conversely, <span class="math inline">\(\inf \psi&#39;_X \leq X\leq \sup \psi&#39;_X\)</span> a.s.</li>
<li>If <span class="math inline">\(X\)</span> is not a constant, then <span class="math inline">\(\psi_X\)</span> is strictly convex.</li>
</ol>
</div>
<details>
<summary>
Proof (partial)
</summary>
<ol style="list-style-type: decimal">
<li>Fix <span class="math inline">\(\theta\in (0, 1)\)</span>, define the <span class="math inline">\(L_p\)</span> norm of a r.v. by
<span class="math inline">\(\|U\|_p = \left(\mathbb E|U|^p\right)^{1/p}\)</span>. Apply Holder’s
inequality <span class="math inline">\(\mathbb E|UV| \leq \|U\|_p\|V\|_q\)</span> with
<span class="math inline">\((p, q) = (1/\theta, 1/\bar \theta)\)</span> yields
<span class="math display">\[\begin{align}
\mathbb E|e^{\lambda_1/p + \lambda_2/q}|
\leq \| e^{\lambda_1 X/p}\|_p \| e^{\lambda_2 X/q}\|_q
= \mathbb E[e^{\lambda_1 X}]^\theta \mathbb E[e^{\lambda_x X}]^{\bar \theta}
\end{align}\]</span>
Take the logarithm on both sides to obtain complexity.</li>
<li>A convex function must be continuous on the interior of its domain.</li>
<li>To first demonstrate that <span class="math inline">\(\mathbb E|X e^{\lambda X}|\)</span> exists,
note that <span class="math inline">\(e^{|X|} \leq e^X + e^{-X}\)</span>, then
<span class="math display">\[
     |X e^{\lambda X}| \leq e^{(\lambda+1)X} \leq e^{(\lambda+1)X}
     + e^{-(\lambda + 1)X}
\]</span>
both quantities are absolutely integrable.
Next, <span class="math inline">\(u\mapsto \mathbb E|X e^{uX}|\)</span> is integrable on <span class="math inline">\([0, \lambda]\)</span>,
then applying Fubini yields
<span class="math display">\[\begin{align}
     e^{\psi_X(\lambda)}
     &amp;= \mathbb E[e^{\lambda X}] = \mathbb E\left[
         1 + \int_0^\lambda X e^{uX}\, du
     \right] = 1 + \int_0^\lambda \mathbb E[X e^{uX}]\, du \\
     \psi&#39;_X(\lambda) e^{\psi_X(\lambda)}
     &amp;= \mathbb E[X e^{\lambda X}]
\end{align}\]</span></li>
<li><span class="math inline">\(A\leq X\leq B\implies \psi&#39;_X(\lambda) \in [A, B]\)</span> by applying part (3).</li>
<li>The converse of (4) and (5) omitted.</li>
</ol>
</details>
<div class="lemma">
<p><span id="lem:chernoff" class="lemma"><strong>Lemma 11.2  (Chernoff bound) </strong></span>Given i.i.d <span class="math inline">\(X^n\)</span> and <span class="math inline">\(\lambda \geq 0\)</span> and let <span class="math inline">\(\bar X\)</span> denote
the empirical mean. Let <span class="math inline">\(\mathbb E[X]&lt;\gamma&lt;\mathrm{esssup}[X]\)</span>, then
<span class="math display">\[
    \mathrm{Pr}\left[
        \bar X \geq \gamma
    \right] \leq \exp \left[-n\lambda \gamma + n \psi_X(\lambda)\right]
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Apply Morkov’s inequality <span class="math inline">\(\mathrm{Pr}[X\geq a]\leq \dfrac 1 a \mathbb E[X]\)</span>
after exponentiating the inequality;
note that <span class="math inline">\(\log \mathbb E[\exp(n\lambda \bar X)]
= \log \phi_{X^n}(\lambda) = n\psi_X(\lambda)\)</span>.
<span class="math display">\[\begin{align}
    \mathrm{Pr}[n\bar X\geq n\gamma]
    &amp;= \mathrm{Pr}[\exp(n\lambda \bar X) \geq \exp(n\lambda \gamma)] \\
    &amp;\leq \exp(-n\lambda \gamma) \mathbb E[\exp(n\lambda \bar X)] \\
    &amp;= \exp \left(-n\lambda \gamma + n\psi_X(\lambda)\right)
\end{align}\]</span>
</details>
<div class="theorem">
<p><span id="thm:unlabeled-div-99" class="theorem"><strong>Theorem 11.7  (large-sample mean deviations) </strong></span>Given a r.v <span class="math inline">\(X\)</span> whose log-MGF <span class="math inline">\(\psi_X\)</span>
is finite everywhere. Let <span class="math inline">\(B=\mathrm{esssup}\, X\)</span> and <span class="math inline">\(\mathbb E[X]&lt;\gamma&lt;B\)</span>, then
<span class="math display">\[
    \mathrm{Pr}\left[\dfrac 1 n \sum_{j=1}^n X_j \geq \gamma\right]
    = \exp \left[-nE(\gamma) + o(n)\right]
\]</span>
where <span class="math inline">\(\lambda_X^*(\gamma) = \sup_{\lambda \geq 0}
\lambda \gamma - \psi_X(\lambda)\)</span> is the <em>rate function.</em></p>
</div>
<details>
<summary>
Proof (non-asymptotic upper-bound)
</summary>
Recalling the Chernoff bound (lemma <a href="hypothesis-testing-large-deviations.html#lem:chernoff">11.2</a>),
we obtain <span class="math inline">\(\mathrm{Pr}[\bar X\geq \gamma]
\leq \exp[-n\lambda \gamma + n\psi_X(\lambda)]\)</span>. Optimize
over <span class="math inline">\(\lambda\)</span> naturally yields the Legendre transform <span class="math inline">\(\psi_X^*\)</span>
which is <em>non-asymptotic</em>:
<span class="math display" id="eq:largeSamplePrecChernoff">\[
    \mathrm{Pr}[\bar X\geq \gamma] \leq \exp[-n \psi_X^*(\gamma)]
    \tag{11.1}
\]</span>
</details>
<div class="definition">
<p><span id="def:unlabeled-div-100" class="definition"><strong>Definition 11.8  (tilting) </strong></span>Given <span class="math inline">\(X\sim P\)</span> and <span class="math inline">\(\lambda \in \mathbb R\)</span>, the tilted measure
<span class="math inline">\(P_\lambda\)</span> is defined by
<span class="math display">\[
    P_\lambda(dx)
    = \dfrac{e^{\lambda x}}{\mathbb E[e^{\lambda x}]} P(dx)
    = e^{\lambda x - \psi_X(\lambda)} P(dx)
\]</span></p>
</div>
<p>Fixing <span class="math inline">\(P\)</span>, the one-parameter family of tilted distributions <span class="math inline">\(P_\lambda\)</span>
are exactly the natural exponential families. In particular, the
follow results follow from straightforward calculation:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-101" class="theorem"><strong>Theorem 11.8  (properties of NEF) </strong></span></p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\psi_{P_\lambda}(u) = \psi_X(\lambda + u) - \psi_X(\lambda)\)</span>.</li>
<li>Tilting trades mean for divergence:
<span class="math display">\[
\lambda&gt;0\implies \mathbb E_{P_\lambda}[X] = \psi&#39;_X(\lambda) \geq \mathbb E_P[X]
\]</span>
The same is true with <span class="math inline">\(&gt;\)</span> replaced by <span class="math inline">\(&lt;\)</span>. In terms of divergence,
<span class="math display">\[
D(P_\lambda \|P) = (\psi_X^*\circ \psi_X&#39;)(\lambda)
= \psi_X^*(\mathbb E_{P_\lambda}[X])
\]</span></li>
<li><span class="math inline">\(\mathrm{Var}_{P_\lambda}(X) = \psi_X&#39;&#39;(\lambda)\log e\)</span>.</li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:ldeInfoProj" class="theorem"><strong>Theorem 11.9  (large-deviation estimate is related to information projection) </strong></span>Given <span class="math inline">\(X_1, \dots\sim P\)</span> i.i.d, then for any <span class="math inline">\(\gamma\in \mathbb R\)</span>:
<span class="math display">\[
    \lim_{n\to \infty} \dfrac 1 n \log \dfrac 1 {P[\bar X &gt; \gamma]}
    = \inf_{Q:\mathbb E_Q[X]&gt;\gamma} D(Q\|P)
\]</span>
The same holds for <span class="math inline">\(&gt;\mapsto \geq\)</span>. For every <span class="math inline">\(n\)</span> we obtain the firm upper bound
<span class="math display">\[
    P[\bar X_n \geq \gamma] \leq \exp \left[-n \inf_{\mathbb E_Q[X]\geq \gamma} D(Q\|P)\right].
\]</span></p>
</div>
<details>
<summary>
Proof (strict deviation case)
</summary>
<p>The inequalities hold trivially if events have zero probability.
Let <span class="math inline">\(P[E_n]\)</span> be the probability that the empirical mean with <span class="math inline">\(n\)</span>
samples is greater than <span class="math inline">\(\gamma\)</span>.</p>
To prove the lower bound, fix <span class="math inline">\(Q\)</span> such that <span class="math inline">\(\mathbb E_Q[X]&gt;\gamma\)</span>.
Start by applying DPI
<span class="math display">\[\begin{align}
    -\log 2 + Q[E_n] \log \dfrac 1 {P[E_n]}
    &amp;\leq d(Q[E_n]\|P[E_n]) \leq D(Q_{X^n}\| P_{X^n}) = nD(Q\|P) \\
    \log P[E_n]
    &amp;\leq \dfrac{-nD(Q\|P) - \log 2}{Q[E_n]}
\end{align}\]</span>
Also note that by WLLN, <span class="math inline">\(Q[E_n] = 1 - o(1)\)</span>.
Optimize over <span class="math inline">\(Q\)</span> to obtain (here <span class="math inline">\(\limsup\)</span> creeps in
in order to combat the <span class="math inline">\(-\log2\)</span> and <span class="math inline">\(Q[E_n]\approx 1\)</span> factors.
<span class="math display">\[
    \limsup_{n\to \infty}
    \dfrac 1 n \log \dfrac 1 {P[E_n]}
    \leq \inf_{\mathbb E_Q[X]&gt;\gamma} D(Q\|P)
\]</span>
To prove the upper bound, crucially note that for any event <span class="math inline">\(E\)</span>
with <span class="math inline">\(P_X[E]&gt;0\)</span>, we have
<span class="math inline">\(\log \frac 1 {P_X(E)} = D(P_{X|X\in E} \| P_X)\)</span>.
Define <span class="math inline">\(\tilde P_{X^n} = P_{X^n|\bar X&gt;\gamma}\)</span>, then
<span class="math display">\[
    \log \dfrac 1 {P[E_n]} = D(\tilde P_{X^n} \| P_{X^n})
    = \inf_{\mathbb E_{Q^n}[\bar X]&gt;\gamma} D(Q_{X^n} \| P_{X^n})
\]</span>
The last infimum problem tensorizes by the tensorization
property of products in the second argument and convexity
in the first argument in the KL-divergence:
<span class="math display">\[
    D(Q_{X^n} \| P_{X^n})
    \geq \sum_{j=1}^n D(Q_{X_j} \| P) \geq nD(\bar Q \| P)
\]</span>
This yields the desired inequality
<span class="math display">\[
    \log \dfrac 1 {P[E_n]}
    = \inf_{\mathbb E_{Q^n}[\bar X]&gt;\gamma} D(Q_{X^n} \| P_{X^n})
    = n\inf_{\mathbb E_Q[X]\geq \gamma} D(Q\|P)
\]</span>
</details>
<p>The previous part motivates the study of the problem
<span class="math inline">\(\inf_{Q\in \mathcal E} D(Q\|P)\)</span> where <span class="math inline">\(\mathcal E\subset \Omega\)</span>
is a convex subset of distributions.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-102" class="theorem"><strong>Theorem 11.10  (orthogonality of information projection) </strong></span>Given a convex set <span class="math inline">\(\mathcal E\)</span> of distributions.
If there exists <span class="math inline">\(Q^*\in \mathcal E\)</span> such that
<span class="math inline">\(D(Q^* \| P) = \min_{Q\in \mathcal E} D(Q\|P)&lt;\infty\)</span>,
then <span class="math inline">\(\forall Q\in \mathcal E\)</span>, we obtain
<span class="math display">\[
    D(Q\|P) \geq D(Q\|Q^*) + D(Q^* \| P)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Consider the nontrivial case <span class="math inline">\(D(Q^* \| P) \leq D(Q\|P)&lt;\infty\)</span>.
For <span class="math inline">\(\lambda\in [0, 1]\)</span>, consider the convex combination
<span class="math inline">\(Q^{(\lambda)} = \bar \lambda Q^* + \lambda Q\)</span>. By <span class="math inline">\(Q^*\)</span>
being the minimizer of <span class="math inline">\(D(Q\|P)\)</span>, we obtain
<span class="math display">\[
    0 \leq \dfrac d {d\lambda}\bigg|_{\lambda=0} D(Q^{(\lambda)} \| P)
    = D(Q\|P) - D(Q\|Q^*) - D(Q^* \| P)
\]</span>
</details>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lossless-compression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture-notes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
