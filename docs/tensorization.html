<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Tensorization | 6.7480 Notes</title>
  <meta name="description" content="7 Tensorization | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Tensorization | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Tensorization | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="extremization.html"/>
<link rel="next" href="f-divergence.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html"><i class="fa fa-check"></i><b>12</b> Noisy Channel Coding</a>
<ul>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#optimal-decoder-weak-converse"><i class="fa fa-check"></i>Optimal decoder, weak converse</a></li>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#random-and-maximal-coding"><i class="fa fa-check"></i>Random and maximal coding</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>13</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tensorization" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Tensorization<a href="tensorization.html#tensorization" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ol style="list-style-type: decimal">
<li>Theorem <a href="tensorization.html#thm:miTensorization">7.1</a>
states that the MI-maximizing input for product channel
is product input, and the MI-minimizing channel
for product input is the product channel.</li>
<li>Gaussian saddle point problem: solve a <span class="math inline">\(n=1\)</span> problem by going to
<span class="math inline">\(n&gt;1\)</span> problem and use convexity / concavity of information measures
and symmetry to obtain the optimal solution.</li>
<li>Stationary processes have an uppder limit on entropy rate
(theorem <a href="tensorization.html#thm:stationaryProperties">7.3</a>).</li>
<li>Entropy rate of HMM has a sandwich limit (proposition <a href="tensorization.html#prp:hmmEntropyRate">7.1</a>).</li>
</ol>
<div id="mi-gaussian-saddle-point" class="section level2 unnumbered hasAnchor">
<h2>MI, Gaussian saddle point<a href="tensorization.html#mi-gaussian-saddle-point" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:miTensorization" class="theorem"><strong>Theorem 7.1  (MI behavior) </strong></span></p>
<ol style="list-style-type: decimal">
<li>For a memoryless channel <span class="math inline">\(P_{Y^n|X^n} = \prod P_{Y_j|X_j}\)</span>
<span class="math display" id="eq:tensorChannel">\[
I(X^n; Y^n) \leq \sum_{j=1}^n I(X_j; Y_j)
\tag{7.1}
\]</span>
with equality iff <span class="math inline">\(P_{Y^n} = \prod P_{Y_j}\)</span>.</li>
<li>The (unconstrained) capacity is additive for memoryless channels:
<span class="math display">\[
\max_{P_X^n} I(X^n; Y^n) = \sum_{j=1}^n \max_{P_{X_j}} I(X_j; Y_j)
\]</span></li>
<li>If the source is memoryless <span class="math inline">\(P_{X^n} = \prod P_{X_j}\)</span>, then
<span class="math display" id="eq:tensorInput">\[
I(X^n; Y) \geq \sum_{j=1}^n I(X_j; Y)
\tag{7.2}
\]</span>
with equality iff <span class="math inline">\(P_{X^n|Y} = \prod P_{X_j|Y}\)</span> almost-surely
under <span class="math inline">\(P_Y\)</span>.</li>
<li><span class="math inline">\(\min_{P_{Y^n|X^n}} I(X^n; Y^n) = \sum_j  \min_{P_{Y_j|X_j}} I(X_j; Y_j)\)</span>.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
For (1), recall definition <a href="mutual-information.html#def:mutInfDef">4.1</a> for MI,
introduce an additional product and cross-identify the terms
<span class="math display">\[\begin{align}
    I(X^n; Y^n)
    &amp;= D(P_{X^nY^n} \| P_{X^n} P_{Y^n})
    = D\left(\prod P_{Y_j|X_j}\| P_{Y^n} | P_{X^n}\right) \\
   &amp;= \mathbb E_{X^n} \left[
        \log \dfrac{P_{Y^n|X^n}}{P_{Y^n}} \cdot \dfrac{\prod P_{Y_j}}{\prod P_{Y_j}}
   \right] = \mathbb E
    \log \prod \dfrac{P_{Y^j|X^j}}{P_{Y_j}}
    - \mathbb E\log \dfrac{P_{Y^n}}{P_{Y_j}} \\
    &amp;= \sum_{j=1}^n I(X_j; Y_j) - D\left(P_{Y^n} \| \prod P_{Y_j}\right)
\end{align}\]</span>
(1) naturally implies (2). To show (3), again use the
product condition and cross-identify terms
<span class="math display">\[\begin{align}
    I(X^n; Y)
    &amp;= \mathbb E_{X^n, Y} \log \dfrac{P_{X^n|Y}}{P_{X^n}}
    \cdot \prod \dfrac{P_{X_j|Y}}{P_{X_j|Y}} \\
    &amp;= \mathbb E\log \prod \dfrac{P_{X_j|Y}}{P_{X_j}}
    + \mathbb E\log \dfrac{P_{X^n|Y}}{\prod P_{X_j|Y}} \\
    &amp;= \sum_{j=1}^n I(X_j; Y) + D\left(X^n \| \prod P_{X_j} | Y\right)
\end{align}\]</span>
Now (3) implies (4), since the saturation condition and
memoryless source implies that the channel is memoryless.
</details>
<p>Several examples to the theorem above:</p>
<ol style="list-style-type: decimal">
<li><u>Non-product input for non-product channels</u>:
Let <span class="math inline">\(X_1\perp\!\!\!\perp X_2\sim \mathrm{Ber}(1/2)\)</span> and <span class="math inline">\(Y_1=X_1+X_2, Y_2=X_1\)</span>,
then <span class="math inline">\(I(X_1; Y_1)=I(X_2; Y_2)=0\)</span> but <span class="math inline">\(I(X_1, X_2; Y_1, Y_2)=2\)</span>.</li>
<li><u>Strict inequality for 7.1.1</u>:
for all inputs equal <span class="math inline">\(Y_k=X_k=U\sim \mathrm{Ber}(1/2)\)</span>,
we have <span class="math inline">\(I(X_k; Y_k)=I(X^n; Y^n)=1\leq n\)</span>.</li>
<li><u>Strict inequality for 7.1.3</u>:
Let <span class="math inline">\(X_1 \perp\!\!\!\perp X_2\sim \mathrm{Ber}(1/2)\)</span> and <span class="math inline">\(Y=X_1\oplus X_2\)</span>.</li>
</ol>
<div class="theorem">
<p><span id="thm:unlabeled-div-45" class="theorem"><strong>Theorem 7.2  (extremality of Gaussian channels) </strong></span>Let <span class="math inline">\(X_g\sim \mathcal N(0, \sigma_X^2), N_g\sim \mathcal N(0, \sigma_N^2)\perp\!\!\!\perp X_g\)</span>, then</p>
<ol style="list-style-type: decimal">
<li><em>Gaussian capacity:</em>
<span class="math display">\[
C = I(X_g; X_g + N_g) = \dfrac 1 2 \log \left(
     1 + \dfrac{\sigma_X^2}{\sigma_N^2}
\right)
\]</span></li>
<li><em>Gaussian input is the caid for Gaussian noise:</em> under
the power constraint <span class="math inline">\(\mathrm{Var}(X) \leq \sigma_X^2\)</span> and <span class="math inline">\(X\perp\!\!\!\perp N_g\)</span>
<span class="math display">\[
I(X; X+N_g) \leq I(X_g; X_g+N_g)
\]</span>
with equality saturated iff <span class="math inline">\(X=X_g\)</span> (in distribution).</li>
<li><em>Gaussian noise is the worst for Gaussian input:</em> under
the power constraint <span class="math inline">\(\mathbb E[N^2] \leq \sigma_N^2\)</span> and <span class="math inline">\(\mathbb E[X_gN] = 0\)</span>
<span class="math display">\[
I(X_g; X_g+N) \geq I(X_g; X_g+N_g)
\]</span>
with equality iff <span class="math inline">\(N=N_g\)</span> (in distribution and <span class="math inline">\(N\perp\!\!\!\perp X_g\)</span>.</li>
</ol>
</div>
<details>
<summary>
Proof by direct calculation
</summary>
<p>Define <span class="math inline">\(Y_g=X_g+N_g\)</span> and the conditional divergence
<span class="math display">\[\begin{align}
    f(x) &amp;= D(P_{Y_g|X_g=x} \| P_{Y_g})
    = D(\mathcal N(x, \sigma_N^2) \| \mathcal N(0, \sigma_X^2 + \sigma_N^2)) \\
    &amp;= \dfrac 1 2 \log \left(1 + \dfrac{\sigma_X^2}{\sigma_N^2}\right)
    + \dfrac{\log e}{2} \dfrac{x^2 - \sigma_X^2}{\sigma_X^2 + \sigma_N^2}
\end{align}\]</span></p>
<ol style="list-style-type: decimal">
<li>By definition <a href="mutual-information.html#def:mutInfDef">4.1</a> of mutual information,
<span class="math inline">\(I(X_g; X_g+N_g) = \mathbb E[f(X_g)] = C\)</span>.</li>
<li>By center of gravity formula <a href="variational-characterizations.html#cor:centerOfGravityMI">5.1</a>,
<span class="math display">\[
I(X; X+N_g) \leq D(P_{Y|X} \| P_{Y_g} |P_X) \leq C
\]</span>
By the uniqueness of caod <a href="extremization.html#cor:caodUniqueness">6.1</a>,
we have <span class="math inline">\(P_Y=P_{Y_g}\implies X\sim \mathcal N(0, \sigma_X^2)\)</span>.</li>
<li>Let <span class="math inline">\(P_{Y|X_g}\)</span> denote the kernel <span class="math inline">\(Y=X_g+N\)</span>, apply theorem
<a href="variational-characterizations.html#thm:miLowerBound">5.4</a>, Baye’s formula
<span class="math display">\[\begin{align}
I(X_g; Y=X_g+N)
&amp;\geq \mathbb E\log \dfrac{dP_{X_g|Y_g}(X_g|Y)}{dP_{X_g}(X_g)}
= \mathbb E\log \dfrac{dP_{Y_g|X_g}(Y|X_g)}{dP_{Y_g}(Y)} \\
&amp;= C + \dfrac{\log e}{2} \mathbb E\left[
     \dfrac{Y^2}{\sigma_X^2 + \sigma_N^2}
     - \dfrac{N^2}{\sigma_N^2}
\right]
\end{align}\]</span>
The first inequality is saturated when <span class="math inline">\(P_{X_g|Y_g} = P_{Y|X_g}\)</span>;
the second term implies that <span class="math inline">\(X_g\)</span> must be conditionally Gaussian.
This is enough to imply that <span class="math inline">\(Y\)</span> is Gaussian.</li>
</ol>
</details>
<details>
<summary>
Proof by symmetry and tensorization
</summary>
<p>Proof of (1), (2): suppose we wish to solve, for <span class="math inline">\(Z^n\sim \mathcal N(0, I_n)\)</span>,
the following equation. Apply equation <a href="tensorization.html#eq:tensorChannel">(7.1)</a> to obtain
<span class="math display">\[
    \max_{\mathbb E[\sum X_k^2]\leq n \sigma_X^2} I(X^n; X^n+Z^n)
    = \max_{\mathbb E[\sum X_k^2]\leq n \sigma_X^2} \sum_{k=1}^n I(X_k; X_k+Z_k)
\]</span>
Next apply an averaging argument:
given a distribution <span class="math inline">\(P_{X^n}\)</span>, the average of its marginals
<span class="math inline">\(\bar P_X = \frac 1 n \sum P_{X_k}\)</span>
will also satisfy the constraint and
yield larger mutual information by the concavity
of <span class="math inline">\(I(P_X, P_{Y|X})\)</span> in <span class="math inline">\(P_X\)</span> (theorem <a href="extremization.html#thm:MIextremality">6.3</a>).
Thus
<span class="math display">\[
    \max_{\mathbb E[\sum X_k^2]\leq n \sigma_X^2} I(X^n; X^n+Z^n)
    = n \max_{\mathbb E[X^2] \leq \sigma_X^2} I(X; X+Z)
\]</span>
Next, for any orthogonal transformation <span class="math inline">\(U\in O(n)\)</span>, we have
<span class="math display">\[
    I(P_{X^n}, P_{Y^n|X^n}) = I(P_{UX^n}, P_{UY^n|UX^n})
    = I(P_{UX^n}, P_{Y^n|X^n})
\]</span>
Averaging over all orthogonal rotations <span class="math inline">\(U\)</span> can only
make the mutual information larger, and the optimal input distribution
can be chosen to be invariant under orthogonal transformations.
The only product distribution satisfying power constraints and
rotational symmetry is the isotropic Gaussian, so
<span class="math inline">\(P_{Y^n} = \mathcal N(0, 1+\sigma_X^2)^{\otimes n}\)</span>.
This also determines <span class="math inline">\(P_{X^*}\)</span> uniquely.</p>
Proof of (3): apply equation <a href="tensorization.html#eq:tensorInput">(7.2)</a> and use
the convexity of <span class="math inline">\(P_{Y|X}\mapsto I(P_X; P_{Y|X})\)</span>
to argue that the channel additive noise <span class="math inline">\(N\)</span> must
be additive and rotationally symmetric.
</details>
</div>
<div id="information-rates" class="section level2 unnumbered hasAnchor">
<h2>Information rates<a href="tensorization.html#information-rates" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>See the <a href="#stochasticProcessPrelim">preliminaries</a>
section for more details on stochastic processes.</p>
<div class="definition">
<p><span id="def:entropyRate" class="definition"><strong>Definition 7.1  (entropy rate) </strong></span>The entropy rate of a
random process <span class="math inline">\(\mathbb X=(X_1, X_2, \cdots)\)</span> is
<span class="math display">\[
    H(\mathbb X) = \lim_{n\to \infty} \dfrac 1 n H(X^n)
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:stationaryProperties" class="theorem"><strong>Theorem 7.3  (properties of stationary processes) </strong></span>Given <span class="math inline">\(\mathbb X\)</span> stationary,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H(X_n|X^{n-1}) \leq H(X_{n-1}|X^{n-2})\)</span>.</li>
<li><span class="math inline">\(H(X_n|X^{n-1}) \leq H(X^n)/n \leq H(X^{n-1})/(n-1)\)</span>.</li>
<li>The entropy rate exists
<span class="math inline">\(H(\mathbb X) = \lim_{n\to \infty} H(X^n)/n  = \lim_{n\to \infty} H(X_n|X^{n-1})\)</span>
by approximation from above.</li>
<li>If <span class="math inline">\(\mathbb X\)</span> can be extended to a <span class="math inline">\(\mathbb Z\)</span>-indexed stationary process, then
<span class="math inline">\(H(\mathbb X) = H(X_1|X^0_{-\infty})\)</span> provided <span class="math inline">\(H(X_1)&lt;\infty\)</span>.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
For (1), by conditioning and stationarity
<span class="math display">\[
    H(X_n|X^{n-1}) \leq H(X_n|X_2^{n-1}) = H(X_{n-1}, X^{n-2})
\]</span>
For (2), use the chain rule
<span class="math display">\[\begin{align}
    \dfrac 1 n H(X^n)
    &amp;= \dfrac 1 n \sum H(X_j|X^{j-1}) \geq H(X_n|X^{n-1}) \\
    H(X^n)
    &amp;= H(X^{n-1}) + H(X_n|X^{n-1}) \leq H(X^{n-1}) + \dfrac 1 n H(X^n)
\end{align}\]</span>
For (3), use the right inequality in (2) to argue that
<span class="math inline">\(H(X^n)/n\)</span> is a decreasing sequence lower-bounded by <span class="math inline">\(0\)</span>,
so the limit <span class="math inline">\(H(\mathbb X)\)</span> exists.
By the chain rule, the second sequence is the Cesaro’s mean
by <span class="math inline">\(H(X^n)/n = \sum H(X_j|X^{j-1})/n\)</span>.
For (4), use the limit definition of <span class="math inline">\(H(\mathbb X)\)</span> and pass the
limit through the monotone convergence of mutual information:
<span class="math display">\[
    \lim_{n\to \infty} H(X_1) - H(X_1|X_{-n}^0)
    = \lim_{n\to \infty} I(X_1; X_{-n}^0) = I(X_1; X_{-\infty}^0)
    = H(X_1) - H(X_1|X_{-\infty}^0)
\]</span>
</details>
<p>Examples of stationary processes:</p>
<ol style="list-style-type: decimal">
<li>Memoryless source (i.i.d)</li>
<li>A mixed source: given two stationary sources
<span class="math inline">\(\mathbb X, \mathbb Y\)</span> and flip a single biased coin to set
<span class="math inline">\(\mathbb Z=\mathbb X\)</span> or <span class="math inline">\(\mathbb Y\)</span>.</li>
<li>Stationary Markov process: let <span class="math inline">\(\mathbb X\)</span> be a Markov
chain <span class="math inline">\(X_1\to X_2\to \cdots\)</span> with transition
kernel <span class="math inline">\(K(b|a)\)</span> and initialized with an invariant
distribution <span class="math inline">\(X_1\sim \mu\)</span> so that <span class="math inline">\(X_2\sim \mu\)</span>.
Then <span class="math inline">\(H(X_n|X^{n-1}) = H(X_n|X_{n-1}\)</span> and
<span class="math display">\[
     H(\mathbb X) = H(X_2|X_1)
     = \sum_{ab}\mu(a)K(b|a) \log \dfrac 1 {K(b|a)}
\]</span></li>
</ol>
<div class="definition">
<p><span id="def:unlabeled-div-46" class="definition"><strong>Definition 7.2  (Hidden Markov Model (HMM)) </strong></span>Given a stationary Markov chain <span class="math inline">\(\cdots, S_{-1}, S_0, S_1, \cdots\)</span>
on state space <span class="math inline">\(\mathcal S\)</span> and a Markov kernel <span class="math inline">\(P_{X|S}:\mathcal S\to \mathcal X\)</span>,
a HMM is a stationary process <span class="math inline">\(\cdots, X_{-1}, X_0, X_1, \cdots\)</span>
which is an observation of <span class="math inline">\(\mathbb S\)</span> with a stationary
memoryless channel (emission channel) <span class="math inline">\(P_{X_1|S_1}\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:hmmEntropyRate" class="proposition"><strong>Proposition 7.1  (entropy rate of HMM) </strong></span>Given a HMM process <span class="math inline">\(\mathbb X\)</span> with state process <span class="math inline">\(\mathbb S\)</span>, we have
<span class="math display">\[
    H(X_n|X_1^{n-1}, S_0) \leq H(\mathbb X)\leq H(X_n|X_1^{n-1})
\]</span>
Both sides converge monotonically as <span class="math inline">\(n\to \infty\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
The upper bound is established by theorem <a href="tensorization.html#thm:stationaryProperties">7.3</a>.
To establish the lower bound, consider
<span class="math display">\[\begin{align}
    H(\mathbb X)
    &amp;= H(X_n | X_{-\infty}^{n-1})
    \geq H(X_n | X_{-\infty}^{n-1}, S_0)
    = H(X_n|X_1^{n-1}, S_n)
\end{align}\]</span>
the last equality follows by the HMM assumption. To show that the sequence
is increasing,
<span class="math display">\[\begin{align}
    H(X_{n+1} | X_1^n, S_0)
    &amp;= H(X_n | X_0^{n-1}, S_{-1})
    \geq H(X_n | X_0^{n-1}, S_{-1}, S_0) \\
    &amp;= H(X^n | X_1^{n-1}, S_0)
\end{align}\]</span>
Finally, to show that this converges to the correct limit,
use the mutual information chain rule
(theorem <a href="mutual-information.html#thm:mutInfoMoreProperties">4.2</a>)
to identify the two limits. The equation goes to zero by
<span class="math inline">\(I(S_0; X_1^\infty) = I(S_0; X_1^{n\to \infty}) \leq H(S_0) = \infty\)</span>.
<span class="math display">\[\begin{align}
    I(S_0; X_1^n) - I(S_0; X_1^{n-1})
    &amp;= I(X_n; S_0 | X_1^{n-1}) \\
    &amp;= H(X_n|X_1^{n-1}) - H(X_n | X_1^{n-1}, S_0)\to 0
\end{align}\]</span>
</details>
<div class="definition">
<p><span id="def:miRate" class="definition"><strong>Definition 7.3  (mutual information rate) </strong></span>The mutual information rate between two processes
<span class="math inline">\((\mathbb X, \mathbb Y)\)</span> is
<span class="math display">\[
    I(\mathbb X, \mathbb Y) = \lim_{n\to \infty} \dfrac 1 n I(X^n; Y^n)
\]</span></p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="extremization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="f-divergence.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
