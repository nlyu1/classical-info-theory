<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.7480 Notes</title>
  <meta name="description" content="6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-11-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="entropy.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>12</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">6.7480 Notes</h1>
<p class="author"><em>Nicholas Lyu</em></p>
<p class="date"><em>2024-11-06</em></p>
</div>
<div id="preface" class="section level1 unnumbered hasAnchor">
<h1>Preface<a href="index.html#preface" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="recurring-themes" class="section level2 unnumbered hasAnchor">
<h2>Recurring themes<a href="index.html#recurring-themes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Fundamentally, information and entropy captures
how our uncertainty in a quantity changes after
observations. Some recurring themes in the book are:</p>
<ol style="list-style-type: decimal">
<li>Fundamental properties of information measures (e.g. entropy, mutual information, divergence,
Fisher information) are:
<ul>
<li>Nonnegativity.</li>
<li>Monotonicity: joint provides more information than marginals.
For especially well-behaved ones (entropy, KL, Fisher information) we expect
additive decomposition, i.e. chain rule.</li>
<li>Data-processing inequality.</li>
</ul></li>
<li>Divergence as a fundamental concept in information theory.
<ol style="list-style-type: lower-alpha">
<li>Quantifies difference between distributions;
the choice of this quantification depends on our problem.</li>
<li>Divergence gives rise to mutual information; monotonicity yields DPI.</li>
</ol></li>
<li><strong>Convexity</strong> of <span class="math inline">\(f\)</span> for <span class="math inline">\(f\)</span>-divergence is mathematically equivalent
to our information intuitions: data-processing inequality, monotonicity, etc.
<ol style="list-style-type: lower-alpha">
<li>KL-divergence (and Rényi) is special because its <span class="math inline">\(\log\)</span> form
admits additive decomposition of joint divergence.</li>
</ol></li>
<li>Convexity of information measures correspond to <strong>variational characterizations</strong>,
which are extremely useful because:
<ol style="list-style-type: lower-alpha">
<li>They provide bounds: choose the varied quantity to be our friend!</li>
<li>Provide tractable variational approximations
using numerical optimization methods (e.g. VAE, GAN).</li>
<li>Combined with tensorization and convexity properties, such
results yield powerful asymptotics for high-dimensional problems.</li>
</ol></li>
<li>Mutual information as information radius, or center of gravity; capacity as a minimax saddle point.</li>
<li>Divergence measures are extremely useful for bounding events:
bound the event for a tractable distribution, bound the divergence between the tractable
and given distribution, and we can bound the event for the given distribution.</li>
<li>Operationally, entropy <span class="math inline">\(H\)</span> is the answer to compression, mutual information
<span class="math inline">\(I\)</span> is the answer to universal compression, and divergence <span class="math inline">\(D\)</span> is the
answer to hypothesis testing.</li>
<li>The Sanov principle establishes a fundamental bridge between probability
and divergence.</li>
</ol>
<p>Nontrivial results in this book include:</p>
<ol style="list-style-type: decimal">
<li><u>Golden formula</u>: variational characterization of mutual information.</li>
<li><u>Saddle point characterization of divergence</u>.</li>
<li><u>Donsker-Varadhan</u> (theorem <a href="variational-characterizations.html#thm:donskerVaradhan">5.5</a>):
variational characterization of KL, extended to <span class="math inline">\(f\)</span>-divergences
in theorem <a href="f-divergence.html#thm:fVar">8.9</a>.</li>
<li><u>Harremoës-Vadja</u> (theorem <a href="f-divergence.html#thm:harremoesVajda">8.8</a>):
one theorem to rule them all for <span class="math inline">\(f\)</span>-divergence joint range.</li>
<li>Most <span class="math inline">\(f\)</span>-divergences are locally <span class="math inline">\(\chi^2\)</span>-like about
<span class="math inline">\(\lim_{\lambda \to 0^+} D_f(\lambda P + \bar \lambda Q \| Q)\)</span>
and decays quadratically (theorem <a href="f-divergence.html#thm:localChiExpansion">8.11</a>).</li>
<li><u>Kraft-McMillan</u> (theorem <a href="lossless-compression.html#thm:kraftMcMillan">10.3</a>):
constructive bijection between inequality-satisfying code
lengths and prefix-free codes.</li>
<li><u>Duality between Bayes and minimax risk</u> (proposition <a href="statistical-decision-applications.html#prp:minimaxSeparation">9.1</a>):
at the heart of every minimax theorem is the hyperplane separation theorem.</li>
<li><u>van-Trees inequality</u> (theorem <a href="statistical-decision-applications.html#thm:vanTrees">9.5</a>):
using the variational <span class="math inline">\(\chi^2\)</span> formula, obtain information bound
for general (not just unbiased) estimators.</li>
<li><u>Large-sample mean deviation reduces to information projection</u>
(theorem <a href="hypothesis-testing-large-deviations.html#thm:ldeInfoProj">11.9</a>).</li>
</ol>
</div>
<div id="insightful-proof-techniques" class="section level2 unnumbered hasAnchor">
<h2>Insightful proof techniques<a href="index.html#insightful-proof-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Asymptotic analysis proofs: rewrite a difference as an integral, then
apply the monotone convergence theorem.
<ul>
<li>Theorems <a href="f-divergence.html#thm:localFisherDiv">8.12</a>, <a href="f-divergence.html#thm:localChiExpansion">8.11</a>.</li>
</ul></li>
<li>Unique extremality proofs: reduce the desired inequality to
the information inequality (theorem <a href="kullback-leibler-divergence.html#thm:infoInequality">3.1</a>)
or a nonnegative information quantity (e.g. entropy).
<ul>
<li>Corollary <a href="kullback-leibler-divergence.html#cor:minimalCE">3.1</a>, theorem <a href="kullback-leibler-divergence.html#thm:gaussianCovExtremality">3.3</a>,
lemma <a href="lossless-compression.html#lem:finiteExpectationExtremality">10.2</a>; the latter two
characterized Gaussian and geometric distributions.</li>
<li>Occasionally we need to introduce a tilted distribution based on
both inputs, see Donsker-Varadhan (<a href="variational-characterizations.html#thm:donskerVaradhan">5.5</a>) or</li>
</ul></li>
<li>Bounds on conditional quantities: consider the joint quantity
and decompose in <span class="math inline">\(2\)</span> different ways.
<ul>
<li>KL-DPI theorem <a href="kullback-leibler-divergence.html#thm:klDPI">3.5</a>.</li>
</ul></li>
<li>Meta-converse: instead of studying <span class="math inline">\(P_{X\hat X}\)</span> for
arbitrary <span class="math inline">\(\hat X\)</span>, introduce a tractable <span class="math inline">\(Q_{X\hat X}\)</span>,
to bound an event, then use DPI (or divergence inequalities like Donsker-Varadhan)
to bound the same event for <span class="math inline">\(P_{X\hat X}\)</span> together with some
divergence quantity.
<ul>
<li>Fano’s inequality <a href="mutual-information.html#thm:simpleFano">4.3</a>.</li>
</ul></li>
<li>Convexity proofs: introduce an additional parameter
<span class="math inline">\(\theta \sim \mathrm{Ber}(\lambda)\)</span> to rewrite
<span class="math inline">\(\lambda f(x)+\bar \lambda f(y)\)</span> as a conditional quantity,
then use decomposition.
<ul>
<li>See section on <a href="extremization.html#extremizationConvexity">convexity</a>.</li>
</ul></li>
<li>Counting bounds: to bound <span class="math inline">\(|\mathcal C|\)</span>, draw an element uniformly from <span class="math inline">\(\mathcal C\)</span>,
then upper-bound the joint-entropy by marginal bound, Han’s inequality, or chain rule.</li>
<li>Typicality methods (compression): break region into tractable region of
large probability and difficult region with low probability.
<ul>
<li><span class="math inline">\(\mathrm{Pr}[A]= \mathrm{Pr}[A, B] + \mathrm{Pr}[A, B^c] \leq \mathrm{Pr}[B] + \mathrm{Pr}[A, B^c]\)</span> then bound
<span class="math inline">\(\mathrm{Pr}[A, B^c]\)</span> through atypicality of <span class="math inline">\(B^c\)</span>:
soure coding distribution theorem <a href="lossless-compression.html#thm:varCompOptDistribution">10.2</a>.</li>
</ul></li>
<li>Break a single <span class="math inline">\(\sup\)</span> into nested ones.
<ul>
<li>Solve the inner <span class="math inline">\(\sup\)</span> analytically after introducing a redundant
linear (in fact any parameterized) bijective transform:
useful characterization of <span class="math inline">\(\chi^2\)</span> <a href="f-divergence.html#eq:chiStatVar">(8.5)</a> and
KL (proposition <a href="f-divergence.html#prp:klDV">8.12</a>).</li>
<li>Alternate inner and alter closed-form solution: EM algorithm.</li>
</ul></li>
<li>Large-sample asymptotic proofs: relate to an information measure,
use their tensorization property and convexity to reduce to the
single-letter case (theorem <a href="hypothesis-testing-large-deviations.html#thm:ldeInfoProj">11.9</a>).</li>
</ol>
</div>
<div id="administrative-trivia" class="section level2 unnumbered hasAnchor">
<h2>Administrative trivia<a href="index.html#administrative-trivia" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Professor email: <code>yp@mit.edu</code>.</p>
<p>Office Hours: LIDS (<span class="math inline">\(6\)</span>th floor of <span class="math inline">\(D\)</span>-tower, lobby).</p>
</div>
<div id="motivation" class="section level2 unnumbered hasAnchor">
<h2>Motivation<a href="index.html#motivation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Why is training GPT <span class="math inline">\(\iff\)</span> building a great text compressor?</li>
<li>Given <span class="math inline">\(2\)</span> families of distributions
<span class="math inline">\(P_\theta, \theta\in \Theta_1, \Theta_2\)</span>.
To have <span class="math inline">\(P_{\mathrm{err}} \leq \delta\)</span>, the number of samples
is asymptotically
<span class="math inline">\(\left(\min D(P_{\theta_1}\|P_{\theta_2})\right)^{-1} \log(1/\delta)\)</span></li>
<li>Noisy channel coding.</li>
<li>Cramer-Rau bound
<span class="math display">\[
\min_{\hat \theta}
\max_{\theta\in \Theta} \mathbb E[(\theta - \hat \theta^2]
\asymp \dfrac 1 {n\min \mathcal I_F(\theta)}
\]</span></li>
<li>How many bits to represent a random function / signal / picture
with fidelity <span class="math inline">\(\epsilon\)</span>.</li>
</ol>
<p>Key concepts: entropy, KL-divergence, mutual information,
fisher information, mutual information.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="entropy.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
