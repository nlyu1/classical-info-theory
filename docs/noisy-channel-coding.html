<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12 Noisy Channel Coding | 6.7480 Notes</title>
  <meta name="description" content="12 Noisy Channel Coding | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="12 Noisy Channel Coding | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12 Noisy Channel Coding | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-11-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hypothesis-testing-large-deviations.html"/>
<link rel="next" href="channel-capacity.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html"><i class="fa fa-check"></i><b>12</b> Noisy Channel Coding</a>
<ul>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#optimal-decoder-weak-converse"><i class="fa fa-check"></i>Optimal decoder, weak converse</a></li>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#random-and-maximal-coding"><i class="fa fa-check"></i>Random and maximal coding</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="channel-capacity.html"><a href="channel-capacity.html"><i class="fa fa-check"></i><b>13</b> Channel Capacity</a></li>
<li class="chapter" data-level="14" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>14</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-18-channel-coding-iii"><i class="fa fa-check"></i>Nov 18, Channel Coding III</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#decoding-with-constraint"><i class="fa fa-check"></i>Decoding with constraint</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-20.-quantization"><i class="fa fa-check"></i>Nov 20. Quantization</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#water-filling"><i class="fa fa-check"></i>Water-filling</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#metric-entropy"><i class="fa fa-check"></i>Metric-entropy</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-25-rate-distortion-theorem"><i class="fa fa-check"></i>Nov 25: Rate-distortion theorem</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#scalar-quantization"><i class="fa fa-check"></i>Scalar quantization</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#vector-quantization"><i class="fa fa-check"></i>Vector quantization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="noisy-channel-coding" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">12</span> Noisy Channel Coding<a href="noisy-channel-coding.html#noisy-channel-coding" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Main takeaways:</p>
<ol style="list-style-type: decimal">
<li>Weak converse (theorem <a href="noisy-channel-coding.html#thm:weakConverse">12.1</a>):
number of reliably transmittable bits (intractable,
combinatorial operational quantity) is upper-bounded
by mutual information (an information quantity).</li>
<li>The probabilistic method: if some expected attribute of random selections
is bounded by a certain value, then there exists an object whose attribute
is bounded.</li>
</ol>
<div id="optimal-decoder-weak-converse" class="section level2 unnumbered hasAnchor">
<h2>Optimal decoder, weak converse<a href="noisy-channel-coding.html#optimal-decoder-weak-converse" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-103" class="definition"><strong>Definition 12.1  </strong></span>Given a kernel <span class="math inline">\(P_{Y|X}:\mathcal X\to \mathcal Y\)</span>:</p>
<ul>
<li>Given natural number <span class="math inline">\(M\)</span>, a <span class="math inline">\(M\)</span>-code
is a pair of kernels <span class="math inline">\(f, g\)</span> (encoder, decoder)
with chain
<span class="math display">\[
  W\xrightarrow f X\xrightarrow {P_{Y|X}} Y
  \xrightarrow g \hat W
\]</span>
Here <span class="math inline">\(f:[M]\to \mathcal X\)</span> and <span class="math inline">\(g:\mathcal Y\to \mathcal M\cup \{e\}\)</span>.</li>
<li>The image <span class="math inline">\(f([M])\)</span> are codewords.</li>
<li><span class="math inline">\(\{D_{j\in [M]}=g^{-1}(i)\}\)</span> are the decoding regions.</li>
</ul>
</div>
<p>In the chain of random variables <span class="math inline">\(W\to X\to Y\to \hat W\)</span>,
they’re called original message, channel input, channel output,
and decoded message. We assume a uniform distribution on <span class="math inline">\(W\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-104" class="definition"><strong>Definition 12.2  (performance metrics) </strong></span>The <em>maximum error probability</em> <span class="math inline">\(P_{e, \mathrm{max}}(f, g)
=\max_{m\in [M]} \mathrm{Pr}[\hat w\neq m|W=m]\)</span>.
The <em>average error probability</em> <span class="math inline">\(P_e(f, g)=\mathrm{Pr}[W\neq \hat W]\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-105" class="definition"><strong>Definition 12.3  (code definitions) </strong></span>A code <span class="math inline">\((f, g)\)</span> is a <span class="math inline">\((M, \epsilon)\)</span> code for <span class="math inline">\(P_{Y|X}\)</span>
if <span class="math inline">\(P_e(f, g)\leq \epsilon\)</span>. Similar definition
for <span class="math inline">\((M, \epsilon)_{\mathrm{max}}\)</span> code.</p>
</div>
<ul>
<li><span class="math inline">\(M^*(\epsilon; P_{Y|X})\)</span> is the maximum <span class="math inline">\(M\)</span> such that
there exists <span class="math inline">\((M, \epsilon)\)</span>-code.</li>
<li><span class="math inline">\(\epsilon^*(M, P_{Y|X})\)</span> is the minimim <span class="math inline">\(\epsilon\)</span>
such that there exists a <span class="math inline">\((M, \epsilon)\)</span> code;
then <span class="math inline">\(\log_2M^*(\epsilon)\)</span> is the maximum number of
transmittable bits.</li>
</ul>
<p>Since the codewords are equiprobable, the decoder that
minimizes <span class="math inline">\(P_e\)</span> is the MAP decoder, this is since
<span class="math display">\[\begin{align}
    P_e = \sum_{y\in \mathcal Y} \mathrm{Pr}[Y=y]\mathrm{Pr}[\hat W\neq W|Y=y], \quad
    \mathrm{Pr}[\hat W\neq W|Y=y] = 1 - \mathrm{Pr}[g(y)= W|Y=y]
\end{align}\]</span>
and MAP maximizes <span class="math inline">\(\mathrm{Pr}[g(y)= W|Y=y]\)</span>.
Since codewords are equiprobable,
the MAP decoder is equivalent to the ML decoder.
<span class="math display">\[
    g^*(y) = \mathop{\mathrm{argmax}}_{m\in [M]} \mathrm{Pr}[W=m|Y=y]
    = \mathop{\mathrm{argmax}}_{m\in [M]} \mathrm{Pr}[Y=y|W=m]
\]</span><br />
As in the case of Bayesian risk, w.l.o.g.
we can restrict ourselves to deterministic encoder and
decoders: representing additional randomness using <span class="math inline">\(U\perp\!\!\!\perp W\)</span>,
we have <span class="math inline">\(\mathrm{Pr}[W\neq \hat W] = \mathbb E[\mathrm{Pr}[W\neq \hat W|U]]\)</span>.</p>
<p>Negative (non-existence) results are called converse, since
they usually follow an existence result by “Conversely, …”</p>
<div class="theorem">
<p><span id="thm:weakConverse" class="theorem"><strong>Theorem 12.1  (weak converse) </strong></span>For every <span class="math inline">\((M, \epsilon)\)</span>-code for <span class="math inline">\(P_{Y|X}\)</span>,
recall the binary entropy function <span class="math inline">\(h(x)=H(\mathrm{Ber}(x))\)</span>,
we obtain
<span class="math display">\[
    \log M \leq \dfrac{\sup_{P_X} I(X; Y) + h(\epsilon)}{1-\epsilon}
\]</span></p>
</div>
<p>Given a <span class="math inline">\(M\)</span>-code, compute <span class="math inline">\(P_Y=P_{Y|X}\circ P_X\)</span>, where <span class="math inline">\(P_X\)</span>
is uniform on the codewords. We can hypothesis-test
<span class="math inline">\(P_{XY}\)</span> versus <span class="math inline">\(P_XP_Y\)</span> using <span class="math inline">\(Z=1_{W\neq \hat W}\)</span>. Note
<span class="math display">\[
    W\perp\!\!\!\perp\hat W\implies P_XP_Y[Z=0] = \dfrac 1 M, \quad
    P_{XY}[Z=0] = 1 - \epsilon
\]</span>
Next apply DPI with the data-processor <span class="math inline">\(1_{W\neq \hat W}\)</span> to obtain
<span class="math display">\[
    D(P_{XY}\|P_XP_Y) \geq d(1-\epsilon\| 1 /M)
    \geq -h(\epsilon) - (1-\epsilon)\log M
\]</span>
The proof concludes by <span class="math inline">\(I(X, Y)\leq \sup_{P_X} I(X; Y)\)</span> and
monotonicity of <span class="math inline">\(p\mapsto h(p)/(1-p)\)</span>.</p>
</div>
<div id="random-and-maximal-coding" class="section level2 unnumbered hasAnchor">
<h2>Random and maximal coding<a href="noisy-channel-coding.html#random-and-maximal-coding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Fixing the encoder, choosing the decoder is equivalent
to solving a <span class="math inline">\(M\)</span>-ary HT problem. The problem of choosing the
encoder is then equivalent to choosing the best condition
with optimal <span class="math inline">\(M\)</span>-ary HT solutions.</p>
<p>New notation: consider independent pairs <span class="math inline">\((X, Y)\perp\!\!\!\perp(\bar X, \bar Y)\)</span>
with joint distribution
<span class="math display">\[
    P_{XY\bar X\bar Y}(a, b, \bar a, \bar b) = P_X(a)P_{Y|X}(b|a)
    P_X(\bar a)P_{Y|X}(\bar b|a)
\]</span>
Here <span class="math inline">\(X\)</span> is the codeword while <span class="math inline">\(\bar X\)</span> is the unsent codeword.</p>
<div class="definition">
<p><span id="def:unlabeled-div-106" class="definition"><strong>Definition 12.4  (information density) </strong></span>Let <span class="math inline">\(P_{XY}, P_XP_Y\ll \mu\)</span> with densities <span class="math inline">\(f(x, y)\)</span> and <span class="math inline">\(\bar f(x, y)\)</span>
respectively, define the information density
<span class="math display">\[
    i_{P_{XY}}(x; y) = \mathrm{Log}\dfrac{f(x, y)}{\bar f(x, y)}
\]</span>
For <span class="math inline">\(P_{XY}\ll P_XP_Y\)</span> we have <span class="math inline">\(i(x; y) = \log \dfrac{P_{XY}(x, y)}{P_X(x)P_Y(y)}\)</span> and
<span class="math inline">\(I(X; Y) = \mathbb E[i(x; y)]\)</span>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing-large-deviations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="channel-capacity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
