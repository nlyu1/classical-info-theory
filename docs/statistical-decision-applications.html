<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Statistical Decision Applications | 6.7480 Notes</title>
  <meta name="description" content="9 Statistical Decision Applications | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Statistical Decision Applications | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Statistical Decision Applications | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-12-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="f-divergence.html"/>
<link rel="next" href="lossless-compression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>12</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-18-channel-coding-iii"><i class="fa fa-check"></i>Nov 18, Channel Coding III</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#decoding-with-constraint"><i class="fa fa-check"></i>Decoding with constraint</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-20.-quantization"><i class="fa fa-check"></i>Nov 20. Quantization</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#water-filling"><i class="fa fa-check"></i>Water-filling</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#metric-entropy"><i class="fa fa-check"></i>Metric-entropy</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-25-rate-distortion-theorem"><i class="fa fa-check"></i>Nov 25: Rate-distortion theorem</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#scalar-quantization"><i class="fa fa-check"></i>Scalar quantization</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#vector-quantization"><i class="fa fa-check"></i>Vector quantization</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#dec-4-density-estimation"><i class="fa fa-check"></i>Dec 4: Density estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review-of-metric-entropy"><i class="fa fa-check"></i>Review of metric entropy</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#dec-9.-strong-dpi"><i class="fa fa-check"></i>Dec 9. Strong DPI</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#minimax-stats"><i class="fa fa-check"></i>Minimax stats</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#combinatorial-statistics"><i class="fa fa-check"></i>Combinatorial statistics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#dec-11-sdpi-distributed-estimation"><i class="fa fa-check"></i>Dec 11: SDPI, Distributed Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#spiked-wigner"><i class="fa fa-check"></i>Spiked Wigner</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#correlation-estimation"><i class="fa fa-check"></i>Correlation estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-decision-applications" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Statistical Decision Applications<a href="statistical-decision-applications.html#statistical-decision-applications" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li>A statistical model is a family <span class="math inline">\(\{P_\theta\}\)</span> of distributions indexed
by parameter values <span class="math inline">\(\theta \in \Theta\)</span>.</li>
<li><span class="math inline">\(R_\theta(\hat \theta)\)</span> is the expectation of loss over <span class="math inline">\(X\sim P_\theta\)</span>.</li>
<li>There are two ways turn <span class="math inline">\(R_\theta(\hat\theta)\)</span> into a scalar:
<ul>
<li>Eliminating <span class="math inline">\(\theta\)</span> with <span class="math inline">\(\sup_\theta\)</span>, then taking <span class="math inline">\(\inf\)</span>
over all estimators leads to the minimax risk <span class="math inline">\(R^*\)</span>.</li>
<li>Replacing <span class="math inline">\(\theta\)</span> by averaging over a prior <span class="math inline">\(\pi\)</span>,
taking <span class="math inline">\(\inf\)</span> over <span class="math inline">\(\hat \theta\)</span>, then taking <span class="math inline">\(\sup_\pi\)</span> adversarially
over all priors <span class="math inline">\(\pi\)</span> leads to the Bayes risk.</li>
</ul></li>
<li>The minimax and Bayes risks are related by <a href="minimaxDual">duality</a>.</li>
<li>Under mild assumptions, <span class="math inline">\(R^* = R^*_{\mathrm{Bayes}}\)</span>
(proposition <a href="statistical-decision-applications.html#prp:minimaxSeparation">9.1</a>).</li>
<li>Sample complexity <a href="statistical-decision-applications.html#def:sampleComplexity">9.3</a> is the number of samples
to achieve <span class="math inline">\(R^*_n&lt; \epsilon\)</span>.</li>
<li>Tensor product experiments <a href="statistical-decision-applications.html#def:tpExperiment">9.4</a>
have independent but not necessarily identically distributed observations;
theoreom <a href="statistical-decision-applications.html#thm:tpMinimax">9.3</a> yields a bound on its minimax risk.</li>
<li>Given i.i.d. observations,
an analogue of Cramer-Rao (theorem <a href="statistical-decision-applications.html#thm:minimaxLB">9.7</a>) still holds
even for biased estimators.</li>
</ol>
<div id="minimax-and-bayes-risks" class="section level2 unnumbered hasAnchor">
<h2>Minimax and Bayes risks<a href="statistical-decision-applications.html#minimax-and-bayes-risks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>A <strong>statistical model</strong> is a collection
<span class="math inline">\(\mathcal P = \{P_\theta:\theta \in \Theta\}\)</span> of distributions
over some measurable space <span class="math inline">\((\mathcal X, \Sigma)\)</span>.
Here <span class="math inline">\(\Theta\)</span> is the <strong>parameter space</strong>.</li>
<li>An <strong>estimand</strong> <span class="math inline">\(T(\theta)\)</span> has signature <span class="math inline">\(T:\Theta\to \mathcal Y\)</span>;
in the trivial case <span class="math inline">\(T\)</span> is just the identity.</li>
<li>An <strong>estimator</strong> (decision rule) has type <span class="math inline">\(\hat T:\Omega\to \hat {\mathcal Y}\)</span>.
The action space <span class="math inline">\(\hat {\mathcal Y}\)</span> does not have to be
the estimand space <span class="math inline">\(\mathcal Y\)</span> (e.g. when we’re estimating a confidence interval).
<ul>
<li><span class="math inline">\(\hat T\)</span> can be deterministic or randomized.</li>
</ul></li>
<li>To evaluate the estimator, the <strong>loss function</strong> <span class="math inline">\(l:\mathcal Y\times \hat{\mathcal Y}\to \mathbb R\)</span>
defines the <strong>risk</strong> of <span class="math inline">\(\hat T\)</span> for estimating <span class="math inline">\(T\)</span>.</li>
<li>The (expected) risk of the estimator <span class="math inline">\(\hat T\)</span> for estimating <span class="math inline">\(T\)</span> is
<span style="color:green">a function of the true parameter <span class="math inline">\(\theta\)</span>
and estimator <span class="math inline">\(\hat T\)</span> </span>:
<span class="math display">\[
  R_\theta(\hat T)
  = \mathbb E_{X\sim \theta}[l(T(\theta), \hat T(X)]
  = \int P_\theta(dx) P_{\hat T|X}(\hat t|x) l(T(\theta), \hat t)
\]</span></li>
</ul>
<p>Two comments in order:</p>
<ul>
<li>For the minimax risk, it is sometimes necessary to
randomize. To minimize the average risk over a prior (Bayesian approach),
though, it suffices to consider deterministic functions.</li>
<li>The space of randomized estimators (as Markov kernels) is convex.
This is necessary for minimax theorems.</li>
<li>For <span class="math inline">\(\hat T\mapsto l(T, \hat T)\)</span> convex, we can derandomize
it by consider <span class="math inline">\(\mathbb E[\hat T|X]\)</span>, then
<span class="math display">\[
      R_\theta(\hat T) = \mathbb E_\theta l(T, \hat T) \geq
      \mathbb E_\theta l(T, \mathbb E[\hat T|X])
  \]</span></li>
</ul>
<div class="definition">
<p><span id="def:unlabeled-div-72" class="definition"><strong>Definition 9.1  (Bayes risk) </strong></span>Given a prior <span class="math inline">\(\pi\)</span>, the average risk w.r.t. <span class="math inline">\(\pi\)</span>
of an estimator is
<span class="math display">\[
    R_\pi(\hat \theta)
    = \mathbb E_{\theta \sim \pi} R_\theta(\hat \theta)
    = \mathbb E_{\theta \sim \pi, X\sim P_\theta} l(\theta, \hat \theta(X))
\]</span>
The <strong>Bayes risk</strong> of a prior is its minimal average risk
<span class="math inline">\(R_\pi^* = \inf_{\hat \theta} R_\pi(\hat \theta)\)</span>.
The optimal <span class="math inline">\(\hat \theta\)</span> is the Bayes estimator.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-73" class="definition"><strong>Definition 9.2  (minimax risk) </strong></span>Given a parameter family <span class="math inline">\(P_{\theta\in \Theta}\)</span> and
loss function <span class="math inline">\(l\)</span>, the minimax risk is<br />
<span class="math display">\[
    R^* = \inf_{\hat \theta} \sup_{\theta \in \Theta}
    R_\theta(\hat \theta)
    = \inf_{\hat \theta} \sup_{\theta \in \Theta}
    \mathbb E_{X\sim P_\theta} l(\hat \theta(X), \theta)
\]</span>
To prove minimax risk, one needs to establish for arbitrary <span class="math inline">\(\epsilon\)</span></p>
<ol style="list-style-type: decimal">
<li>an estimator <span class="math inline">\(\hat \theta^*\)</span> satisfying
<span class="math inline">\(R_\theta(\hat \theta^*)\leq R^*+\epsilon\)</span>.</li>
<li>For arbitrary <span class="math inline">\(\hat \theta\)</span>,
<span class="math inline">\(\sup_{\theta} R_\theta(\hat \theta)\geq R^*-\epsilon\)</span>.</li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-74" class="theorem"><strong>Theorem 9.1  (duality between minimax and Bayes risk) </strong></span>Let <span class="math inline">\(\mathcal P(\Theta)\)</span> denote the set of probability distributions
on <span class="math inline">\(\Theta\)</span>, then
<span class="math display">\[
    R^* = \inf_{\hat \theta}\sup_{\theta \in \Theta} R_\theta(\hat \theta)
    \geq R^*_{\mathrm{Bayes}}
    = \sup_{\pi \in \mathcal P(\theta)} \inf_{\hat \theta}
    R_\pi(\hat \theta)
\]</span></p>
</div>
<p><em>Proof:</em> <span class="math inline">\(R^* = \inf_{\hat \theta}\sup_{\theta \in \Theta} R_\theta(\hat \theta)
= \inf_{\hat \theta}\sup_{\pi \in \mathcal P(\Theta)} R_\pi(\hat \theta)
\geq R^*_{\mathrm{Bayes}}\)</span> by <span class="math inline">\(\inf\sup \geq \sup\inf\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-75" class="example"><strong>Example 9.1  (strict inequality) </strong></span>Let <span class="math inline">\(\theta, \hat \theta \in \{1, 2, \cdots\}\)</span> and
<span class="math inline">\(l(\theta, \hat \theta) = 1_{\hat \theta &lt; \theta}\)</span>, then
<span class="math inline">\(R^* = 1\)</span> while <span class="math inline">\(R^*_{\mathrm{Bayes}} = 0\)</span>.</p>
</div>
</div>
<div id="minimaxDual" class="section level2 unnumbered hasAnchor">
<h2>A duality perspective<a href="statistical-decision-applications.html#minimaxDual" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For simplicity, let <span class="math inline">\(\Theta\)</span> be a finite set and <span class="math inline">\(l\)</span> be convex, then
<span class="math display">\[
    R^* = \min_{P_{\hat \theta|X}} \max_{\theta \in \Theta}
    \mathbb E_\theta \, l(\theta, \hat \theta)
\]</span>
This is a convex optimization problem since
<span class="math display">\[
    P_{\hat \theta|X}\mapsto \mathbb E_\theta \, l(\theta, \hat \theta)
    = \mathbb E_{X\sim P_\theta, \hat \theta\sim
    P_{\hat \theta|X}}[l(\theta, \hat \theta)]
\]</span>
is linear and the pointwise supremum of convex functions is convex.
Considering its dual, rewrite
<span class="math display">\[
    R^* = \min_{P_{\hat \theta|X}, t} \, t\quad \text{s.t. }
    \mathbb E_\theta\, l(\theta, \hat \theta) \leq t, \quad \forall \theta \in \Theta
\]</span>
Let <span class="math inline">\(\pi_\theta\geq 0\)</span> for each inequality constraint.
The Lagrangian is
<span class="math display">\[
    \mathcal L(P_{\hat \theta|X}, t, \pi)
    = t + \sum_{\theta \in \Theta} \pi_\theta\cdot \left(
        \mathbb E_\theta\, l(\theta, \hat \theta) - t
    \right) = \left(
        1 - \sum_{\theta \in \Theta} \pi_\theta
    \right)t + \sum_{\theta \in \Theta} \pi_\theta \mathbb E_\theta\, l(\theta, \hat \theta)
\]</span>
The first term implies that <span class="math inline">\(\pi\)</span> must be a probability measure,
yielding the dual problem
<span class="math display">\[
    \max_\pi \min_{P_{\hat \theta|X}}
    \sum_{\theta \in \Theta} \pi_\theta \mathbb E_\theta\, l(\theta, \hat \theta)
    = \max_{\pi \in \mathcal P(\Theta)} R_\pi^*
\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-76" class="theorem"><strong>Theorem 9.2  (general minimax equality) </strong></span><span class="math inline">\(R^* = R^*_{\mathrm{Bayes}}\)</span> if the following conditions hold:</p>
<ol style="list-style-type: decimal">
<li>The experiment is dominated: <span class="math inline">\(P_{\forall \theta} \ll \nu\)</span> for some <span class="math inline">\(\nu\)</span>.</li>
<li>The action space <span class="math inline">\(\hat \Theta\)</span> (codomain of the estimator)
is a locally compact topological space with a countable basis.</li>
<li>The loss function is level compact: for each <span class="math inline">\(\theta\in \Theta, l(\theta, \cdot)\)</span>
is bounded from below and <span class="math inline">\(\{\hat \theta:l(\theta, \hat \theta)\leq a\}\)</span>
is compact for each <span class="math inline">\(a\)</span>.</li>
</ol>
</div>
<p>We will prove the following special case for demonstration.</p>
<div class="proposition">
<p><span id="prp:minimaxSeparation" class="proposition"><strong>Proposition 9.1  (special minimax equality) </strong></span><span class="math inline">\(R^* = R^*_{\mathrm{Bayes}}\)</span> if
<span class="math inline">\(\Theta\)</span> is finite and <span class="math inline">\(l\)</span> is bounded from below (e.g. quadratic).</p>
</div>
<span style="color:green">
Proof ideas: work in the vector space of risk vectors
with inner product given by average risk. Next
separate the convex sets of (1) all risk vectors, and (2)
all vectors component-wise less than <span class="math inline">\(R^*\)</span>.<br />
</span>
<details>
<summary>
Proof
</summary>
<ul>
<li>First consider the edge case <span class="math inline">\(R^*=\infty \iff R^*_{\mathrm{Bayes}} = \infty\)</span>;
this is established by considering the uniform prior <span class="math inline">\(\pi\)</span> on <span class="math inline">\(\Theta\)</span>.</li>
<li>Next consider <span class="math inline">\(R^*&lt;\infty\)</span>. Given an estimator <span class="math inline">\(\hat \theta\)</span>, denote its
risk vector <span class="math inline">\(R(\hat \theta)_\theta = \mathbb E_{X\sim P_\theta} l(\theta, \hat \theta)\)</span>
with components in <span class="math inline">\(\theta\)</span>.</li>
<li>The average risk is given by the inner product <span class="math inline">\(\langle R(\hat \theta), \pi\rangle\)</span>.</li>
<li>Define the set <span class="math inline">\(S\)</span> of all possible risk vectors for randomized estimators;
Note that <span class="math inline">\(S\)</span> is convex because linear interpolations of risk vectors
are given by mixtures of their corresponding random estimators.</li>
<li>The set <span class="math inline">\(T=\{t\in R^\Theta: t_{\forall \theta} &lt; R^*\}\)</span>
is convex, and <span class="math inline">\(S\cap T=\emptyset\)</span> since for every valid estimator at
least one component achieves <span class="math inline">\(R^*\)</span>.</li>
<li>By the hyperplane separation theorem, there exists <span class="math inline">\(\pi \in R^\Theta\)</span>
and <span class="math inline">\(c\in \mathbb R\)</span> such that <em>{sS}, sc</em>{tT}, t$.
Now <span class="math inline">\(\pi\)</span> must be componentwise positive else <span class="math inline">\(\sup_{t\in T}\langle\pi, t\rangle= \infty\)</span>,
so w.l.o.g. <span class="math inline">\(\pi\)</span> is a probability vector.</li>
<li>Thus we have established <span class="math inline">\(R^*_{\mathrm{Bayes}} \geq R_\pi^* \geq R^*\)</span>.</li>
</ul>
</details>
</div>
<div id="sample-complexity-tensor-products" class="section level2 unnumbered hasAnchor">
<h2>Sample complexity, tensor products<a href="statistical-decision-applications.html#sample-complexity-tensor-products" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given an experiment <span class="math inline">\(P_{\theta\in \Theta}\)</span>, the <em>independent sampling model</em>
refers to the experiment
<span class="math display">\[
    \mathcal P_n = \{P_{\theta \in \Theta}^{\otimes n}\}, \quad n\geq 1
\]</span>
Define <span class="math inline">\(R_n^*(\Theta) = \inf_\hat \theta \sup_{\theta \in \Theta} \mathbb E_\theta\, l(\theta, \hat \theta)\)</span>
to be the minimax risk when <span class="math inline">\(\hat \theta\)</span> consumes
<span class="math inline">\(X=(X_1, \cdots, X_n)\)</span> consisting of independent observations. Note that:</p>
<ul>
<li><span class="math inline">\(n\mapsto R_n^*(\Theta)\)</span> is non-increasing since we can always discard extra observations.</li>
<li>We typically expect <span class="math inline">\(R_n^*(\Theta)\to 0\)</span> as <span class="math inline">\(n\to \infty\)</span>.</li>
</ul>
<div class="definition">
<p><span id="def:sampleComplexity" class="definition"><strong>Definition 9.3  (sample complexity) </strong></span>The sample complexity is the minimum sample size required to obtain a prescribed
error <span class="math inline">\(\epsilon\)</span> in the worst case (of actual parameter):
<span class="math display">\[
    n^*(\epsilon) = \min\{n\in \mathbb N: R^*_n(\Theta)\leq \epsilon\}
\]</span></p>
</div>
<div class="definition">
<p><span id="def:tpExperiment" class="definition"><strong>Definition 9.4  (tensor product experiment) </strong></span>Given statistical experiments <span class="math inline">\(\mathcal P_i = \{P_{\theta_i \in \Theta_i}\}\)</span>
and loss <span class="math inline">\(l_i\)</span> for each <span class="math inline">\(i\in [d]\)</span>, the tensor product experiment
<span class="math display">\[
    \mathcal P = \left\{\prod P_{\theta_j}, \quad
    (\theta_j)\in \Theta = \prod \Theta_j\right\}
\]</span>
the loss function is <span class="math inline">\(l(\theta, \hat \theta) = \sum_j l_j(\theta_j, \hat \theta_j)\)</span>.
In this model, the observation <span class="math inline">\((X_1, \cdots, X_d)\)</span> are independent but not
necessarily identically distributed.</p>
</div>
<div class="theorem">
<p><span id="thm:tpMinimax" class="theorem"><strong>Theorem 9.3  (minimax risk of tensor product) </strong></span><span class="math display">\[
    \sum_{j=1}^d R^*_{\mathrm{Bayes}}(\mathcal P_j) \leq R^*(\mathcal P)
    \leq \sum_{j=1}^d R^*(\mathcal P_j)
\]</span>
If the minimax theorem <span class="math inline">\(R^*(\mathcal P_i) = R^*_{\mathrm{Bayes}}(\mathcal P_i)\)</span>,
then it holds for the product experiment and the minimax risk additively
decomposes.</p>
</div>
<span style="color:green">
Proof idea: right inequality follows by unrolling definitions.
For the left inequality, choose <span class="math inline">\(\theta_j\)</span>’s independent from a
product prior and argue that each component of the estimator only
obtain information from <span class="math inline">\(X_j\)</span> alone.
</span>
<details>
<summary>
Proof
</summary>
For the right inequality, choose <span class="math inline">\(\hat \theta^* = (\hat \theta_j^*)\)</span>
to be the component minimax estimators without considering any other observations.
For the left inequality, consider for each <span class="math inline">\((\pi_j)\)</span> a product prior
<span class="math inline">\(\pi = \prod \pi_j\)</span>; under this prior, <em>both <span class="math inline">\(\theta_j\)</span>’s and <span class="math inline">\(X_j\)</span>’s are independent.</em>
For any component <span class="math inline">\(\hat \theta_j = \hat \theta_j(X, U_j)\)</span> of any randomized
estimator <span class="math inline">\(\hat \theta\)</span>, the non-<span class="math inline">\(X_j\)</span> components <span class="math inline">\((U_j, X_{\bar j})\)</span>
by independence can be viewed as a randomized estimator based on <span class="math inline">\(X_i\)</span> alone
(all other <span class="math inline">\(X_{\bar j}\)</span>’s do not yield any information about <span class="math inline">\(\theta_j\)</span> by independence
of <span class="math inline">\(\theta\)</span>’s). Thus its average risk satisfies
<span class="math inline">\(R_{\pi_j}(\hat \theta_j) \geq R^*_{\pi_j}\)</span>.
Take supremum over all <span class="math inline">\(\pi_j\)</span> and sum over <span class="math inline">\(j\)</span> to obtain the left inequality.
</details>
</div>
<div id="hcr-lower-bound" class="section level2 unnumbered hasAnchor">
<h2>HCR lower bound<a href="statistical-decision-applications.html#hcr-lower-bound" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:hcrBound" class="theorem"><strong>Theorem 9.4  (HCR lower bound) </strong></span>The quadratic loss of any estimator <span class="math inline">\(\hat \theta\)</span> at
<span class="math inline">\(\theta\in \Theta\subset \mathbb R^d\)</span> satisfies
<span class="math display">\[
    R_\theta(\hat \theta) = \mathbb E_{
        \theta, X\sim P_\theta
    } [\hat \theta(X) - \theta]^2
    \geq \mathrm{Var}_\theta(\hat \theta) \geq
    \sup_{\theta&#39;\neq \theta} \dfrac{
        (\mathbb E_\theta \hat \theta - \mathbb E_{\theta&#39;} \hat \theta)^2
    }{\chi^2(P_{\theta&#39;} \| P_\theta)}
\]</span></p>
</div>
<span style="color:green">
<em>Proof idea</em>: fix <span class="math inline">\(\theta&#39; \neq \theta\)</span>
and treat <span class="math inline">\(X\mapsto \hat \theta(X)\)</span> as a data-processor;
apply the variational characterization of <span class="math inline">\(\chi^2\)</span>.
</span>
<details>
<summary>
Proof
</summary>
Recall the variational characterization of <span class="math inline">\(\chi^2\)</span>
(proposition <a href="f-divergence.html#prp:chiVariation">8.11</a>): for all <span class="math inline">\(g:\mathcal X\to \mathbb R\)</span>
<span class="math display">\[
    \mathrm{Var}_Q[g^2] \geq \dfrac{(\mathbb E_P g - \mathbb E_Q g)^2}{\chi^2(P\|Q)}
\]</span>
Given two different underlying parameters <span class="math inline">\(\theta, \theta&#39;\)</span>,
let <span class="math inline">\(P_X=P_\theta, Q_X=P_{\theta&#39;}\)</span>, then applying the
data processor <span class="math inline">\(X\mapsto \hat \theta(X)\)</span>
and the variational characterization
<span class="math display">\[
    \chi^2(P_X \| Q_X) \geq \chi^2(P_{\hat \theta} \| Q_{\hat \theta})
    \geq \dfrac{(\mathbb E_\theta\hat \theta - \mathbb E_{\theta&#39;} \hat \theta)^2}
    {\mathrm{Var}_\theta(\hat \theta)}
\]</span>
</details>
<div class="corollary">
<p><span id="cor:cramerRao" class="corollary"><strong>Corollary 9.1  (Cramer-Rau lower-bound) </strong></span>For an unbiased estimator <span class="math inline">\(\hat \theta\)</span> satisfying
<span class="math inline">\(\mathbb E_{\forall \theta}[\hat \theta] = \theta\)</span>, we obtain
<span class="math display">\[
    \mathrm{Var}_\theta(\hat \theta) \geq \mathcal J_F(\theta)^{-1}
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Apply the unbiased condition to theorem <a href="statistical-decision-applications.html#thm:hcrBound">9.4</a>,
take <span class="math inline">\(\theta = \theta + \xi_{\to 0}\)</span> and apply the local Fisher
behavior of divergence theorem <a href="f-divergence.html#thm:localFisherDiv">8.12</a>
<span class="math display">\[\begin{align}
    \mathrm{Var}_\theta(\hat \theta)
    &amp;\geq \dfrac{
        \xi^2
    }{\chi^2(P_{\theta + \xi} \| P_\theta)}
    = \lim_{\xi\to 0} \xi\dfrac{
        \xi^2
    }{\mathcal J_F(0) \xi^2 + \cdots} = \mathcal J_F(\theta)^{-1}
\end{align}\]</span>
</details>
<p>To generalize to <span class="math inline">\(\theta \in \Theta\subset \mathbb R^d\)</span>, assume
<span class="math inline">\(\hat \theta\)</span> _is unbiased and
<span class="math display">\[
    \mathrm{Cov}_\theta(\hat \theta)
    = \mathbb E_\theta[(\hat \theta - \theta)(\hat \theta - \theta)^T]
\]</span>
is positive-semidefinite,
apply theorem <a href="statistical-decision-applications.html#thm:hcrBound">9.4</a>
to each data-processor <span class="math inline">\(\langle a, \hat \theta(X)\rangle,
a\in \mathbb R^d\)</span> to obtain
<span class="math display">\[
    \chi^2(P_\theta \| P_{\theta&#39;}) \geq
    \dfrac{\langle a, \theta - \theta&#39;\rangle^2}{a^T \mathrm{Cov}_\theta(\hat \theta)a}
\]</span>
Optimizing over <span class="math inline">\(a\)</span> and apply, for <span class="math inline">\(0\preceq \Sigma\)</span>
<span class="math display">\[
    \sup_{x\neq 0} \dfrac{\langle x, y\rangle^2}{x^T\Sigma x} = y^T \Sigma^{-1} y, \quad
    x = \Sigma^{-1} y
\]</span>
yields <span class="math inline">\(\chi^2(P_\theta \| P_{\theta&#39;}) \geq
(\theta - \theta&#39;)^T \mathrm{Cov}_\theta(\hat \theta)^{-1}
(\theta - \theta&#39;)\)</span>. Applying the multivariate local
approximation again yields <span class="math inline">\(\mathcal J_F^{-1}(\theta) \preceq \mathrm{Cov}_\theta(\hat \theta)\)</span>.</p>
<p>Further recall that the Fisher information is additive
under i.i.d. tensorization; taking the trace on both
sides, we obtain, for unbiased estimators,
<span class="math display">\[
    \mathbb E_\theta \|\hat \theta - \theta\|_2^2
    \geq \dfrac 1 2 \mathrm{tr}\mathcal J_F^{-1}(\theta)
\]</span></p>
</div>
<div id="bayesian-perspective" class="section level2 unnumbered hasAnchor">
<h2>Bayesian perspective<a href="statistical-decision-applications.html#bayesian-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In minimax settings, it is often wise to
trade bias with variance to achieve a smaller
overall risk. Fixing a prior <span class="math inline">\(\pi \in \mathcal P(\Theta)\)</span>,<br />
consider the following joint distributions for
<span class="math inline">\(\theta, X)\)</span>:</p>
<ul>
<li>Under <span class="math inline">\(Q\)</span>, <span class="math inline">\(\theta\sim \pi\)</span> and <span class="math inline">\(X\sim P_\theta\)</span> after
sampling <span class="math inline">\(\theta\)</span>.</li>
<li>Under <span class="math inline">\(P, \theta\sim T_\delta\pi\)</span> where <span class="math inline">\(T_\delta\pi\)</span>
is the displaced prior <span class="math inline">\(T_\delta \pi(A) = \pi(A - \delta)\)</span>, and
<span class="math inline">\(X\sim P_{\theta - \delta}\)</span> conditioning on <span class="math inline">\(\theta\)</span>.</li>
</ul>
<div class="theorem">
<p><span id="thm:vanTrees" class="theorem"><strong>Theorem 9.5  (van Trees inequality) </strong></span>Fixing a differentiable prior <span class="math inline">\(\pi\)</span>
and using the setup for <span class="math inline">\(P_{X\theta}, Q_{X\theta}\)</span>
above, under regularity conditions
<span class="math display">\[
    R^* \geq R^*_\pi = \inf_{\hat \theta} \mathbb E_\pi[(\hat \theta - \theta)^2]
    \geq \dfrac 1 {J(\pi) + \mathbb E_{\theta\sim \pi} \mathcal J_F(\theta)}
\]</span>
Note that <span class="math inline">\(\mathbb E_\pi[(\hat \theta - \theta)^2]\)</span> is
in fact expectation over <span class="math inline">\(\theta\sim \pi\)</span>, then <span class="math inline">\(X\sim P_\theta\)</span>,
and <span class="math inline">\(J(\pi)\)</span> is the Fisher information
of the location family <span class="math inline">\(\pi\)</span>. The regularity conditions are:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\pi\)</span> is differentiable, supported on <span class="math inline">\([\theta_0, \theta_1]\)</span> and
<span class="math inline">\(\pi(\theta_0) = \pi(\theta_1) = 0\)</span>.</li>
<li>The location-family Fisher information of the prior is finite
<span class="math display">\[
J(\pi) = \int_{\theta_0}^{\theta_1} \dfrac{\pi&#39;(\theta)^2}{\pi(\theta)}\, d\theta &lt; \infty
\]</span></li>
<li>The family has a dominating measure: <span class="math inline">\(P_\theta = P_\theta\mu\)</span>, and <span class="math inline">\(p_\theta(x)\)</span>
is differentiable in <span class="math inline">\(\theta\)</span> a.e.</li>
<li>For <span class="math inline">\(\pi\)</span>-almost every <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\int \partial_{\theta }p_\theta(x)\, d\mu = 0\)</span>.<br />
</li>
</ol>
</div>
<details>
<summary>
Proof by data-processing
</summary>
<p>Fix any (possibly randomized) estimator <span class="math inline">\(\hat \theta\)</span>,
the marginals (but not the joint!)
<span class="math inline">\(P_X = Q_X\)</span> so <span class="math inline">\(\mathbb E_P\hat \theta = \mathbb E_Q\hat \theta\)</span>
while <span class="math inline">\(\mathbb E_P \theta = \mathbb E_Q \theta + \delta\)</span>. Then
<span class="math display">\[\begin{align}
    \chi^2(P_{\theta X} \| Q_{\theta X})
    &amp;\geq \chi^2(P_{\theta \hat \theta} \| Q_{\theta\hat \theta})
    \geq \chi^2(P_{\theta - \hat \theta} \| Q_{\theta - \hat \theta})
     \\
     &amp;\geq \dfrac{(\mathbb E_P[\theta - \hat \theta] -
    \mathbb E_Q[\theta - \hat \theta])^2}{\mathrm{Var}_Q(\hat \theta - \theta)}
    = \dfrac{\delta^2}{\mathrm{Var}_Q(\hat \theta - \theta)} \geq
    \dfrac{\delta^2}{\mathbb E_Q[\hat \theta - \theta]^2}
\end{align}\]</span>
To obtain the second inequality, first consider
the local expansions:</p>
<ul>
<li><span class="math inline">\(\chi^2(P_\theta \| Q_\theta) = \chi^2(T_\delta \pi \| \pi)
  \approx J(\pi) \delta^2\)</span>.</li>
<li><span class="math inline">\(\chi^2(P_{X|\theta} \| Q_{X|\theta}) \approx \mathcal J_F(\theta) \delta^2\)</span>.
Next, apply the <span class="math inline">\(\chi^2\)</span> chain rule (proposition <a href="f-divergence.html#prp:chiSqChain">8.6</a>)
to obtain
<span class="math display">\[\begin{align}
  \chi^2(P_{X\theta} \| Q_{X\theta})
  &amp;= \chi^2(P_\theta \| Q_{\theta}) + \mathbb E_Q \left[
      \chi^2(P_{X|\theta} \| Q_{X|\theta})
      \left(\dfrac{dP_\theta}{dQ_\theta}\right)^2
  \right] \\
  R_\pi^*
  &amp;\geq \dfrac{\delta^2}{J(\pi) \delta^2 +
  \mathbb E_Q \left[\mathcal J_F(\theta) \delta^2 (dP_\theta/dQ_\theta)^2\right]}\\
  &amp;\geq \dfrac 1 {J(\pi) + \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\end{align}\]</span>
In the last step, we can also take <span class="math inline">\(dP_\theta/dQ_\theta\to 1\)</span>
by continuity of <span class="math inline">\(\pi\)</span>.</li>
</ul>
</details>
<details>
<summary>
Direct proof
</summary>
For the Bayes setting, w.l.o.g. assume that the estimator is deterministic.
For each <span class="math inline">\(x\)</span>, integrate by parts to obtain
<span class="math display">\[
    \int_{\theta_0}^{\theta_1} (\hat \theta - \theta)
    \partial_{\theta}[p_\theta \pi(\theta)]
    = \int_{\theta_0}^{\theta_1} p_\theta \pi(\theta)\, d\theta
\]</span>
Next integrate both sides over <span class="math inline">\(\mu(dx)\)</span> to obtain
<span class="math display">\[
    \mathbb E_{\theta, X}[(\hat \theta - \theta)V(\theta, X)] = 1, \quad
    V(\theta, x) = \partial_{\theta }\log[p_\theta(x) \pi(\theta)]
\]</span>
Next apply Cauchy-Schwarz to obtain
<span class="math inline">\(\mathbb E[(\hat \theta - \theta)^2]\mathbb E[V(\theta, X)^2]\geq 1\)</span> and
<span class="math display">\[
    \mathbb E[V(\theta, X)^2]
    = \mathbb E[V(\theta, X)^2] = \mathbb E[(\partial_{\theta }\log P_\theta(X))^2]
    + \mathbb E[(\partial_{\theta }\log \pi(\theta))^2] = \mathbb E_\theta \mathcal J_F(\theta)
    + J(\pi)
\]</span>
</details>
<ul>
<li>We assume a prior density that vanishes at the boundary.
A uniform prior yields the Chernoff-Rubin-Stein inequality.</li>
<li>To obtain the tightest lower bound, one shouold use regular prior
with minimum Fisher information, which is known to be
<span class="math inline">\(g(u) = \cos^2(\pi u/2)\)</span> supported on <span class="math inline">\([-1, 1]\)</span> with Fisher information <span class="math inline">\(\pi^2\)</span>.</li>
</ul>
<p>We only state the multivariate version of the theorem above.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-77" class="theorem"><strong>Theorem 9.6  (multivariate BCR) </strong></span>Given a product prior density <span class="math inline">\(\pi(\theta) = \prod_{j=1}^d \pi_j(\theta_j)\)</span>
such that each <span class="math inline">\(\pi_j\)</span> is compactly supported and vanishes
on the boundary; suppose that for <span class="math inline">\(\pi\)</span>-almost every <span class="math inline">\(\theta\)</span>
<span class="math display">\[
    \int \mu(dx) \nabla_\theta p_\theta(x) = 0
\]</span>
Then, for <span class="math inline">\(J(\pi) = \mathrm{diag}(\{J(\pi_j)\})\)</span> we have (first inverse
then trace)
<span class="math display">\[
    R_\pi^* = \inf_{\hat \theta} \mathbb E_\pi \|\theta - \hat \theta\|_2^2
    \geq \mathrm{tr}\left(
        \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta) + J(\pi)
    \right)^{-1}
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:minimaxLB" class="theorem"><strong>Theorem 9.7  (lower bound on asymptotic minimax risk) </strong></span>Assuming <span class="math inline">\(\theta \mapsto \mathcal J_F(\theta)\)</span> continuous;
let <span class="math inline">\(X_1, \cdots X_n\sim P_\theta\)</span> i.i.d. and define
the minimax risk <span class="math inline">\(R_n^* = \inf_{\hat \theta} \sup_{\theta \in \Theta}
\mathbb E_\theta \|\hat \theta - \theta\|_2^2\)</span>, then
<span class="math display">\[
    R_n^* \geq \dfrac{1+o(1)}{n} \sup_{\theta \in \Theta} \mathrm{tr}\mathcal J_F^{-1}(\theta)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Fix <span class="math inline">\(\theta\in \Theta\)</span> and choose a small <span class="math inline">\(\delta\)</span>
such that <span class="math inline">\(\theta + [-\delta, \delta]^d \subset \Theta\)</span>
and let <span class="math inline">\(\pi_j(\theta_j)\)</span> be the compact minimal-Fisher information
cosine prior. Then
<span class="math display">\[
    J(\pi_j) = \dfrac 1 {\delta^2} J(g) = \dfrac{\pi^2}{\delta^2}\implies
    J(\pi) = \dfrac{\pi^2}{\delta^2}I_d
\]</span>
Next, continuity <span class="math inline">\(\theta \mapsto \mathcal J_F(\theta)\)</span> guarantees
the hypothesis of the HCR bound, then applying
the additivity of Fisher information, we obtain
<span class="math display">\[
    R_n^* \geq \mathrm{tr}\left(
        n \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta) + J(\pi)
    \right)^{-1}
    = \dfrac 1 n \mathrm{tr}\left(
        \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta) + \dfrac{\pi^2}{n\delta^2}I_d
    \right)^{-1}
\]</span>
Choose <span class="math inline">\(\delta = n^{-1/4}\)</span> and apply the continuity
of <span class="math inline">\(\mathcal J_F(\theta)\)</span> in at <span class="math inline">\(\theta\)</span>.
</details>
</div>
<div id="miscellany-mle" class="section level2 unnumbered hasAnchor">
<h2>Miscellany: MLE<a href="statistical-decision-applications.html#miscellany-mle" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider <span class="math inline">\(X^n=(X_j)\sim P_{\theta_0}\)</span> i.i.d., the
maximum likelihood estimator (MLE) is
<span class="math display">\[
    \hat \theta_{\mathrm{MLE}}
    = \mathrm{arg}\max_{\theta\in \Theta} L_\theta(X^n), \quad
    L_\theta(X^n) = \sum_{j=1}^n \log p_\theta(X_j)
\]</span>
For discrete distributions, we also have
<span class="math inline">\(\hat \theta_{\mathrm{MLE}} \in
\mathrm{arg}\min_{\theta \sin \Theta} D(\hat P_n \| P_\theta)\)</span>.
This implies that the expected log-likelihood is maximized
at the true parameter value <span class="math inline">\(\theta_0\)</span>.
- Suppose <span class="math inline">\(\theta \mapsto P_\theta\)</span> is injective, then
<span class="math display">\[
    \mathbb E_{\theta_0}[L_\theta - L_{\theta_0}]
    = \mathbb E_{\theta_0} \left[
        -\sum \log \dfrac{p_{\theta_0}(X_j)}{p_\theta(X_j)}
    \right] = -nD(P_{\theta_0} \| P_\theta) &lt; 0
\]</span>
Assuming regularity conditions and together
with l.l.n, this ensures
consistency <span class="math inline">\(\hat \theta_{n\to \infty}\to \theta_0\)</span>.</p>
<ul>
<li>Assuming more regularity conditions, we can approximate
<span class="math display">\[
  L_\theta = L_{\theta_0} +
  (\theta - \theta_0)^T \sum_j V(\theta_0, X_j)
  + \dfrac 1 2 (\theta - \theta_0^T \left(
      \sum H(\theta_0, X_j)
  \right)(\theta - \theta_0) + \cdots
\]</span>
Under regularity conditions, we have
<span class="math inline">\(\sum H(\theta_0, X_j) \to -n \mathcal J_F(\theta_0)\)</span>
and the linear term vanishing. This yields the stochastic
approximation
<span class="math display">\[
  L_\theta \approx L_{\theta_0}
  + \langle\sqrt{n\mathcal J_F(\theta_0)} Z, \theta - \theta_0\rangle
  - \dfrac n 2 (\theta - \theta_0)^T \mathcal J_F(\theta_0) (\theta - \theta_0)
\]</span>
Maximizing the RHS yields the asymptotic normal expression
<span class="math display">\[
  \hat \theta_{\mathrm{MLE}} \approx \theta_0
  + \dfrac 1 {\sqrt n} \mathcal J_F(\theta_0)^{-1/2} Z
\]</span></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="f-divergence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lossless-compression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
