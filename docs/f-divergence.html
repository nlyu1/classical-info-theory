<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 f-Divergence | 6.7480 Notes</title>
  <meta name="description" content="8 f-Divergence | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="8 f-Divergence | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 f-Divergence | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-10-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tensorization.html"/>
<link rel="next" href="data-compression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics and geometry</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#brégmans-theorem"><i class="fa fa-check"></i>Brégman’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="divergence.html"><a href="divergence.html"><i class="fa fa-check"></i><b>3</b> Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#kl-divergence"><i class="fa fa-check"></i>KL-Divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#sufficient-statistic"><i class="fa fa-check"></i>Sufficient statistic</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#entropy-power-inequality"><i class="fa fa-check"></i>Entropy-power Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html"><i class="fa fa-check"></i><b>5</b> Variational Measures of Information</a>
<ul>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#geometric-interpretations-of-mi"><i class="fa fa-check"></i>Geometric interpretations of MI</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#convexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#gaussian-saddle-point"><i class="fa fa-check"></i>Gaussian saddle point</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#empirical-distribution-and-χ²"><i class="fa fa-check"></i>Empirical distribution and χ²</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-compression.html"><a href="data-compression.html"><i class="fa fa-check"></i>Data compression</a></li>
<li class="chapter" data-level="9" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>9</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="f-divergence" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> f-Divergence<a href="f-divergence.html#f-divergence" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li>TV as binary hypothesis testing.</li>
<li>Several divergences are metrics; one fast way to
compare is to compare to metric, use triangle inequality,
then use comparison inequality again.</li>
<li>There is no chain rule for <span class="math inline">\(f\)</span>-divergences other than
KL-divergence.</li>
<li>Convexity of <span class="math inline">\(D_f\)</span> <a href="f-divergence.html#thm:fInfoProperties">8.3</a> <span class="math inline">\(\iff\)</span>
DPI <a href="f-divergence.html#thm:fDPI">8.2</a> <span class="math inline">\(\iff\)</span>
monotonicity <a href="f-divergence.html#thm:fMonotonicity">8.1</a> <span class="math inline">\(\iff\)</span> convexity of <span class="math inline">\(f\)</span>.</li>
<li>TV does not enjoy tensorization properties.</li>
<li><span class="math inline">\(f\)</span>-divergence is equivalent modulo <span class="math inline">\(+c(t-1)\)</span>; then
convexity plus <span class="math inline">\(f(t)=1\)</span> implies locally <span class="math inline">\(\chi^2\)</span>;
w.l.o.g we can assume <span class="math inline">\(f\geq 0\)</span> and <span class="math inline">\(f&#39;(1)=0\)</span>.</li>
<li>The powerful approximation theorem <a href="f-divergence.html#thm:fFiniteApprox">8.4</a>
reduces arbitrary spaces to finite ones.</li>
<li>Rényi entropy enjoys tensorization and chain rule (up to tilting);
it includes KL, <span class="math inline">\(\chi^2\)</span> and Hellinger as special cases.</li>
<li>The Hellinger divergence is both a metric and tensorizes well.</li>
</ol>
<div id="definition" class="section level2 unnumbered hasAnchor">
<h2>Definition<a href="f-divergence.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-53" class="definition"><strong>Definition 8.1  (f-divergence) </strong></span>Given a convex function <span class="math inline">\(f:(0, \infty)\to \mathbb R\)</span> with <span class="math inline">\(f(1)=0\)</span>,
for every two probability distributions over
<span class="math inline">\(\mathcal X\)</span>, if <span class="math inline">\(P\ll Q\)</span> then the <span class="math inline">\(f\)</span> divergence is
<span class="math display">\[
        D_f(P\|Q) = \mathbb E_Q \left[
            f\left(\dfrac{dP}{dQ}\right)
        \right]
    \]</span>
Here <span class="math inline">\(f(0) = f(0_+)\)</span> per limit. More generally,
define <span class="math inline">\(f&#39;(\infty) = \lim_{x\to 0^+} xf(1/x)\)</span>,
we have
<span class="math display">\[
        D_f(P\|Q) = \int_{q&gt;0} q(x) f\left[
            \dfrac{p(x)}{q(x)}
        \right] \, d\mu + f&#39;(\infty) P[Q=0]
    \]</span>
this last generalization is needed to account for
divergences like total variation.
Intuitively, sums for terms with <span class="math inline">\(dQ=0\)</span> are like
<span class="math display">\[
        \int_{dQ=0} dQ f\left(\dfrac{dP}{dQ}\right)
        = \lim_{t=dP/dQ\to 0^+} \int \dfrac{dP}{t} f(1/t)
    \]</span></p>
</div>
<p>Examples of <span class="math inline">\(f\)</span>-divergences:</p>
<ol style="list-style-type: decimal">
<li><strong>KL-divergence</strong>: <span class="math inline">\(f(x)=x\log x\)</span> to recover KL-divergence.</li>
<li><strong>Total variation (TV)</strong>: <span class="math inline">\(f(x) = \dfrac 1 2 |x - 1|\)</span>:
<span class="math display">\[
\mathrm{TV}(P, Q) = \dfrac 1 2 \mathbb E_Q \left|\dfrac{dP}{dQ} - 1\right|
= \dfrac 1 2 \int |dP - dQ| = 1 - \int d(P\wedge Q)
\]</span>
Recall that <span class="math inline">\(P\wedge Q\)</span> is the pointwise minimum measure
so <span class="math inline">\(\int d(P\wedge Q)\)</span> is the overlap measure.</li>
<li><strong><span class="math inline">\(\chi^2\)</span>-divergence</strong>: <span class="math inline">\(f(x)=(x-1)^2\)</span>.
<span class="math display">\[
\chi^2(P\|Q) = \mathbb E_Q \left(\dfrac{dP}{dQ} - 1\right)^2
= \int \dfrac{(dP - dQ)^2}{dQ}
= \int \dfrac{dP^2}{dQ} + dQ - 2dP = \int \dfrac{dP^2}{dQ} - 1
\]</span></li>
<li><strong>Squared Hellinger distance</strong>: <span class="math inline">\(f(x) = \left(1 - \sqrt x\right)^2\)</span>.
<span class="math display" id="eq:hellingerDiv">\[
H^2(P, Q) = \int \left(\sqrt{dP} - \sqrt{dQ}\right)^2
= 2 - 2\int \sqrt{dPdQ}
\tag{8.1}
\]</span>
The quantity <span class="math inline">\(B(P, Q) = \int \sqrt{dPdQ}\)</span> is the
<strong>Bhattacharyya coefficient</strong> (Hellinger affinity).
Note that <span class="math inline">\(H(P, Q) = \sqrt{H^2(P, Q)}\)</span>.
The Hellinger distance is <span class="math inline">\(H(P, Q) = \sqrt{H^2(P, Q)}\)</span>.</li>
<li><strong>Le Cam divergence</strong>: <span class="math inline">\(f(x) = \dfrac{1-x}{2x+2}\)</span>,
<span class="math display">\[
\mathrm{LC}(P, Q) = \dfrac 1 2 \int \dfrac{(dP - dQ)^2}{dP + dQ}
\]</span>
The square root <span class="math inline">\(\sqrt{\mathrm{LC}(P, Q)}\)</span>, the Le Cam distance,
is a metric.</li>
<li><strong>Jensen-Shannon divergence</strong> take
<span class="math inline">\(f(x) = x\log \dfrac{2x}{x+1} + \log \dfrac 2 {x+1}\)</span>.
<span class="math display">\[
\mathrm{JS}(P, Q) = D\left(P \| \dfrac{P+Q}{2}\right)
+ D\left(Q \| \dfrac{P+Q}{2}\right)
\]</span></li>
</ol>
<div class="proposition">
<p><span id="prp:unlabeled-div-54" class="proposition"><strong>Proposition 8.1  </strong></span><span class="math inline">\(\mathrm{TV}(P, Q) = \dfrac 1 2 \int |dP - dQ| = 1 - \int d(P\wedge Q)\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
<em>Proof:</em> Let <span class="math inline">\(E = \{x:dP &gt; dQ\}\)</span>, then
<span class="math display">\[\begin{align}
    \int |dP - dQ|
    &amp;= \int_E dP - d(P\wedge Q) + \int_{E^c} dQ - d(P\wedge Q)
    = \int_E dP + \int_{E^c} dQ - \int d(P\wedge Q) \\
    &amp;= \int_E dP + \int_{E^c} d(P\wedge Q)_{=dP} + \int_{E^c} dQ + \int_E d(P\wedge Q)_{=dQ}
    - 2\int d(P\wedge Q) \\
    &amp;= 2 - 2\int d(P\wedge Q)
\end{align}\]</span>
</details>
<div class="proposition">
<p><span id="prp:unlabeled-div-55" class="proposition"><strong>Proposition 8.2  </strong></span>The following quantities derived from divergences are
metrics on the space of probability distributions
<span class="math display">\[
    \mathrm{TV}(P, Q), \quad H(P, Q), \quad \sqrt{\mathrm{JS}(P, Q)}, \quad \sqrt{\mathrm{LC}(P, Q)}
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-56" class="proposition"><strong>Proposition 8.3  (closure properties) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D_f(Q \| P) = D_g(P \|Q)\)</span> for <span class="math inline">\(g(x) = xf(1/x)\)</span>.</li>
<li><span class="math inline">\(D_f(P\|Q)\)</span> is a <span class="math inline">\(f\)</span>-divergence, then
<span class="math display">\[
D_f(\lambda P + \bar \lambda Q \|Q), \quad
D_f(P \| \lambda P + \bar \lambda Q), \quad
\forall \lambda\in [0, 1]
\]</span>
are <span class="math inline">\(f\)</span>-divergences.</li>
<li>Linearity: <span class="math inline">\(D_{f+g} = D_f + D_g\)</span>.</li>
<li>Distinguishability: <span class="math inline">\(D_f(P \| P) = 0\)</span></li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
For the first claim,
<span class="math display">\[
    D_g(P \|Q) = \int dQ \left(\dfrac{dP}{dQ}\right) f\left(\dfrac{dQ}{dP}\right)
    = D_f(Q \| P)
\]</span>
For the second claim, the other case can be obtained by using the equation above.
<span class="math display">\[
    D_f(\lambda P + \bar \lambda Q \|Q)
    = \int dQ f\left(\lambda \dfrac{dP}{dQ} + \bar \lambda\right)
    \implies \tilde f(x) = f\left(\lambda x + \bar \lambda\right)
\]</span>
</details>
<div class="proposition">
<p><span id="prp:fEquivProperties" class="proposition"><strong>Proposition 8.4  (equivalence properties) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D_f(P\|Q) = 0\)</span> for all <span class="math inline">\(P\neq Q\)</span> iff <span class="math inline">\(f(x)=c(x-1)\)</span> for some <span class="math inline">\(c\)</span>.</li>
<li><span class="math inline">\(D_f = D_{f+c(x-1)}\)</span>; thus we can always assume <span class="math inline">\(f\geq 0\)</span> and <span class="math inline">\(f&#39;(1)=0\)</span>.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
Claim <span class="math inline">\(1\)</span> proceeds by computation (assuming continuity)
<span class="math display">\[
    D_f(P \|Q) = c \int (dP / dQ - 1) dQ = 0
\]</span>
This means that <span class="math inline">\(c(x-1)\)</span> is in the kernel of the linear operator
<span class="math inline">\(f\mapsto D_f\)</span>. Pick <span class="math inline">\(c=-f&#39;(1)\)</span>, then <span class="math inline">\(f(1)=f&#39;(1)=0\)</span>; by convexity <span class="math inline">\(f\geq 0\)</span>.
</details>
<div class="proposition">
<p><span id="prp:specialfMonotonicity" class="proposition"><strong>Proposition 8.5  (special case of monotonicity) </strong></span><span style="color:green">
Joint divergence is unchanged through the same channel
</span>
<span class="math display">\[
    D_f(P_XP_{Y|X} \| Q_XP_{Y|X}) = D_f(P_X \| Q_X)
\]</span>
in particular, for the source-agnostic channel we have
<span class="math display">\[
    D_f(P_XP_Y \| Q_XP_Y) = D_f(P_X \| Q_X)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Direct computation
<span class="math display">\[\begin{align}
    D_f(P_XP_{Y|X} \| Q_XP_{Y|X})
    &amp;= \int Q_X(x) dx \int P_{Y|X=x}(y)dy\,
    f\left[\dfrac{P_X(x) P_{Y|X=x}(y)}{Q_X(x) Q_{Y|X=x}(y)}\right]\\
    &amp;= \int Q_X(x) dx f\left(\dfrac{P_X(x)}{Q_X(x)}\right)
    = D_f(P_X \| Q_X)
\end{align}\]</span>
</details>
</div>
<div id="information-properties-mi" class="section level2 unnumbered hasAnchor">
<h2>Information properties, MI<a href="f-divergence.html#information-properties-mi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:fMonotonicity" class="theorem"><strong>Theorem 8.1  (monotonicity) </strong></span>The joint is more distinguishable than the marginal:
<span class="math display">\[
    D_f(P_{XY} \| Q_{XY}) \geq D_f(P_X \| Q_X)
\]</span>
inequality is saturated when <span class="math inline">\(P_{Y|X} = Q_{Y|X}\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Assume <span class="math inline">\(P_{XY} \ll Q_{XY}\)</span>, then expand and
apply Jensen’s inequality
<span class="math display">\[\begin{align}
    D_f(P_{XY} \| Q_{XY})
    &amp;= \mathbb E_{X\sim Q_X} \mathbb E_{Y\sim Q_{Y|X}} f\left(
        \dfrac{dP_{Y|X} P_X}{dQ_{Y|X}Q_X}
    \right)
    \geq \mathbb E_{X\sim Q_X} f\left(\mathbb E_{Y\sim Q_{Y|X}}
        \dfrac{dP_{Y|X} P_X}{dQ_{Y|X}Q_X}
    \right) \\
    &amp;\geq \mathbb E_{X\sim Q_X} f\left(
        \dfrac{dP_X}{dQ_X}
    \right) = D_f(P_X \| Q_X)
\end{align}\]</span>
To be more careful, on the first line we have
<span class="math display">\[\begin{align}
    \mathbb E_{Y\sim Q_{Y|X=x}}
        \dfrac{P_{Y|X=x}(y) P_X(x)}{Q_{Y|X=x}(y)Q_X(x)}
    &amp;= \dfrac{P_X(x)}{Q_X(x)} \sum_y
    Q_{Y|X=x}(y) \dfrac{P_{Y|X=x}(y)}{Q_{Y|X=x}(y)}
    = \dfrac{P_X(x)}{Q_X(x)}
\end{align}\]</span>
Inequality is saturated when, for every <span class="math inline">\(x\)</span>,
<span class="math inline">\(P_{Y|X=x}=Q_{Y|X=x}\)</span>.
</details>
<div class="definition">
<p><span id="def:unlabeled-div-57" class="definition"><strong>Definition 8.2  (conditional f-divergence) </strong></span>Given <span class="math inline">\(P_{Y|X}, Q_{Y|X}\)</span> and <span class="math inline">\(P_X\)</span>, the conditional
<span class="math inline">\(f\)</span>-divergence is
<span class="math display">\[
    D_f(P_{Y|X} \| Q_{Y|X} | P_X)
    = D_f(P_{Y|X} P_X \| Q_{Y|X} P_X)
    = \mathbb E_{x\sim P_X}\left[
        D_f(P_{Y|X=x}\| Q_{Y|X=x})
    \right]
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The second statement requires some justification:
<span class="math display">\[\begin{align}
    D_f(P_{Y|X} P_X \| Q_{Y|X} P_X)
    &amp;= \mathbb E_{x\sim P_X}
    \mathbb E_{y\sim Q_{Y|X=x}}\left[
        f\left(\dfrac{P_{Y|X=x}(y)}{Q_{Y|X=x}(y)}\right)
    \right]
\end{align}\]</span>
</details>
<div class="theorem">
<p><span id="thm:fDPI" class="theorem"><strong>Theorem 8.2  (data-processing inequality) </strong></span>Given a channel <span class="math inline">\(P_{Y|X}\)</span> with two inputs <span class="math inline">\(P_X, Q_X\)</span>
<span class="math display">\[
    D_f(P_{Y|X}
    \circ P_X \| P_{Y|X} \circ Q_X)
    \leq D_f(P_X \| Q_X)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
<span class="math inline">\(D_f(P_X \| Q_X) = D_f(P_{XY} \| Q_{XY}) \geq D_f(P_Y \| Q_Y)\)</span>,
with two equalities given by proposition <a href="f-divergence.html#prp:specialfMonotonicity">8.5</a>
and theorem <a href="f-divergence.html#thm:fMonotonicity">8.1</a>. Inequality
is saturated by the monotonicity condition <span class="math inline">\(P_{X|Y} = Q_{X|Y}\)</span>.
</details>
<div class="theorem">
<p><span id="thm:fInfoProperties" class="theorem"><strong>Theorem 8.3  (information properties of f-divergences) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span style="color:blue">Non-negativity</span>: <span class="math inline">\(D_f(P\|Q) \geq 0\)</span>. If <span class="math inline">\(f\)</span> is <em>strictly convex</em>
at 1, i.e. 
<span class="math display">\[
     \forall s, t\in [0, \infty), \alpha \in (0, 1) \text{ with }
     \alpha s + \bar \alpha t = 1: \quad \alpha f(s)+ \bar \alpha f(t)
     &gt; f(1) = 0
\]</span>
then <span class="math inline">\(D_f(P\|Q) = 0 \iff P=Q\)</span>.</li>
<li><span style="color:blue">Conditional <span class="math inline">\(f\)</span>-divergence; conditioning increases divergence</span>:
<span class="math inline">\(D_f(P_{Y|X} \circ P_X \| Q_{Y|X} \circ P_X) \leq D_f(P_{Y|X} \| Q_{Y|X} | Q_X)
     = D_f(P_{Y|X} P_X \| Q_{Y|X} P_X)\)</span>.</li>
<li><span style="color:blue">Joint-convexity</span>: <span class="math inline">\((P, Q)\mapsto D_f(P\|Q)\)</span> is jointly convex;
consequently, <span class="math inline">\(P\mapsto D_f(P\|Q)\)</span> and <span class="math inline">\(Q\mapsto D_f(P\|Q)\)</span>
are also convex.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
<p>For non-negativity, apply monotonicity to
<span class="math display">\[
    D_f(P_X \| P_Y) = D_f(P_{X, 1} \| P_{Y, 1}) \geq D_f(1 \| 1) = 0
\]</span>
Assume <span class="math inline">\(P\neq Q\)</span> so there exists measurable <span class="math inline">\(A\)</span> such that
<span class="math inline">\(P[A]=p \neq Q[A] = Q\)</span>, then apply the <span class="math inline">\(\chi_A\)</span> channel and apply DPI;
both cases <span class="math inline">\(q=1\)</span> and <span class="math inline">\(q\neq 1\)</span> contradict strict convexity.
Claim (2) follows from monotonicity and recognize <span class="math inline">\(P_{Y|x}\circ P_X\)</span> as the
marginal of <span class="math inline">\(P_{Y|X}P_X\)</span>.
Joint convexity follows from standard latent variable argument:
to prove joint convexity
<span class="math display">\[
    D(\lambda P_0 + \bar \lambda P_1 \| Q_0 + \bar \lambda Q_1)
    \leq \lambda D(P_0 \| Q_0) + \bar \lambda D(P_1 \| Q_1)
\]</span>
Take <span class="math inline">\(\theta \sim \mathrm{Ber}_\lambda \to (P, Q)\)</span>, then the RHS is
<span class="math inline">\(D(P_{P|\lambda} \| P_{Q|\lambda} | P_\lambda)\)</span> while the LHS
is <span class="math inline">\(D(P_{P|\lambda}\circ P_\lambda\| P_{Q|\lambda} \circ P_\lambda)\)</span></p>
The following powerful theorem allows us to reduce any general
problem to finite alphabets.
</details>
<div class="theorem">
<p><span id="thm:fFiniteApprox" class="theorem"><strong>Theorem 8.4  (finite approximation theorem) </strong></span>Given two probability measures <span class="math inline">\(P, Q\)</span> on <span class="math inline">\(\mathcal X\)</span> with <span class="math inline">\(\sigma\)</span>-algebra
<span class="math inline">\(\mathcal F\)</span>. Given a finite <span class="math inline">\(\mathcal F\)</span>-measurable partition
<span class="math inline">\(\mathcal E = \{E_1, \cdots, E_n\}\)</span>, define the distribution <span class="math inline">\(P_{\mathcal E}\)</span>
on <span class="math inline">\([n]\)</span> by <span class="math inline">\(P_{\mathcal E}(j) = P[E_j]\)</span>, similarly for <span class="math inline">\(Q\)</span>, then
<span class="math display">\[
    D_f(P\|Q) = \sup_{\mathcal E} D_f(P_{\mathcal E} \| Q_{\mathcal E})
\]</span>
where <span class="math inline">\(\sup\)</span> is over all finite <span class="math inline">\(\mathcal F\)</span>-measurable partitions.</p>
</div>
<p><span style="color:red">
TODO
</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-58" class="definition"><strong>Definition 8.3  (f-information) </strong></span>The <span class="math inline">\(f\)</span>-information is defined by
<span class="math display">\[
    I_f(X; Y) = D_f(P_{XY} | P_XP_Y)
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-59" class="definition"><strong>Definition 8.4  (f-DPI) </strong></span>For <span class="math inline">\(U\to X\to Y\)</span>, we have <span class="math inline">\(I_f(U; Y) \leq I_f(U; X)\)</span>.</p>
</div>
<p>Proof: <span class="math inline">\(I_f(U; X) = D_f(P_{UX} \|P_UP_X) \geq
D_f(P_{UY} \| P_UP_Y) = I_f(U; Y)\)</span>.</p>
</div>
<div id="tv-and-hellinger-hypothesis-testing" class="section level2 unnumbered hasAnchor">
<h2>TV and Hellinger, hypothesis testing<a href="f-divergence.html#tv-and-hellinger-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a <em>binary hypothesis testing</em> problem, one is
given an observation <span class="math inline">\(X\)</span>, which is known to be <span class="math inline">\(X\sim P\)</span>
or <span class="math inline">\(X\sim Q\)</span>. The goal is to decide <span class="math inline">\(\lambda\in \{0, 1\}\)</span>
based on <span class="math inline">\(X\)</span>. In other words,
<span class="math display">\[
    \lambda \to X\to \hat \lambda
\]</span>
Our objective is to find a possibly randomized
decision function <span class="math inline">\(\phi:\mathcal X\to \{0, 1\}\)</span> such that
<span class="math display">\[
    P[\phi(X)=1] + Q[\phi(X) = 0]
\]</span>
is minimized. We will see that optimization leads to TV, while
asymptotic tensorization leads to <span class="math inline">\(H^2\)</span>.</p>
<div class="theorem">
<p><span id="thm:tvVarChar" class="theorem"><strong>Theorem 8.5  (variational characterizations of TV) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sup\)</span>-representation: let
<span class="math inline">\(\mathcal F = \{f:\mathcal X\to \mathbb R, \|f\|_\infty \leq 1\}\)</span>, then
<span class="math display">\[
\mathrm{TV}(P, Q) = \sup_E P(E) - Q(E) = \dfrac 1 2 \sup_{f\in \mathcal F}
\left[\mathbb E_P f(X) - \mathbb E_Q f(X)\right]
\]</span>
Supremum is achieved by <span class="math inline">\(f=\chi_E\)</span>, where <span class="math inline">\(E=\{x:p(x)&gt;q(x)\}\)</span>.</li>
<li><span class="math inline">\(\inf\)</span>-representation: Provided the diagonal is measurable,
<span class="math display">\[
\mathrm{TV}(P, Q) = \min_{P_{XY}} \{P_{XY}[X\neq Y]
\text{ subject to } P_X=P, P_Y=Q\}
\]</span></li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
<p>The upper bound by <span class="math inline">\(\mathrm{TV}\)</span> is intuitive;
to demonstrate saturation, let <span class="math inline">\(E = \{x:p(x)&gt;q(x)\}\)</span>, then
<span class="math display">\[\begin{align}
    0 = \int [p(x) - q(x)]\, d\mu
    &amp;= \int_E + \int_{E^c} [p(x) - q(x)]\, d\mu  \\
    \int_E [q(x) - p(x)]\, d\mu
    &amp;= \int_{E^c} [p(x) - q(x)]\, d\mu
\end{align}\]</span>
The sum of these two integrals (note the definition of <span class="math inline">\(E\)</span>)
equals <span class="math inline">\(2\mathrm{TV}\)</span>, then
<span class="math display">\[
    \mathrm{TV}(P, Q) = \dfrac 1 2 \int \chi_E[(q(x) - p(x)]\, d\mu
    = \dfrac 1 2 \mathbb E_P \chi_E(X) - \mathbb E_Q \chi_E(X)
\]</span>
For the <span class="math inline">\(\inf\)</span> representation, given any coupling <span class="math inline">\(P_{XY}\)</span>,
for <span class="math inline">\(f\in \mathcal F\)</span> we have
<span class="math display">\[
    \mathbb E_P f(X) - \mathbb E_Q f(X)
    = \mathbb E_{P_{XY}}[f(X) - f(Y)]
    \leq 2 P_{XY}[X\neq Y]
\]</span>
This shows that the <span class="math inline">\(\inf\)</span>-representation is always an
upper bound;
<span style="color:green">
we obtain saturation when <span class="math inline">\(X\neq Y\)</span> only happens for
possible values of <span class="math inline">\(X\)</span> disjoint from possible values of <span class="math inline">\(Y\)</span>,
and <span class="math inline">\(f\)</span> is the indicator function
on the disjoint support.
</span>
This is satisfied by the following
construction given <span class="math inline">\(P, Q\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\pi = \int \pi(x)\, d\mu\)</span> denote the overlap scalar, where
<span class="math inline">\(\pi(x) = \min(p(x), q(x))\)</span>.</li>
<li>With probability <span class="math inline">\(\pi\)</span> take <span class="math inline">\(X=Y\)</span> sampled from the overlap density
<span class="math display">\[
r(x) = \dfrac 1 \pi \pi(x)
\]</span></li>
<li>With probability <span class="math inline">\(1-\pi\)</span> sample <span class="math inline">\(X, Y\)</span> independently from
<span class="math display">\[
p_1(x) = \dfrac{p(x) - \pi(x)} {1 - \pi}, \quad
q_1(x)=\dfrac{q(x) - \pi(x)}{1 - \pi}
\]</span>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="images/tvInf.jpeg" alt="Visual representation of joint construction." width="80%" />
<p class="caption">
Figure 8.1: Visual representation of joint construction.
</p>
</div></li>
</ol>
Note that <span class="math inline">\(p_1, q_1\)</span> have disjoint supports.
Now, the marginals are indeed <span class="math inline">\(P, Q\)</span>, and this saturates
the inequality since <span class="math inline">\(P_{XY}[X\neq Y] = 1-\pi=\mathrm{TV}(P, Q)\)</span>
</details>
</div>
<div id="joint-range" class="section level2 unnumbered hasAnchor">
<h2>Joint range<a href="f-divergence.html#joint-range" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first provide a special case of an inequality.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-60" class="theorem"><strong>Theorem 8.6  (Pinsker's inequality) </strong></span>For any two distributions,
<span class="math inline">\(D(P\|Q) \geq (2\log e) \mathrm{TV}(P, Q)^2\)</span></p>
</div>
<details>
<summary>
Proof
</summary>
By DPI, it suffices to consider
Bernoulli distributions via the channel <span class="math inline">\(1_E\)</span>
which results in Bernoulli with parameter
<span class="math inline">\(P(E)\)</span> or <span class="math inline">\(Q(E)\)</span>. Working in natural units,
Pinsker’s inequality
for Bernoulli distributions yield
<span class="math display">\[
    \sqrt{\dfrac 1 2 D(P\|Q)}
    \geq \mathrm{TV}(1_E\circ P, Q_E\circ Q) =
    |P(E) - Q(E)|
\]</span>
Taking supremum over all <span class="math inline">\(E\)</span> yields
the desired inequality per the TV
variational characterization theorem <a href="f-divergence.html#thm:tvVarChar">8.5</a>.
</details>
<div class="definition">
<p><span id="def:unlabeled-div-61" class="definition"><strong>Definition 8.5  (joint range) </strong></span>Given two <span class="math inline">\(f\)</span>-divergences <span class="math inline">\(D_f\)</span> and <span class="math inline">\(D_g\)</span>, their
joint range <span class="math inline">\(\mathcal R\subset [0, \infty]^2\)</span>
is defined by
<span class="math display">\[
    \mathcal R = \mathrm{Image}\left[
        (P, Q)\mapsto (D_f(P\|Q, D_g(P\|Q))
    \right]
\]</span>
The joint range over <span class="math inline">\(k\)</span>-ary distribution is denoted <span class="math inline">\(\mathcal R_k\)</span>.</p>
</div>
<p>Our next result will characterize the set of
<span class="math inline">\(f\)</span>-divergences.</p>
<div class="lemma">
<p><span id="lem:convexBoundaryThm" class="lemma"><strong>Lemma 8.1  (Fenchel-Eggleston-Carathéodory theorem) </strong></span>Let <span class="math inline">\(S\subset \mathbb R^d\)</span> and <span class="math inline">\(x\in \mathrm{co}(S)\)</span>. There exists
a set of <span class="math inline">\(d+1\)</span> points <span class="math inline">\(S&#39;=\{x_1, \cdots x_{d+1}\}\in S\)</span>
such that <span class="math inline">\(x\in \mathrm{co}(S&#39;)\)</span>. If <span class="math inline">\(S\)</span> has at most
<span class="math inline">\(d\)</span> connected components, then <span class="math inline">\(d\)</span> points are enough.</p>
</div>
<p>As a corollary of the following theorem, it
suffices to prove joint range for Bernouli, then
convexify the range.</p>
<div class="theorem">
<p><span id="thm:harremoesVajda" class="theorem"><strong>Theorem 8.7  (Harremoës-Vajda) </strong></span><span class="math inline">\(\mathcal R = \mathrm{co}(\mathcal R_2) = \mathcal R_4\)</span>
where <span class="math inline">\(\mathrm{co}\)</span> denotes the convex hull with
a natural extension of convex operations to
<span class="math inline">\([0, \infty]^2\)</span>. In particular,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathrm{co}(\mathcal R_2) \subset\mathcal R_4\)</span>:
standard latent variable argument.</li>
<li><span class="math inline">\(\mathcal R_k \subset \mathrm{co}(\mathcal R_2) = \mathcal R_4\)</span>.</li>
<li><span class="math inline">\(\mathcal R = \mathcal R_4\)</span>: the approximation
theorem already implies <span class="math inline">\(\mathcal R = \overline{\bigcup_k \mathcal R_k}\)</span>;
the closure is technical.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
<p>First consider claim <span class="math inline">\(1\)</span>.
Construct a convex divergence as follows:
for two pairs of distributions <span class="math inline">\((P_0, Q_0)\)</span> and
<span class="math inline">\((P_1, Q_1)\)</span> on <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\lambda \in [0, 1]\)</span>.
Define the typical Bernoulli latent joint <span class="math inline">\((X, B)\)</span>
by <span class="math inline">\(P_B = Q_B = \mathrm{Ber}(\alpha)\)</span> and <span class="math inline">\((P, Q)_{X|B=j} = (P_j, Q_j)\)</span>.
Applying the conditional divergence to obtain
<span class="math display">\[
    D_f(P_{XB} \| Q_{XB})
    = \bar \alpha D_f(P_0\|Q_0) + \alpha D_f(P_1 \| Q_1)
    \implies \mathrm{co}(\mathcal R_2)\subset \mathcal R_4
\]</span>
Onto the most nontrivial claim <span class="math inline">\(2\)</span>: fixing <span class="math inline">\(k\)</span> and
distributions <span class="math inline">\(P, Q\)</span> on <span class="math inline">\([k]\)</span> with distributions <span class="math inline">\((p_j), (q_j)\)</span>,
w.l.o.g. make <span class="math inline">\(q_{j&gt;1}&gt;0\)</span> and concentrate <span class="math inline">\(q_1=0\)</span> (i.e. concentrate
all empty points onto <span class="math inline">\(j=1\)</span>.
<span style="color:blue">
Consider the equivalence class <span class="math inline">\(\mathcal S\)</span> of all
<span class="math inline">\((\tilde p_j, \tilde q_j)\)</span> <u>which have the same likelihood ratio
on the support</u> of <span class="math inline">\(q\)</span>:
</span>
Let <span class="math inline">\(\phi_{j&gt;1} = p_j / q_j\)</span> and consider
<span class="math display">\[
    \mathcal S = \left\{
        \tilde Q = (\tilde q_j)_{j\in [k]}:
        \tilde q_j\geq 0, \sum \tilde q_j=1, \tilde q_1=0,
    \right\}
\]</span>
Note that <span class="math inline">\(\mathcal S\)</span> it the intersection of</p>
<ol style="list-style-type: decimal">
<li>The simplex of all distributions <span class="math inline">\(\tilde q\)</span>.</li>
<li>The half-space specified by <span class="math inline">\(\tilde q\cdot \phi \leq 1\)</span>.</li>
</ol>
<p>We can next identify the boundary of <span class="math inline">\(\mathcal S_e\subset \mathcal S\)</span>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\tilde q_{j\geq 2}=1\)</span> and <span class="math inline">\(\phi_j\leq 1\)</span>.</li>
<li><span class="math inline">\(\tilde q_{j_1}+\tilde q_{j_2}=1\)</span> and
<span class="math inline">\(\tilde q_{j_1}\phi_{j_1} + \tilde q_{j_2}\phi_{j_2}=1\)</span>.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="images/joint_range.jpeg" alt="Gray line corresponds to $S$; blue and green dots correspond to elements of $S_e$ identified by (1) and (2), respectively. " width="50%" />
<p class="caption">
Figure 8.2: Gray line corresponds to <span class="math inline">\(S\)</span>; blue and green dots correspond to elements of <span class="math inline">\(S_e\)</span> identified by (1) and (2), respectively.
</p>
</div>
<p>We next have <span class="math inline">\(\mathcal S = \mathrm{co}(\mathcal S_e)\)</span>; so for <span class="math inline">\(Q\)</span>,
there exists extreme points <span class="math inline">\(\tilde Q_j\in \mathcal S_e\)</span>
(note that <span class="math inline">\(\mathcal S_e\)</span> is dependent upon <span class="math inline">\(Q\)</span>!) with support on
at most <span class="math inline">\(2\)</span> atoms (binary distributions) such that
<span class="math inline">\(Q = \alpha_j \tilde Q_j\)</span>.
The map asspciating <span class="math inline">\(\tilde P\)</span> given <span class="math inline">\(\tilde Q\)</span> is
<span class="math display">\[
    \tilde p_j = \begin{cases}
        \phi_j \tilde q_j &amp; j\in \{2, \cdots, k\}, \\
        1 - \sum_{j=2}^k \phi_j \tilde q_j &amp; j = 1
    \end{cases}
\]</span>
On this particular set which fixes the likelihood ratio,
the divergence is an affine map:
<span class="math display">\[
    \tilde Q \mapsto D_f(\tilde P \| \tilde Q)
    = \sum_{j\geq 2}\tilde q_j f(\phi_j) + f&#39;(\infty)\tilde p_1 \implies
    D_f(P\|Q) = \sum_{j=1}^m \alpha_i D_f(\tilde P_i \|\tilde Q_i)
\]</span></p>
</details>
<p>We defer detailed examples of joint ranges to the book (7.6).</p>
</div>
<div id="rényi-divergence" class="section level2 unnumbered hasAnchor">
<h2>Rényi divergence<a href="f-divergence.html#rényi-divergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Rényi divergences are a monotone transformation
of <span class="math inline">\(f\)</span>-divergences; they satisfy DPI and other properties.</p>
<div class="definition">
<p><span id="def:unlabeled-div-62" class="definition"><strong>Definition 8.6  (Rényi divergence) </strong></span>For <span class="math inline">\(\lambda\in \mathbb R- \{0, 1\}\)</span>, the Rényi divergence
of order <span class="math inline">\(\lambda\)</span> between distributions <span class="math inline">\(P, Q\)</span> is
<span class="math display">\[
    D_\lambda(P \| Q) = \dfrac 1 {\lambda - 1} \log \mathbb E_Q \left[
        \left(\dfrac{dP}{dQ}\right)^\lambda
    \right]
\]</span></p>
</div>
<p>To see its connection with entropy, note that
<span class="math display">\[
    \mathbb E_Q \left(\dfrac{dP}{dQ}\right)^\lambda
    = \mathrm{sgn}(\lambda - 1) D_f(P\|Q) + 1, \quad f = \mathrm{sgn}(\lambda - 1)(x^\lambda - 1)
\]</span>
with which the Rényi entropy becomes (the <span class="math inline">\(\mathrm{sgn}(\lambda - 1)\)</span> is just there
to keep <span class="math inline">\(f\)</span> convex):
<span class="math display" id="eq:renyiMonotone">\[
    D_\lambda(P\|Q) = \dfrac 1 {\lambda - 1} \log \left[1 +
        \mathrm{sgn}(\lambda - 1) D_f(P\|Q)
    \right], \quad f = \mathrm{sgn}(\lambda - 1)(x^\lambda - 1)
    \tag{8.2}
\]</span></p>
<div class="proposition">
<p><span id="prp:unlabeled-div-63" class="proposition"><strong>Proposition 8.6  </strong></span>Under regularity conditions,
<span class="math display">\[
    \lim_{\lambda \to 1} D_\lambda (P \|Q) = D(P \| Q)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Expand <span class="math inline">\((d_QP)^\lambda = \exp(\lambda \ln d_QP)\)</span> about <span class="math inline">\(\lambda=1\)</span>:
<span class="math display">\[
    (d_QP)^{\lambda} \approx d_QP + \ln d_QP \cdot d_QP \cdot (\lambda - 1)
\]</span>
Taking <span class="math inline">\(\mathbb E_Q\)</span> yields <span class="math inline">\(1 + (\lambda - 1)\mathbb E_P[\ln d_QP]\)</span>; then
substituting into <span class="math inline">\(\log x \approx 1+x\)</span> yields
<span class="math display">\[
    D_{\lambda_\to 1}(P \| Q) = \dfrac 1 {\lambda - 1} \log \left[
        1 + (\lambda - 1)\mathbb E_P[\ln d_QP]
    \right]
    = \mathbb E_P \ln d_QP = D(P \| Q)
\]</span>
</details>
<div class="proposition">
<p><span id="prp:specialRenyi" class="proposition"><strong>Proposition 8.7  (special cases of Rényi divergence) </strong></span>Consider <span class="math inline">\(\lambda = 1/2, 2\)</span>:
<span class="math display">\[
    D_2 = \log(1 + \chi^2), \quad D_{1/2} = -2\log\left(1 - \dfrac{H^2}{2}\right)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The <span class="math inline">\(D_2\)</span> case is apparant in light of equation <a href="f-divergence.html#eq:renyiMonotone">(8.2)</a>.
Substitute <span class="math inline">\(\lambda = 1/2\)</span>:
<span class="math display">\[
    D_{1/2} = \dfrac 1 {1/2 - 1} \log[1 - D_{x\mapsto 1 - \sqrt x}(P\|Q)]
\]</span>
It remains to show that <span class="math inline">\(D_{1 - \sqrt x} (P\|Q) = \dfrac{H^2}{2}\)</span>, applying
equation <a href="f-divergence.html#eq:hellingerDiv">(8.1)</a>
<span class="math display">\[
    \mathbb E_Q\left[1 - \sqrt{d_QP}\right]
    = \dfrac 1 2 \mathbb E_Q \left(1 - \sqrt{d_QP}\right)^2 = 1 - \int \sqrt{dPdQ}
\]</span>
</details>
<p>Several other properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\lambda \mapsto D_\lambda D(P\|Q)\)</span> is non-decreasing and
<span class="math inline">\(\lambda \mapsto (1 - \lambda) D_\lambda(P\|Q)\)</span> is concave.</li>
<li>For <span class="math inline">\(\lambda \in [0, 1]\)</span> the divergence <span class="math inline">\(D_\lambda\)</span> is jointly convex.</li>
<li>The Rényi entropy for finite alphabet is <span class="math inline">\(H_\lambda(P) = \log m - D_\lambda(P \| U)\)</span>.</li>
</ol>
<div class="definition">
<p><span id="def:unlabeled-div-64" class="definition"><strong>Definition 8.7  (conditional Rényi entropy) </strong></span>Given <span class="math inline">\(P_{X|Y}, Q_{X|Y}\)</span> and <span class="math inline">\(P_Y\)</span>
<span class="math display">\[\begin{align}
    D_\lambda(P_{X|Y} \| Q_{X|Y} | P_Y)
    &amp;= D_\lambda(P_{X|Y} P_Y \| Q_{X|Y} P_Y)
    = \dfrac 1 {\lambda - 1} \log \mathbb E_{Q_{X|Y}P_Y}
        \left[
            \dfrac{(P_{X|Y}P_Y)(X, Y)}{(Q_{X|Y}P_Y)(X, Y)}
        \right]^\lambda \\
    &amp;= \dfrac 1 {\lambda - 1} \log \mathbb E_{y\sim P_Y}
    \int_{\mathcal X} P_{X|Y=y}(x)^\lambda Q_{X|Y=y}(x)^{1-\lambda}
\end{align}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-65" class="proposition"><strong>Proposition 8.8  (Rényi chain rule) </strong></span>Given <span class="math inline">\(P_{AB}, Q_{AB}\)</span>, define the <span class="math inline">\(\lambda\)</span>-tilting
of <span class="math inline">\(P_B\)</span> towards <span class="math inline">\(Q_B\)</span> by
<span class="math display">\[
    P_B^{(\lambda)}(b) = P_B^\lambda(b) Q_B^{1-\lambda}(b)
    \exp \left[
        -(\lambda - 1) D_\lambda(P_B \| Q_B)
    \right]
\]</span>
joint Rényi divergence decomposes as
<span class="math display">\[
    D_\lambda(P_{AB} \| Q_{AB}) = D_\lambda(P_B \| Q_B) +
    D_\lambda( P_{A|B} \| Q_{A|B} | P_B^{(\lambda)})
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
First need to prove that <span class="math inline">\(P_B^{(\lambda)}\)</span> is indeed
correctly normalized:
<span class="math display">\[\begin{align}
    \exp \left[
        -(\lambda - 1) D_\lambda(P_B \| Q_B)
    \right]
    &amp;= \mathbb E_Q \left[\left(\dfrac{dP}{dQ}\right)^\lambda\right]
    = \int P_B^\lambda(b) Q_B^{1-\lambda}(b)\, db
\end{align}\]</span>
Next up, computing the RHS explicitly, we have
<span class="math display">\[\begin{align}
    (\lambda - 1) [D_\lambda(P_B \| Q_B) +
    D_\lambda( P_{A|B} \| Q_{A|B} | P_B^{(\lambda)})]
    &amp;= \log \int_{\mathcal B} \left[
        P_B(b)^\lambda Q_B(b)^{1-\lambda}
        \cdot \int_{\mathcal A} P_{A|B=b}(a)^\lambda Q_{A|B=b}(a)^{1-\lambda}
    \right] \\
    &amp;= \log \mathbb E_{Q_{AB}} \left(\dfrac{dP_{AB}}{dQ_{AB}}\right)^\lambda
    = D_\lambda(P_{AB} \| Q_{AB})
\end{align}\]</span>
</details>
<div class="corollary">
<p><span id="cor:unlabeled-div-66" class="corollary"><strong>Corollary 8.1  (Rényi tensorization) </strong></span>Specializing the chain rule to independent joints,
<span class="math display">\[
    D_\lambda \left(\prod P_{X_j} \| \prod Q_{X_j}\right)
    = \sum_j D_\lambda(P_{X_j} \| Q_{X_j})
\]</span></p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-67" class="corollary"><strong>Corollary 8.2  (tensorization of χ² and Hellinger) </strong></span>Applying tensorization and proposition <a href="f-divergence.html#prp:specialRenyi">8.7</a>:
<span class="math display">\[\begin{align}
    1 + \chi^2 \left(\prod_j P_j \| \prod_j Q_j\right)
    &amp;= \prod_j 1 + \chi^2(P_j \| Q_j)  \\
    1 - \dfrac 1 2 H^2 \left(\prod P_j, \prod Q_j\right)
    &amp;= \prod_j 1 - \dfrac 1 2 H^2(P_j, Q_j)
\end{align}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-68" class="proposition"><strong>Proposition 8.9  (variational characterization via KL) </strong></span>Show that <span class="math inline">\(\forall \alpha \in \mathbb R\)</span>:
<span class="math display">\[
    \bar \alpha D_\alpha(P \|Q)
    = \inf_R \left[
        \alpha D(R\|P) + \bar \alpha D(R\|Q)
    \right]
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The KL case <span class="math inline">\(\alpha=1\)</span> holds trivially with <span class="math inline">\(R=P\)</span>.
otherwise expand the LHS to
<span class="math display">\[\begin{align}
    \bar \alpha D_\alpha(P\|Q)
    &amp;= -\log \mathbb E_Q \left(\dfrac{dP}{DQ}\right)^\alpha
    = \log \mathbb E_Q \left(
        \dfrac{dQ}{dP}
    \right)^\alpha  
\end{align}\]</span>
Expand the RHS to
<span class="math display">\[\begin{align}
    \alpha D(R\|P) + \bar \alpha D(R\|Q)
    &amp;= \mathbb E_R \log \left[
        \left(\dfrac{dR}{dP}\right)^\alpha
        \left(\dfrac{dR}{dQ}\right)^{\bar \alpha}
    \right]
    = \mathbb E_R \log
        \dfrac{dR}{dP^\alpha dQ^{\bar \alpha}} \\
    &amp;= \mathbb E_R \log \dfrac{dR}{dQ} \cdot \left(\dfrac{dQ}{dP}\right)^\alpha
    = D(R \| Q) + \mathbb E_R \log \left(\dfrac{dQ}{dP}\right)^\alpha
\end{align}\]</span>
We wish to establish the bound
<span class="math display">\[\begin{align}
    \log \mathbb E_Q \left(
        \dfrac{dQ}{dP}
    \right)^\alpha
    \leq \mathbb E_R \log \left[
        \dfrac{dR}{dP^\alpha dQ^{\bar \alpha}}
    \right]
    &amp;= D(R \| Q) + \mathbb E_R \log \left(\dfrac{dQ}{dP}\right)^\alpha  \\
    D(R\|Q) &amp;\geq \mathbb E_R \log \left(\dfrac{dP}{dQ}\right)^\alpha
    - \log \mathbb E_Q \left(\dfrac{dP}{dQ}\right)^\alpha
\end{align}\]</span>
Comparison between <span class="math inline">\(\log \mathbb E\)</span> and <span class="math inline">\(\mathbb E\log\)</span> screams Donsker-Varadhan
<a href="variational-measures-of-information.html#thm:donskerVaradhan">5.5</a>:
<span class="math display">\[
    D(R \|Q) \geq  \mathbb E_R f - \log \mathbb E_Q \exp f
\]</span>
for <span class="math inline">\(f = \alpha \log(dP/dQ)\)</span> being the likelihood ratio.
Recall that the saturation constraint is
<span class="math display">\[
    \log \left(\dfrac{dP}{dQ}\right)^\alpha = f = \log \dfrac{dR}{dQ} + C \iff
    \dfrac{P(x)^\alpha}{Q(x)^\alpha} = \dfrac{R(x)}{Q(x)} \iff
    dR \propto dP^\alpha dQ^{\bar \alpha}
\]</span>
the proportionality freedom comes from invariance of <span class="math inline">\(f\mapsto f+C\)</span>
in Donsker-Varadhan and is determined by the normalization constraint.
The final result is simply the Renyi-tilt of <span class="math inline">\(P\)</span> towards <span class="math inline">\(Q\)</span> given by
<span class="math display">\[
    R(x) = P(x)^\alpha Q(x)^{\bar \alpha} \exp \left[
        -\bar \alpha D_\alpha(P \| Q)
    \right]
\]</span>
</details>
</div>
<div id="variational-characterizations" class="section level2 unnumbered hasAnchor">
<h2>Variational characterizations<a href="f-divergence.html#variational-characterizations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a convex function <span class="math inline">\(f:(0, \infty)\to \mathbb R\)</span>,
recall that is convex conjugate <span class="math inline">\(f^*:\mathbb R\to \mathbb R\cup \{+\infty\}\)</span>
is defined by
<span class="math display">\[
    f^*(y) = \sup_{x\in \mathbb R_+} xy - f(x)
\]</span>
The Legendre transform is convex, involutary, and satisfies
<span class="math display">\[
    f(x) + f^*(y) \geq xy
\]</span>
Similarly, the convex conjugate of any convex
functional <span class="math inline">\(\Psi(P)\)</span> is
<span class="math display">\[
    \Psi^*(g) = \sup_P \int g\, dP - \Psi(P), \quad
    \Psi(P) = \sup_g \int g\, dP - \Psi^*(g)
\]</span>
In this section, we identify <span class="math inline">\(f:(0, \infty)\to \mathbb R\)</span>
with its convex extension <span class="math inline">\(f:\mathbb R\to \mathbb R\cup \{+\infty\}\)</span>
(one can just take <span class="math inline">\(f = \infty\)</span> for all inputs outside
the original domain); different choices of <span class="math inline">\(f\)</span>’s extension
yields a different variational characterization.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-69" class="theorem"><strong>Theorem 8.8  (supremum characterization of f-divergences) </strong></span>Given probability measures <span class="math inline">\(P, Q\)</span> on <span class="math inline">\(\mathcal X\)</span>; fix
an extension of <span class="math inline">\(f\)</span> and corresponding Legendre transform <span class="math inline">\(f^*\)</span>;
denote <span class="math inline">\(\mathrm{dom}(f^*) = (f^*)^{-1}(\mathbb R-\{\infty\})\)</span>, then
<span class="math display">\[
    D_f(P\|Q) = \sup_{g:\mathcal X\to \mathrm{dom}(f^*)}
    \mathbb E_P[g(X)] - \mathbb E_Q[(f^*\circ g)(X)]
\]</span></p>
</div>
</div>
<div id="empirical-distribution-and-χ²" class="section level2 unnumbered hasAnchor">
<h2>Empirical distribution and χ²<a href="f-divergence.html#empirical-distribution-and-χ²" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="fisher-information-location-family" class="section level2 unnumbered hasAnchor">
<h2>Fisher information, location family<a href="f-divergence.html#fisher-information-location-family" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We consider a parameterized set of distributions
<span class="math inline">\(\{P_\theta: \theta\in \Theta\}\)</span> where <span class="math inline">\(\Theta\)</span> is an open
subset of <span class="math inline">\(\mathbb R^d\)</span>. Further suppose that <span class="math inline">\(P_\theta(dx)=p_\theta(x)\mu(dx)\)</span>
for some common dominating measure <span class="math inline">\(\mu\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-70" class="definition"><strong>Definition 8.8  (Fisher information, score) </strong></span>If for each fixed <span class="math inline">\(x\)</span>, the density <span class="math inline">\(p_\theta(x)\)</span> depends smoothly
on <span class="math inline">\(\theta\)</span>, we define the Fisher information matrix w.r.t. <span class="math inline">\(\theta\)</span> as
<span class="math display">\[
    \mathcal J_F(\theta) = \mathbb E_\theta(VV^T), \quad
    V = \nabla_\theta \ln p_\theta(X) = \dfrac 1 {p_\theta(X)} \nabla_\theta p_\theta(X)
\]</span>
The <em>random variable</em> <span class="math inline">\(V\)</span> is known as the score.</p>
</div>
<p>Under technical regularity conditions, we have:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb E_\theta[V]=0\)</span>: for an intuitive argument
<span class="math display">\[
\mathbb E_\theta[\nabla_\theta \ln p_\theta(X)]
= \int \nabla_\theta p_\theta(x) = \nabla_\theta \int p_\theta = 0
\]</span></li>
<li><span class="math inline">\(\mathcal J_F(\theta) = \mathrm{Cov}_\theta(V)
= -\mathbb E_\theta \left[\mathcal H_\theta \ln p_\theta(X)\right]\)</span>: differentiating (1)
<span class="math display">\[\begin{align}
\partial_{jk}^2 \ln p_\theta = \partial_{j} \left(
     \dfrac 1 {p_\theta} \partial_{k} p_\theta
\right) = -\dfrac{(\partial_{j} p_\theta)(\partial_{k} p_\theta)} {p_\theta^2}
+ \left[\dfrac 1 {p_\theta} \partial_{jk}^2 p_\theta\right]_{=0} = (VV^T)_{jk}
\end{align}\]</span>
The second term vanishes when we take the expectation.</li>
<li><span class="math inline">\(D(P_{\theta} \| P_{\theta + \xi})
= \dfrac{\log e}{2} \xi^T \mathcal J_F(\theta)\xi + o(\|\xi\|^2)\)</span>:
this is obtained by integrating
<span class="math display">\[
\ln p_{\theta + \xi}(x) = \ln p_\theta(x) + \xi^T \nabla_\theta \ln p_\theta(x)
+ \dfrac 1 2 \xi^T \mathcal H_\theta \ln p_\theta(x) + o(\|\xi\|^2)
\]</span></li>
<li>Under a smooth invertible map <span class="math inline">\(\theta \to \tilde \theta\)</span>, we have
<span class="math display">\[
\mathcal J_F(\tilde \theta) = J_{\tilde \theta \to \theta}^T
\mathcal J_F(\theta) J_{\tilde \theta\to \theta}
\]</span>
In fact, this allows one to define a Riemannian metric on the parameter space,
called the Fisher-Rao metric.</li>
<li>Tensorization: under i.i.d. observations, <span class="math inline">\(X^n\sim P_\theta\)</span> i.i.d, we have
<span class="math inline">\(\{P_\theta^{\otimes n}:\theta \in \Theta\}\)</span> whose Fisher information
<span class="math inline">\(\mathcal J_F^{\otimes n}(\theta) = n\mathcal J_F(\theta)\)</span>.</li>
</ol>
<div class="definition">
<p><span id="def:unlabeled-div-71" class="definition"><strong>Definition 8.9  (location family) </strong></span>For any density <span class="math inline">\(p_0\)</span> on <span class="math inline">\(\mathbb R^d\)</span>, one can define a location
family of distributions on <span class="math inline">\(\mathbb R^d\)</span> by setting
<span class="math inline">\(P_\theta(dx) = p_0(x-\theta)dx\)</span>. In this case the Fisher information is
translation-invariant, does not depend on <span class="math inline">\(\theta\)</span>, and its written instead
to emphasize its dependence on <span class="math inline">\(p_0\)</span>:
<span class="math display">\[
    J(p_0) = \mathbb E_{X\sim p_0} \left[(\nabla \ln p_0)(\nabla \ln p_0)^T\right]
    = -\mathbb E_{X\sim p_0} \left[
        \mathcal H_X \ln p_0
    \right]
\]</span></p>
</div>
</div>
<div id="local-χ²-behavior" class="section level2 unnumbered hasAnchor">
<h2>Local χ² behavior<a href="f-divergence.html#local-χ²-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The first theorem gives the linear coefficient of
<span class="math inline">\(D_f(\lambda P + \bar \lambda Q \| Q)\)</span> as <span class="math inline">\(\lambda\to 0^+\)</span>.
When <span class="math inline">\(P\ll Q\)</span>, the decay is always sublinear with
quadratic coefficient given by theorem <a href="f-divergence.html#thm:localChiExpansion">8.10</a>.
Else the linear coefficient is given by the theorem below.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-72" class="theorem"><strong>Theorem 8.9  (local linear coefficient) </strong></span>Suppose <span class="math inline">\(D_f(P\|Q)&lt;\infty\)</span> and <span class="math inline">\(f&#39;(1)\)</span> exists, then
<span class="math display">\[
    \lim_{\lambda\to 0} \dfrac 1 \lambda D_f(\lambda P + \bar \lambda Q \|Q)
    = (1 - P[Q\neq 0])f&#39;(\infty)
\]</span>
In particular, for <span class="math inline">\(P\ll Q\)</span>, the decay is always sublinear.</p>
</div>
<details>
<summary>
Proof
</summary>
Per proposition <a href="f-divergence.html#prp:fEquivProperties">8.4</a> we may assume
<span class="math inline">\(f(1)=f&#39;(1)=0\)</span> and <span class="math inline">\(f\geq 0\)</span>. Decompose <span class="math inline">\(p\)</span> into absolutely continuous
and singular parts
<span class="math display">\[
    P = \mu P_1 + \bar \mu P_0, \quad P_0\perp Q, \quad P_1\ll Q
\]</span>
Expand out the definition of divergence to obtain
<span class="math display">\[
    \dfrac 1 \lambda D_f(\lambda P + \bar \lambda Q \| Q)
    = \bar \mu f&#39;(\infty) + \int dQ \dfrac 1 \lambda f\left[
        1 + \lambda \left(
            \mu \dfrac{dP_1}{dQ} - 1
        \right)
    \right]
\]</span>
The function <span class="math inline">\(g(\lambda) = f(1 + \lambda t)\)</span> is positive and
convex for every <span class="math inline">\(\in \mathbb R\)</span>, then <span class="math inline">\(\dfrac 1 \lambda g(\lambda)\)</span>
decreases monotonically to <span class="math inline">\(g&#39;(0)=0\)</span> as <span class="math inline">\(\lambda \to 0^+\)</span>
(picture a zero-centered convex function). The integrand is
<span class="math inline">\(Q\)</span>-integrable at <span class="math inline">\(\lambda=1\)</span> (which dominates <span class="math inline">\(g(t, \lambda\leq 1)\)</span>,
then applying dominated covergence theorem yields the desired result.
</details>
<div class="theorem">
<p><span id="thm:localChiExpansion" class="theorem"><strong>Theorem 8.10  (local quadratic coefficient) </strong></span>Let <span class="math inline">\(f\)</span> be twice continuously differentiable on <span class="math inline">\((0, \infty)\)</span>
with <span class="math inline">\(\limsup_{x\to \infty} f&#39;&#39;(x)\)</span> finite.
If <span class="math inline">\(\chi^2(P\|Q)&lt;\infty\)</span> (in particular, <span class="math inline">\(P\ll Q\)</span>), then
<span class="math inline">\(D_f(\lambda P + \bar \lambda Q \|Q)\)</span> is finite for all
<span class="math inline">\(\lambda \in [0, 1]\)</span> and
<span class="math display">\[
    \lim_{\lambda \to 0} \dfrac 1 {\lambda^2} D_f(\lambda P + \bar \lambda Q \| Q)
    = \dfrac{f&#39;&#39;(1)}{2} \chi^2(P\|Q)
\]</span>
If <span class="math inline">\(\chi^2(P\|Q)=\infty\)</span> with <span class="math inline">\(f&#39;&#39;(1)&gt;0\)</span>, then
<span class="math inline">\(D_f(\bar \lambda Q + \lambda P \|Q) = \omega(\lambda^2)\)</span>.</p>
</div>
Since <span class="math inline">\(P\ll Q\)</span>, we can use the expression <span class="math inline">\(D_f(P\|Q) = \mathbb E_Q[f(d_QP)]\)</span>.
W.l.o.g. again assume <span class="math inline">\(f&#39;(1) = f(1)=0\)</span>, then <span class="math inline">\(\chi^2\)</span> is written as
<span class="math display">\[\begin{align}
    D_f(\bar \lambda Q + \lambda P \| Q)
    &amp;= \int q(x) f\left(\lambda \dfrac{p(x)}{q(x)} + \bar \lambda\right)
    \, d\mu \\
    f(1+u) &amp;= \int
\end{align}\]</span>
<details>
<summary>
Proof
</summary>
</details>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tensorization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-compression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
