<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 f-Divergence | 6.7480 Notes</title>
  <meta name="description" content="8 f-Divergence | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="8 f-Divergence | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 f-Divergence | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tensorization.html"/>
<link rel="next" href="statistical-decision-applications.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html"><i class="fa fa-check"></i><b>12</b> Noisy Channel Coding</a>
<ul>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#optimal-decoder-weak-converse"><i class="fa fa-check"></i>Optimal decoder, weak converse</a></li>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#random-and-maximal-coding"><i class="fa fa-check"></i>Random and maximal coding</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>13</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="f-divergence" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> f-Divergence<a href="f-divergence.html#f-divergence" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(f\)</span>-divergence is equivalent modulo <span class="math inline">\(c(t-1)\)</span>;
convexity plus <span class="math inline">\(f(1)=f&#39;(1)=1\)</span> (which implies <span class="math inline">\(f\geq 0\)</span>) implies locally <span class="math inline">\(\chi^2\)</span>.</li>
<li>Convexity of <span class="math inline">\(D_f\)</span> <a href="f-divergence.html#thm:fInfoProperties">8.3</a> <span class="math inline">\(\iff\)</span>
DPI <a href="f-divergence.html#thm:fDPI">8.2</a> <span class="math inline">\(\iff\)</span>
monotonicity <a href="f-divergence.html#thm:fMonotonicity">8.1</a> <span class="math inline">\(\iff\)</span> convexity of <span class="math inline">\(f\)</span>.</li>
<li>There is no chain rule for <span class="math inline">\(f\)</span>-divergences other than
KL-divergence. Rényi divergence enjoys tensorization and chain rule (up to tilting);
it includes KL, <span class="math inline">\(\chi^2\)</span> and Hellinger as special cases.
The Renyi chain rule does not imply the divergence chain rule,
but <span class="math inline">\(\chi^2\)</span> has a special chain rule (proposition <a href="f-divergence.html#prp:chiSqChain">8.6</a>).</li>
<li>TV as binary hypothesis testing: to reason about TV, break spaces
into <span class="math inline">\(dP&gt;dQ\)</span> and <span class="math inline">\(dP&lt;dQ\)</span>.</li>
<li>Several divergences are metrics (JS, <span class="math inline">\(H^2\)</span>); one fast way to
compare is to compare to metric, use triangle inequality,
then use comparison inequality again.</li>
<li>TV does not enjoy tensorization properties;
Hellinger divergence is both a metric and tensorizes well.</li>
<li>A powerful approximation theorem <a href="f-divergence.html#thm:fFiniteApprox">8.4</a>
reduces arbitrary spaces to finite ones. Similarly,
the Harremoës-Vajda theorem <a href="f-divergence.html#thm:harremoesVajda">8.8</a>
concludes the problem of joint range.</li>
</ol>
<p>Locality and Fisher information:</p>
<ol style="list-style-type: decimal">
<li>Most <span class="math inline">\(f\)</span>-divergences (with bounded <span class="math inline">\(f&#39;&#39;\)</span>) are locally <span class="math inline">\(\chi^2\)</span>:
theorem <a href="f-divergence.html#thm:localChiExpansion">8.11</a> <u>with respect to mixture
interpolations</u>.</li>
<li>Fisher information is only defined <u>with respect to a
family of distributions</u> parameterized by <span class="math inline">\(\theta\)</span>.
<ul>
<li>It is the expectation of the Hessian of the log-pdf about <span class="math inline">\(\theta\)</span>:
equation <a href="f-divergence.html#eq:fisherHessian">(8.6)</a>.</li>
<li>For regular families, <span class="math inline">\(\chi^2\)</span>
(mixture interpolations, in particular) is locally determined
by the Fisher information <a href="f-divergence.html#thm:localFisherDiv">8.12</a>.</li>
</ul></li>
<li>Fisher information matrix is non-negative, monotonic, has its own
chain rule <a href="f-divergence.html#prp:fisherChainRule">8.13</a> and variational characterization.</li>
</ol>
<div id="definition-1" class="section level2 unnumbered hasAnchor">
<h2>Definition<a href="f-divergence.html#definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-47" class="definition"><strong>Definition 8.1  (f-divergence) </strong></span>Given a convex function <span class="math inline">\(f:(0, \infty)\to \mathbb R\)</span> with <span class="math inline">\(f(1)=0\)</span>,
for every two probability distributions over
<span class="math inline">\(\mathcal X\)</span>, if <span class="math inline">\(P\ll Q\)</span> then the <span class="math inline">\(f\)</span> divergence is
<span class="math display">\[
        D_f(P\|Q) = \mathbb E_Q \left[
            f\left(\dfrac{dP}{dQ}\right)
        \right]
    \]</span>
Here <span class="math inline">\(f(0) = f(0_+)\)</span> per limit. More generally,
define <span class="math inline">\(f&#39;(\infty) = \lim_{x\to 0^+} xf(1/x)\)</span>,
we have
<span class="math display">\[
        D_f(P\|Q) = \int_{q&gt;0} q(x) f\left[
            \dfrac{p(x)}{q(x)}
        \right] \, d\mu + f&#39;(\infty) P[Q=0]
    \]</span>
this last generalization is needed to account for
divergences like total variation.
Intuitively, sums for terms with <span class="math inline">\(dQ=0\)</span> are like
<span class="math display">\[
        \int_{dQ=0} dQ f\left(\dfrac{dP}{dQ}\right)
        = \lim_{t=dP/dQ\to 0^+} \int \dfrac{dP}{t} f(1/t)
    \]</span></p>
</div>
<p>Examples of <span class="math inline">\(f\)</span>-divergences:</p>
<ol style="list-style-type: decimal">
<li><strong>KL-divergence</strong>: <span class="math inline">\(f(x)=x\log x\)</span> to recover KL-divergence.</li>
<li><strong>Total variation (TV)</strong>: <span class="math inline">\(f(x) = \dfrac 1 2 |x - 1|\)</span>:
<span class="math display">\[
\mathrm{TV}(P, Q) = \dfrac 1 2 \mathbb E_Q \left|\dfrac{dP}{dQ} - 1\right|
= \dfrac 1 2 \int |dP - dQ| = 1 - \int d(P\wedge Q)
\]</span>
Recall that <span class="math inline">\(P\wedge Q\)</span> is the pointwise minimum measure
so <span class="math inline">\(\int d(P\wedge Q)\)</span> is the overlap measure.</li>
<li><strong><span class="math inline">\(\chi^2\)</span>-divergence</strong>: <span class="math inline">\(f(x)=(x-1)^2\)</span>.
<span class="math display" id="eq:chiSq">\[
\chi^2(P\|Q) = \mathbb E_Q \left(\dfrac{dP}{dQ} - 1\right)^2
= \int \dfrac{(dP - dQ)^2}{dQ}
= \int \dfrac{dP^2}{dQ} + dQ - 2dP = \int \dfrac{dP^2}{dQ} - 1
\tag{8.1}
\]</span></li>
<li><strong>Squared Hellinger distance</strong>: <span class="math inline">\(f(x) = \left(1 - \sqrt x\right)^2\)</span>.
<span class="math display" id="eq:hellingerDiv">\[
H^2(P, Q) = \int \left(\sqrt{dP} - \sqrt{dQ}\right)^2
= 2 - 2\int \sqrt{dPdQ}
\tag{8.2}
\]</span>
The quantity <span class="math inline">\(B(P, Q) = \int \sqrt{dPdQ}\)</span> is the
<strong>Bhattacharyya coefficient</strong> (Hellinger affinity).
Note that <span class="math inline">\(H(P, Q) = \sqrt{H^2(P, Q)}\)</span>.
The Hellinger distance is <span class="math inline">\(H(P, Q) = \sqrt{H^2(P, Q)}\)</span>.</li>
<li><strong>Le Cam divergence</strong>: <span class="math inline">\(f(x) = \dfrac{1-x}{2x+2}\)</span>,
<span class="math display">\[
\mathrm{LC}(P, Q) = \dfrac 1 2 \int \dfrac{(dP - dQ)^2}{dP + dQ}
\]</span>
The square root <span class="math inline">\(\sqrt{\mathrm{LC}(P, Q)}\)</span>, the Le Cam distance,
is a metric.</li>
<li><strong>Jensen-Shannon divergence</strong> take
<span class="math inline">\(f(x) = x\log \dfrac{2x}{x+1} + \log \dfrac 2 {x+1}\)</span>.
<span class="math display">\[
\mathrm{JS}(P, Q) = D\left(P \| \dfrac{P+Q}{2}\right)
+ D\left(Q \| \dfrac{P+Q}{2}\right)
\]</span></li>
</ol>
<div class="proposition">
<p><span id="prp:unlabeled-div-48" class="proposition"><strong>Proposition 8.1  </strong></span><span class="math inline">\(\mathrm{TV}(P, Q) = \dfrac 1 2 \int |dP - dQ| = 1 - \int d(P\wedge Q)\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Let <span class="math inline">\(E = \{x:dP &gt; dQ\}\)</span>, then
<span class="math display">\[\begin{align}
    \int |dP - dQ|
    &amp;= \int_E dP - d(P\wedge Q) + \int_{E^c} dQ - d(P\wedge Q)
    = \int_E dP + \int_{E^c} dQ - \int d(P\wedge Q) \\
    &amp;= \int_E dP + \int_{E^c} d(P\wedge Q)_{=dP} + \int_{E^c} dQ + \int_E d(P\wedge Q)_{=dQ}
    - 2\int d(P\wedge Q) \\
    &amp;= 2 - 2\int d(P\wedge Q)
\end{align}\]</span>
</details>
<div class="proposition">
<p><span id="prp:unlabeled-div-49" class="proposition"><strong>Proposition 8.2  </strong></span>The following quantities derived from divergences are
metrics on the space of probability distributions
<span class="math display">\[
    \mathrm{TV}(P, Q), \quad H(P, Q), \quad \sqrt{\mathrm{JS}(P, Q)}, \quad \sqrt{\mathrm{LC}(P, Q)}
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-50" class="proposition"><strong>Proposition 8.3  (closure properties) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D_f(Q \| P) = D_g(P \|Q)\)</span> for <span class="math inline">\(g(x) = xf(1/x)\)</span>.</li>
<li><span class="math inline">\(D_f(P\|Q)\)</span> is a <span class="math inline">\(f\)</span>-divergence, then
<span class="math display">\[
D_f(\lambda P + \bar \lambda Q \|Q), \quad
D_f(P \| \lambda P + \bar \lambda Q), \quad
\forall \lambda\in [0, 1]
\]</span>
are <span class="math inline">\(f\)</span>-divergences.</li>
<li>Linearity: <span class="math inline">\(D_{f+g} = D_f + D_g\)</span>.</li>
<li>Distinguishability: <span class="math inline">\(D_f(P \| P) = 0\)</span></li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
For the first claim,
<span class="math display">\[
    D_g(P \|Q) = \int dQ \left(\dfrac{dP}{dQ}\right) f\left(\dfrac{dQ}{dP}\right)
    = D_f(Q \| P)
\]</span>
For the second claim, the other case can be obtained by using the equation above.
<span class="math display">\[
    D_f(\lambda P + \bar \lambda Q \|Q)
    = \int dQ f\left(\lambda \dfrac{dP}{dQ} + \bar \lambda\right)
    \implies \tilde f(x) = f\left(\lambda x + \bar \lambda\right)
\]</span>
</details>
<div class="proposition">
<p><span id="prp:fEquivProperties" class="proposition"><strong>Proposition 8.4  (equivalence properties) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D_f(P\|Q) = 0\)</span> for all <span class="math inline">\(P\neq Q\)</span> iff <span class="math inline">\(f(x)=c(x-1)\)</span> for some <span class="math inline">\(c\)</span>.</li>
<li><span class="math inline">\(D_f = D_{f+c(x-1)}\)</span>; thus we can always assume <span class="math inline">\(f\geq 0\)</span> and <span class="math inline">\(f&#39;(1)=0\)</span>.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
Claim <span class="math inline">\(1\)</span> proceeds by computation (assuming continuity)
<span class="math display">\[
    D_f(P \|Q) = c \int (dP / dQ - 1) dQ = 0
\]</span>
This means that <span class="math inline">\(c(x-1)\)</span> is in the kernel of the linear operator
<span class="math inline">\(f\mapsto D_f\)</span>. Pick <span class="math inline">\(c=-f&#39;(1)\)</span>, then <span class="math inline">\(f(1)=f&#39;(1)=0\)</span>; by convexity <span class="math inline">\(f\geq 0\)</span>.
</details>
<div class="proposition">
<p><span id="prp:specialfMonotonicity" class="proposition"><strong>Proposition 8.5  (special case of monotonicity) </strong></span><span style="color:green">
Joint divergence is unchanged through the same channel
</span>
<span class="math display">\[
    D_f(P_XP_{Y|X} \| Q_XP_{Y|X}) = D_f(P_X \| Q_X)
\]</span>
in particular, for the source-agnostic channel we have
<span class="math display">\[
    D_f(P_XP_Y \| Q_XP_Y) = D_f(P_X \| Q_X)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Direct computation
<span class="math display">\[\begin{align}
    D_f(P_XP_{Y|X} \| Q_XP_{Y|X})
    &amp;= \int Q_X(x) dx \int P_{Y|X=x}(y)dy\,
    f\left[\dfrac{P_X(x) P_{Y|X=x}(y)}{Q_X(x) Q_{Y|X=x}(y)}\right]\\
    &amp;= \int Q_X(x) dx f\left(\dfrac{P_X(x)}{Q_X(x)}\right)
    = D_f(P_X \| Q_X)
\end{align}\]</span>
</details>
</div>
<div id="information-properties-mi" class="section level2 unnumbered hasAnchor">
<h2>Information properties, MI<a href="f-divergence.html#information-properties-mi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:fMonotonicity" class="theorem"><strong>Theorem 8.1  (monotonicity) </strong></span>The joint is more distinguishable than the marginal:
<span class="math display">\[
    D_f(P_{XY} \| Q_{XY}) \geq D_f(P_X \| Q_X)
\]</span>
inequality is saturated when <span class="math inline">\(P_{Y|X} = Q_{Y|X}\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Assume <span class="math inline">\(P_{XY} \ll Q_{XY}\)</span>, then expand and
apply Jensen’s inequality
<span class="math display">\[\begin{align}
    D_f(P_{XY} \| Q_{XY})
    &amp;= \mathbb E_{X\sim Q_X} \mathbb E_{Y\sim Q_{Y|X}} f\left(
        \dfrac{dP_{Y|X} P_X}{dQ_{Y|X}Q_X}
    \right)
    \geq \mathbb E_{X\sim Q_X} f\left(\mathbb E_{Y\sim Q_{Y|X}}
        \dfrac{dP_{Y|X} P_X}{dQ_{Y|X}Q_X}
    \right) \\
    &amp;\geq \mathbb E_{X\sim Q_X} f\left(
        \dfrac{dP_X}{dQ_X}
    \right) = D_f(P_X \| Q_X)
\end{align}\]</span>
To be more careful, on the first line we have
<span class="math display">\[\begin{align}
    \mathbb E_{Y\sim Q_{Y|X=x}}
        \dfrac{P_{Y|X=x}(y) P_X(x)}{Q_{Y|X=x}(y)Q_X(x)}
    &amp;= \dfrac{P_X(x)}{Q_X(x)} \sum_y
    Q_{Y|X=x}(y) \dfrac{P_{Y|X=x}(y)}{Q_{Y|X=x}(y)}
    = \dfrac{P_X(x)}{Q_X(x)}
\end{align}\]</span>
Inequality is saturated when, for every <span class="math inline">\(x\)</span>,
<span class="math inline">\(P_{Y|X=x}=Q_{Y|X=x}\)</span>.
</details>
<div class="definition">
<p><span id="def:unlabeled-div-51" class="definition"><strong>Definition 8.2  (conditional f-divergence) </strong></span>Given <span class="math inline">\(P_{Y|X}, Q_{Y|X}\)</span> and <span class="math inline">\(P_X\)</span>, the conditional
<span class="math inline">\(f\)</span>-divergence is
<span class="math display">\[
    D_f(P_{Y|X} \| Q_{Y|X} | P_X)
    = D_f(P_{Y|X} P_X \| Q_{Y|X} P_X)
    = \mathbb E_{x\sim P_X}\left[
        D_f(P_{Y|X=x}\| Q_{Y|X=x})
    \right]
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The second statement requires some justification:
<span class="math display">\[\begin{align}
    D_f(P_{Y|X} P_X \| Q_{Y|X} P_X)
    &amp;= \mathbb E_{x\sim P_X}
    \mathbb E_{y\sim Q_{Y|X=x}}\left[
        f\left(\dfrac{P_{Y|X=x}(y)}{Q_{Y|X=x}(y)}\right)
    \right]
\end{align}\]</span>
</details>
<div class="theorem">
<p><span id="thm:fDPI" class="theorem"><strong>Theorem 8.2  (data-processing inequality) </strong></span>Given a channel <span class="math inline">\(P_{Y|X}\)</span> with two inputs <span class="math inline">\(P_X, Q_X\)</span>
<span class="math display">\[
    D_f(P_{Y|X}
    \circ P_X \| P_{Y|X} \circ Q_X)
    \leq D_f(P_X \| Q_X)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
<span class="math inline">\(D_f(P_X \| Q_X) = D_f(P_{XY} \| Q_{XY}) \geq D_f(P_Y \| Q_Y)\)</span>,
with two equalities given by proposition <a href="f-divergence.html#prp:specialfMonotonicity">8.5</a>
and theorem <a href="f-divergence.html#thm:fMonotonicity">8.1</a>. Inequality
is saturated by the monotonicity condition <span class="math inline">\(P_{X|Y} = Q_{X|Y}\)</span>.
</details>
<div class="theorem">
<p><span id="thm:fInfoProperties" class="theorem"><strong>Theorem 8.3  (information properties of f-divergences) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span style="color:blue">Non-negativity</span>: <span class="math inline">\(D_f(P\|Q) \geq 0\)</span>. If <span class="math inline">\(f\)</span> is <em>strictly convex</em>
at 1, i.e. 
<span class="math display">\[
     \forall s, t\in [0, \infty), \alpha \in (0, 1) \text{ with }
     \alpha s + \bar \alpha t = 1: \quad \alpha f(s)+ \bar \alpha f(t)
     &gt; f(1) = 0
\]</span>
then <span class="math inline">\(D_f(P\|Q) = 0 \iff P=Q\)</span>.</li>
<li><span style="color:blue">Conditional <span class="math inline">\(f\)</span>-divergence; conditioning increases divergence</span>:
<span class="math inline">\(D_f(P_{Y|X} \circ P_X \| Q_{Y|X} \circ P_X) \leq D_f(P_{Y|X} \| Q_{Y|X} | Q_X)  = D_f(P_{Y|X} P_X \| Q_{Y|X} P_X)\)</span>.</li>
<li><span style="color:blue">Joint-convexity</span>: <span class="math inline">\((P, Q)\mapsto D_f(P\|Q)\)</span> is jointly convex;
consequently, <span class="math inline">\(P\mapsto D_f(P\|Q)\)</span> and <span class="math inline">\(Q\mapsto D_f(P\|Q)\)</span>
are also convex.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
<p>For non-negativity, apply monotonicity to
<span class="math display">\[
    D_f(P_X \| P_Y) = D_f(P_{X, 1} \| P_{Y, 1}) \geq D_f(1 \| 1) = 0
\]</span>
Assume <span class="math inline">\(P\neq Q\)</span> so there exists measurable <span class="math inline">\(A\)</span> such that
<span class="math inline">\(P[A]=p \neq Q[A] = Q\)</span>, then apply the <span class="math inline">\(\chi_A\)</span> channel and apply DPI;
both cases <span class="math inline">\(q=1\)</span> and <span class="math inline">\(q\neq 1\)</span> contradict strict convexity.
Claim (2) follows from monotonicity and recognize <span class="math inline">\(P_{Y|x}\circ P_X\)</span> as the
marginal of <span class="math inline">\(P_{Y|X}P_X\)</span>.
Joint convexity follows from standard latent variable argument:
to prove joint convexity
<span class="math display">\[
    D(\lambda P_0 + \bar \lambda P_1 \| Q_0 + \bar \lambda Q_1)
    \leq \lambda D(P_0 \| Q_0) + \bar \lambda D(P_1 \| Q_1)
\]</span>
Take <span class="math inline">\(\theta \sim \mathrm{Ber}_\lambda \to (P, Q)\)</span>, then the RHS is
<span class="math inline">\(D(P_{P|\lambda} \| P_{Q|\lambda} | P_\lambda)\)</span> while the LHS
is <span class="math inline">\(D(P_{P|\lambda}\circ P_\lambda\| P_{Q|\lambda} \circ P_\lambda)\)</span></p>
The following powerful theorem allows us to reduce any general
problem to finite alphabets.
</details>
<div class="theorem">
<p><span id="thm:fFiniteApprox" class="theorem"><strong>Theorem 8.4  (finite approximation theorem) </strong></span>Given two probability measures <span class="math inline">\(P, Q\)</span> on <span class="math inline">\(\mathcal X\)</span> with <span class="math inline">\(\sigma\)</span>-algebra
<span class="math inline">\(\mathcal F\)</span>. Given a finite <span class="math inline">\(\mathcal F\)</span>-measurable partition
<span class="math inline">\(\mathcal E = \{E_1, \cdots, E_n\}\)</span>, define the distribution <span class="math inline">\(P_{\mathcal E}\)</span>
on <span class="math inline">\([n]\)</span> by <span class="math inline">\(P_{\mathcal E}(j) = P[E_j]\)</span>, similarly for <span class="math inline">\(Q\)</span>, then
<span class="math display">\[
    D_f(P\|Q) = \sup_{\mathcal E} D_f(P_{\mathcal E} \| Q_{\mathcal E})
\]</span>
where <span class="math inline">\(\sup\)</span> is over all finite <span class="math inline">\(\mathcal F\)</span>-measurable partitions.</p>
</div>
<p>We omit the technical proof above.</p>
<div class="definition">
<p><span id="def:unlabeled-div-52" class="definition"><strong>Definition 8.3  (f-information) </strong></span>The <span class="math inline">\(f\)</span>-information is defined by
<span class="math display">\[
    I_f(X; Y) = D_f(P_{XY} | P_XP_Y)
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-53" class="definition"><strong>Definition 8.4  (f-DPI) </strong></span>For <span class="math inline">\(U\to X\to Y\)</span>, we have <span class="math inline">\(I_f(U; Y) \leq I_f(U; X)\)</span>.</p>
</div>
<p>Proof: <span class="math inline">\(I_f(U; X) = D_f(P_{UX} \|P_UP_X) \geq D_f(P_{UY} \| P_UP_Y) = I_f(U; Y)\)</span>.</p>
<div class="proposition">
<p><span id="prp:chiSqChain" class="proposition"><strong>Proposition 8.6  (χ² chain rule) </strong></span><span class="math inline">\(\chi^2(P_{AB} \| Q_{AB}) = \chi^2(P_B \| Q_B) + \mathbb E_{Q_B} \left[  \chi^2(P_{A|b} \| Q_{A|b}) \dfrac{P_B^2}{Q_B^2} \right]\)</span></p>
</div>
<details>
<summary>
Proof
</summary>
Invoke equation <a href="f-divergence.html#eq:chiSq">(8.1)</a>:
<span class="math display">\[\begin{align}
    \chi^2 (P_{AB} \| Q_{AB})
    &amp;= \mathbb E_P \left[ \dfrac{P_{AB}^2}{Q_{AB}^2}
    \right] - 1
    = \mathbb E\dfrac{P_B^2}{Q_B^2} - 1 + \mathbb E\left[
        \dfrac{P_B^2}{Q_B^2} \left(
            \dfrac{P_{A|B}^2}{Q_{A|B}^2} - 1
        \right)
    \right] \\
    &amp;= \chi^2(P_B \| Q_B) + \mathbb E_{P_B} \left[\chi^2(P_{A|B} \| Q_{A|B})
    \left(\dfrac{P_B^2}{Q_B^2}\right)^2\right]
\end{align}\]</span>
</details>
</div>
<div id="tv-and-hellinger-hypothesis-testing" class="section level2 unnumbered hasAnchor">
<h2>TV and Hellinger, hypothesis testing<a href="f-divergence.html#tv-and-hellinger-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a <em>binary hypothesis testing</em> problem, one is
given an observation <span class="math inline">\(X\)</span>, which is known to be <span class="math inline">\(X\sim P\)</span>
or <span class="math inline">\(X\sim Q\)</span>. The goal is to decide <span class="math inline">\(\lambda\in \{0, 1\}\)</span>
based on <span class="math inline">\(X\)</span>. In other words,
<span class="math display">\[
    \lambda \to X\to \hat \lambda
\]</span>
Our objective is to find a possibly randomized
decision function <span class="math inline">\(\phi:\mathcal X\to \{0, 1\}\)</span> such that
<span class="math display">\[
    P[\phi(X)=1] + Q[\phi(X) = 0]
\]</span>
is minimized. We will see that optimization leads to TV, while
asymptotic tensorization leads to <span class="math inline">\(H^2\)</span>.</p>
<div class="theorem">
<p><span id="thm:tvVarChar" class="theorem"><strong>Theorem 8.5  (variational characterizations of TV) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sup\)</span>-representation: let
<span class="math inline">\(\mathcal F = \{f:\mathcal X\to \mathbb R, \|f\|_\infty \leq 1\}\)</span>, then
<span class="math display">\[
\mathrm{TV}(P, Q) = \sup_E P(E) - Q(E) = \dfrac 1 2 \sup_{f\in \mathcal F}
\left[\mathbb E_P f(X) - \mathbb E_Q f(X)\right]
\]</span>
Supremum is achieved by <span class="math inline">\(f=\chi_E\)</span>, where <span class="math inline">\(E=\{x:p(x)&gt;q(x)\}\)</span>.</li>
<li><span class="math inline">\(\inf\)</span>-representation: Provided the diagonal is measurable,
<span class="math display">\[
\mathrm{TV}(P, Q) = \min_{P_{XY}} \{P_{XY}[X\neq Y]
\text{ subject to } P_X=P, P_Y=Q\}
\]</span></li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
<p>The upper bound by <span class="math inline">\(\mathrm{TV}\)</span> is intuitive;
to demonstrate saturation, let <span class="math inline">\(E = \{x:p(x)&gt;q(x)\}\)</span>, then
<span class="math display">\[\begin{align}
    0 = \int [p(x) - q(x)]\, d\mu
    &amp;= \int_E + \int_{E^c} [p(x) - q(x)]\, d\mu  \\
    \int_E [q(x) - p(x)]\, d\mu
    &amp;= \int_{E^c} [p(x) - q(x)]\, d\mu
\end{align}\]</span>
The sum of these two integrals (note the definition of <span class="math inline">\(E\)</span>)
equals <span class="math inline">\(2\mathrm{TV}\)</span>, then
<span class="math display">\[
    \mathrm{TV}(P, Q) = \dfrac 1 2 \int \chi_E[(q(x) - p(x)]\, d\mu
    = \dfrac 1 2 \mathbb E_P \chi_E(X) - \mathbb E_Q \chi_E(X)
\]</span>
For the <span class="math inline">\(\inf\)</span> representation, given any coupling <span class="math inline">\(P_{XY}\)</span>,
for <span class="math inline">\(f\in \mathcal F\)</span> we have
<span class="math display">\[
    \mathbb E_P f(X) - \mathbb E_Q f(X)
    = \mathbb E_{P_{XY}}[f(X) - f(Y)]
    \leq 2 P_{XY}[X\neq Y]
\]</span>
This shows that the <span class="math inline">\(\inf\)</span>-representation is always an
upper bound;
<span style="color:green">
we obtain saturation when <span class="math inline">\(X\neq Y\)</span> only happens for
possible values of <span class="math inline">\(X\)</span> disjoint from possible values of <span class="math inline">\(Y\)</span>,
and <span class="math inline">\(f\)</span> is the indicator function
on the disjoint support.
</span>
This is satisfied by the following
construction given <span class="math inline">\(P, Q\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\pi = \int \pi(x)\, d\mu\)</span> denote the overlap scalar, where
<span class="math inline">\(\pi(x) = \min(p(x), q(x))\)</span>.</li>
<li>With probability <span class="math inline">\(\pi\)</span> take <span class="math inline">\(X=Y\)</span> sampled from the overlap density
<span class="math display">\[
r(x) = \dfrac 1 \pi \pi(x)
\]</span></li>
<li>With probability <span class="math inline">\(1-\pi\)</span> sample <span class="math inline">\(X, Y\)</span> independently from
<span class="math display">\[
p_1(x) = \dfrac{p(x) - \pi(x)} {1 - \pi}, \quad
q_1(x)=\dfrac{q(x) - \pi(x)}{1 - \pi}
\]</span>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="images/tvInf.jpeg" alt="Visual representation of joint construction." width="80%" />
<p class="caption">
Figure 8.1: Visual representation of joint construction.
</p>
</div></li>
</ol>
Note that <span class="math inline">\(p_1, q_1\)</span> have disjoint supports.
Now, the marginals are indeed <span class="math inline">\(P, Q\)</span>, and this saturates
the inequality since <span class="math inline">\(P_{XY}[X\neq Y] = 1-\pi=\mathrm{TV}(P, Q)\)</span>
</details>
<p>The total variation distance does not tensorize well,
but we have the following sandwich bound:
<span class="math display" id="eq:TVHellingerBound">\[
    \dfrac 1 2 H^2 \leq \mathrm{TV}\leq
    H\sqrt{1 - \dfrac{H^2}{4}} \leq 1
    \tag{8.3}
\]</span><br />
We also have the following asymptotic theorem:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-54" class="theorem"><strong>Theorem 8.6  (asymptotic equivalence of TV and Hellinger) </strong></span>For any sequence of distributions <span class="math inline">\(P_n, Q_n\)</span>
<span class="math display">\[\begin{align}
\mathrm{TV}(P_n^{\otimes n}, Q_n^{\otimes n}) \to 0
&amp;\iff H^2(P_n, Q_n) = o(1/n) \\
\mathrm{TV}(P_n^{\otimes n}, Q_n^{\otimes n}) \to 1
&amp;\iff H^2(P_n, Q_n) = \omega(1/n)
\end{align}\]</span>
Recall that <span class="math inline">\(o(1/n)\)</span> means asymptotically smaller growth
than <span class="math inline">\(1/n\)</span>, while <span class="math inline">\(\omega(1/n)\)</span> is the opposite.</p>
</div>
<details>
<summary>
Proof
</summary>
We will need the following tensorization
result from corollary <a href="f-divergence.html#cor:renyiTensorCor">8.2</a>:
<span class="math display">\[
    H^2_n = 2 - 2\left(1 - \dfrac 1 2 H^2_1\right)^n
    \approx 2 - 2\exp\left[-n\left(1 - \dfrac 1 2 H^2_1\right)\right]
\]</span>
Using the sandwich bound <a href="f-divergence.html#eq:TVHellingerBound">(8.3)</a>,
<span class="math inline">\(\mathrm{TV}\to 0\)</span> implies the exponential going to <span class="math inline">\(0\)</span>,
then <span class="math inline">\(H^2_1\)</span> has to grow slower than <span class="math inline">\(1/n\)</span>.
The oppose is true for <span class="math inline">\(\mathrm{TV}\to 1\)</span>.
</details>
</div>
<div id="joint-range" class="section level2 unnumbered hasAnchor">
<h2>Joint range<a href="f-divergence.html#joint-range" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first provide a special case of an inequality.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-55" class="theorem"><strong>Theorem 8.7  (Pinsker's inequality) </strong></span>For any two distributions,
<span class="math inline">\(D(P\|Q) \geq (2\log e) \mathrm{TV}(P, Q)^2\)</span></p>
</div>
<details>
<summary>
Proof
</summary>
By DPI, it suffices to consider
Bernoulli distributions via the channel <span class="math inline">\(1_E\)</span>
which results in Bernoulli with parameter
<span class="math inline">\(P(E)\)</span> or <span class="math inline">\(Q(E)\)</span>. Working in natural units,
Pinsker’s inequality
for Bernoulli distributions yield
<span class="math display">\[
    \sqrt{\dfrac 1 2 D(P\|Q)}
    \geq \mathrm{TV}(1_E\circ P, Q_E\circ Q) =
    |P(E) - Q(E)|
\]</span>
Taking supremum over all <span class="math inline">\(E\)</span> yields
the desired inequality per the TV
variational characterization theorem <a href="f-divergence.html#thm:tvVarChar">8.5</a>.
</details>
<div class="definition">
<p><span id="def:unlabeled-div-56" class="definition"><strong>Definition 8.5  (joint range) </strong></span>Given two <span class="math inline">\(f\)</span>-divergences <span class="math inline">\(D_f\)</span> and <span class="math inline">\(D_g\)</span>, their
joint range <span class="math inline">\(\mathcal R\subset [0, \infty]^2\)</span>
is defined by
<span class="math display">\[
    \mathcal R = \mathrm{Image}\left[
        (P, Q)\mapsto (D_f(P\|Q, D_g(P\|Q))
    \right]
\]</span>
The joint range over <span class="math inline">\(k\)</span>-ary distribution is denoted <span class="math inline">\(\mathcal R_k\)</span>.</p>
</div>
<p>Our next result will characterize the set of
<span class="math inline">\(f\)</span>-divergences.</p>
<div class="lemma">
<p><span id="lem:convexBoundaryThm" class="lemma"><strong>Lemma 8.1  (Fenchel-Eggleston-Carathéodory theorem) </strong></span>Let <span class="math inline">\(S\subset \mathbb R^d\)</span> and <span class="math inline">\(x\in \mathrm{co}(S)\)</span>. There exists
a set of <span class="math inline">\(d+1\)</span> points <span class="math inline">\(S&#39;=\{x_1, \cdots x_{d+1}\}\in S\)</span>
such that <span class="math inline">\(x\in \mathrm{co}(S&#39;)\)</span>. If <span class="math inline">\(S\)</span> has at most
<span class="math inline">\(d\)</span> connected components, then <span class="math inline">\(d\)</span> points are enough.</p>
</div>
<p>As a corollary of the following theorem, it
suffices to prove joint range for Bernouli, then
convexify the range.</p>
<div class="theorem">
<p><span id="thm:harremoesVajda" class="theorem"><strong>Theorem 8.8  (Harremoës-Vajda) </strong></span><span class="math inline">\(\mathcal R = \mathrm{co}(\mathcal R_2) = \mathcal R_4\)</span>
where <span class="math inline">\(\mathrm{co}\)</span> denotes the convex hull with
a natural extension of convex operations to
<span class="math inline">\([0, \infty]^2\)</span>. In particular,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathrm{co}(\mathcal R_2) \subset\mathcal R_4\)</span>:
standard latent variable argument.</li>
<li><span class="math inline">\(\mathcal R_k \subset \mathrm{co}(\mathcal R_2) = \mathcal R_4\)</span>.</li>
<li><span class="math inline">\(\mathcal R = \mathcal R_4\)</span>: the approximation
theorem already implies <span class="math inline">\(\mathcal R = \overline{\bigcup_k \mathcal R_k}\)</span>;
the closure is technical.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
<p>First consider claim <span class="math inline">\(1\)</span>.
Construct a convex divergence as follows:
for two pairs of distributions <span class="math inline">\((P_0, Q_0)\)</span> and
<span class="math inline">\((P_1, Q_1)\)</span> on <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\lambda \in [0, 1]\)</span>.
Define the typical Bernoulli latent joint <span class="math inline">\((X, B)\)</span>
by <span class="math inline">\(P_B = Q_B = \mathrm{Ber}(\alpha)\)</span> and <span class="math inline">\((P, Q)_{X|B=j} = (P_j, Q_j)\)</span>.
Applying the conditional divergence to obtain
<span class="math display">\[
    D_f(P_{XB} \| Q_{XB})
    = \bar \alpha D_f(P_0\|Q_0) + \alpha D_f(P_1 \| Q_1)
    \implies \mathrm{co}(\mathcal R_2)\subset \mathcal R_4
\]</span>
Onto the most nontrivial claim <span class="math inline">\(2\)</span>: fixing <span class="math inline">\(k\)</span> and
distributions <span class="math inline">\(P, Q\)</span> on <span class="math inline">\([k]\)</span> with distributions <span class="math inline">\((p_j), (q_j)\)</span>,
w.l.o.g. make <span class="math inline">\(q_{j&gt;1}&gt;0\)</span> and concentrate <span class="math inline">\(q_1=0\)</span> (i.e. concentrate
all empty points onto <span class="math inline">\(j=1\)</span>.
<span style="color:blue">
Consider the equivalence class <span class="math inline">\(\mathcal S\)</span> of all
<span class="math inline">\((\tilde p_j, \tilde q_j)\)</span> <u>which have the same likelihood ratio
on the support</u> of <span class="math inline">\(q\)</span>:
</span>
Let <span class="math inline">\(\phi_{j&gt;1} = p_j / q_j\)</span> and consider
<span class="math display">\[
    \mathcal S = \left\{
        \tilde Q = (\tilde q_j)_{j\in [k]}:
        \tilde q_j\geq 0, \sum \tilde q_j=1, \tilde q_1=0,
    \right\}
\]</span>
Note that <span class="math inline">\(\mathcal S\)</span> it the intersection of</p>
<ol style="list-style-type: decimal">
<li>The simplex of all distributions <span class="math inline">\(\tilde q\)</span>.</li>
<li>The half-space specified by <span class="math inline">\(\tilde q\cdot \phi \leq 1\)</span>.</li>
</ol>
<p>We can next identify the boundary of <span class="math inline">\(\mathcal S_e\subset \mathcal S\)</span>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\tilde q_{j\geq 2}=1\)</span> and <span class="math inline">\(\phi_j\leq 1\)</span>.</li>
<li><span class="math inline">\(\tilde q_{j_1}+\tilde q_{j_2}=1\)</span> and
<span class="math inline">\(\tilde q_{j_1}\phi_{j_1} + \tilde q_{j_2}\phi_{j_2}=1\)</span>.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="images/joint_range.jpeg" alt="Gray line corresponds to $S$; blue and green dots correspond to elements of $S_e$ identified by (1) and (2), respectively. " width="50%" />
<p class="caption">
Figure 8.2: Gray line corresponds to <span class="math inline">\(S\)</span>; blue and green dots correspond to elements of <span class="math inline">\(S_e\)</span> identified by (1) and (2), respectively.
</p>
</div>
<p>We next have <span class="math inline">\(\mathcal S = \mathrm{co}(\mathcal S_e)\)</span>; so for <span class="math inline">\(Q\)</span>,
there exists extreme points <span class="math inline">\(\tilde Q_j\in \mathcal S_e\)</span>
(note that <span class="math inline">\(\mathcal S_e\)</span> is dependent upon <span class="math inline">\(Q\)</span>!) with support on
at most <span class="math inline">\(2\)</span> atoms (binary distributions) such that
<span class="math inline">\(Q = \alpha_j \tilde Q_j\)</span>.
The map asspciating <span class="math inline">\(\tilde P\)</span> given <span class="math inline">\(\tilde Q\)</span> is
<span class="math display">\[
    \tilde p_j = \begin{cases}
        \phi_j \tilde q_j &amp; j\in \{2, \cdots, k\}, \\
        1 - \sum_{j=2}^k \phi_j \tilde q_j &amp; j = 1
    \end{cases}
\]</span>
On this particular set which fixes the likelihood ratio,
the divergence is an affine map:
<span class="math display">\[
    \tilde Q \mapsto D_f(\tilde P \| \tilde Q)
    = \sum_{j\geq 2}\tilde q_j f(\phi_j) + f&#39;(\infty)\tilde p_1 \implies
    D_f(P\|Q) = \sum_{j=1}^m \alpha_i D_f(\tilde P_i \|\tilde Q_i)
\]</span></p>
</details>
<p>We defer detailed examples of joint ranges to the book (7.6).</p>
</div>
<div id="rényi-divergence" class="section level2 unnumbered hasAnchor">
<h2>Rényi divergence<a href="f-divergence.html#rényi-divergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Rényi divergences are a monotone transformation
of <span class="math inline">\(f\)</span>-divergences; they satisfy DPI and other properties.</p>
<div class="definition">
<p><span id="def:unlabeled-div-57" class="definition"><strong>Definition 8.6  (Rényi divergence) </strong></span>For <span class="math inline">\(\lambda\in \mathbb R- \{0, 1\}\)</span>, the Rényi divergence
of order <span class="math inline">\(\lambda\)</span> between distributions <span class="math inline">\(P, Q\)</span> is
<span class="math display">\[
    D_\lambda(P \| Q) = \dfrac 1 {\lambda - 1} \log \mathbb E_Q \left[
        \left(\dfrac{dP}{dQ}\right)^\lambda
    \right]
\]</span></p>
</div>
<p>To see its connection with entropy, note that
<span class="math display">\[
    \mathbb E_Q \left(\dfrac{dP}{dQ}\right)^\lambda
    = \mathrm{sgn}(\lambda - 1) D_f(P\|Q) + 1, \quad f = \mathrm{sgn}(\lambda - 1)(x^\lambda - 1)
\]</span>
with which the Rényi entropy becomes (the <span class="math inline">\(\mathrm{sgn}(\lambda - 1)\)</span> is just there
to keep <span class="math inline">\(f\)</span> convex):
<span class="math display" id="eq:renyiMonotone">\[
    D_\lambda(P\|Q) = \dfrac 1 {\lambda - 1} \log \left[1 +
        \mathrm{sgn}(\lambda - 1) D_f(P\|Q)
    \right], \quad f = \mathrm{sgn}(\lambda - 1)(x^\lambda - 1)
    \tag{8.4}
\]</span></p>
<div class="proposition">
<p><span id="prp:unlabeled-div-58" class="proposition"><strong>Proposition 8.7  </strong></span>Under regularity conditions,
<span class="math display">\[
    \lim_{\lambda \to 1} D_\lambda (P \|Q) = D(P \| Q)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
Expand <span class="math inline">\((d_QP)^\lambda = \exp(\lambda \ln d_QP)\)</span> about <span class="math inline">\(\lambda=1\)</span>:
<span class="math display">\[
    (d_QP)^{\lambda} \approx d_QP + \ln d_QP \cdot d_QP \cdot (\lambda - 1)
\]</span>
Taking <span class="math inline">\(\mathbb E_Q\)</span> yields <span class="math inline">\(1 + (\lambda - 1)\mathbb E_P[\ln d_QP]\)</span>; then
substituting into <span class="math inline">\(\log x \approx 1+x\)</span> yields
<span class="math display">\[
    D_{\lambda_\to 1}(P \| Q) = \dfrac 1 {\lambda - 1} \log \left[
        1 + (\lambda - 1)\mathbb E_P[\ln d_QP]
    \right]
    = \mathbb E_P \ln d_QP = D(P \| Q)
\]</span>
</details>
<div class="proposition">
<p><span id="prp:specialRenyi" class="proposition"><strong>Proposition 8.8  (special cases of Rényi divergence) </strong></span>Consider <span class="math inline">\(\lambda = 1/2, 2\)</span>:
<span class="math display">\[
    D_2 = \log(1 + \chi^2), \quad D_{1/2} = -2\log\left(1 - \dfrac{H^2}{2}\right)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The <span class="math inline">\(D_2\)</span> case is apparant in light of equation <a href="f-divergence.html#eq:renyiMonotone">(8.4)</a>.
Substitute <span class="math inline">\(\lambda = 1/2\)</span>:
<span class="math display">\[
    D_{1/2} = \dfrac 1 {1/2 - 1} \log[1 - D_{x\mapsto 1 - \sqrt x}(P\|Q)]
\]</span>
It remains to show that <span class="math inline">\(D_{1 - \sqrt x} (P\|Q) = \dfrac{H^2}{2}\)</span>, applying
equation <a href="f-divergence.html#eq:hellingerDiv">(8.2)</a>
<span class="math display">\[
    \mathbb E_Q\left[1 - \sqrt{d_QP}\right]
    = \dfrac 1 2 \mathbb E_Q \left(1 - \sqrt{d_QP}\right)^2 = 1 - \int \sqrt{dPdQ}
\]</span>
</details>
<p>Several other properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\lambda \mapsto D_\lambda D(P\|Q)\)</span> is non-decreasing and
<span class="math inline">\(\lambda \mapsto (1 - \lambda) D_\lambda(P\|Q)\)</span> is concave.</li>
<li>For <span class="math inline">\(\lambda \in [0, 1]\)</span> the divergence <span class="math inline">\(D_\lambda\)</span> is jointly convex.</li>
<li>The Rényi entropy for finite alphabet is <span class="math inline">\(H_\lambda(P) = \log m - D_\lambda(P \| U)\)</span>.</li>
</ol>
<div class="definition">
<p><span id="def:unlabeled-div-59" class="definition"><strong>Definition 8.7  (conditional Rényi entropy) </strong></span>Given <span class="math inline">\(P_{X|Y}, Q_{X|Y}\)</span> and <span class="math inline">\(P_Y\)</span>
<span class="math display">\[\begin{align}
    D_\lambda(P_{X|Y} \| Q_{X|Y} | P_Y)
    &amp;= D_\lambda(P_{X|Y} P_Y \| Q_{X|Y} P_Y)
    = \dfrac 1 {\lambda - 1} \log \mathbb E_{Q_{X|Y}P_Y}
        \left[
            \dfrac{(P_{X|Y}P_Y)(X, Y)}{(Q_{X|Y}P_Y)(X, Y)}
        \right]^\lambda \\
    &amp;= \dfrac 1 {\lambda - 1} \log \mathbb E_{y\sim P_Y}
    \int_{\mathcal X} P_{X|Y=y}(x)^\lambda Q_{X|Y=y}(x)^{1-\lambda}
\end{align}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-60" class="proposition"><strong>Proposition 8.9  (Rényi chain rule) </strong></span>Given <span class="math inline">\(P_{AB}, Q_{AB}\)</span>, define the <span class="math inline">\(\lambda\)</span>-tilting
of <span class="math inline">\(P_B\)</span> towards <span class="math inline">\(Q_B\)</span> by
<span class="math display">\[
    P_B^{(\lambda)}(b) = P_B^\lambda(b) Q_B^{1-\lambda}(b)
    \exp \left[
        -(\lambda - 1) D_\lambda(P_B \| Q_B)
    \right]
\]</span>
joint Rényi divergence decomposes as
<span class="math display">\[
    D_\lambda(P_{AB} \| Q_{AB}) = D_\lambda(P_B \| Q_B) +
    D_\lambda( P_{A|B} \| Q_{A|B} | P_B^{(\lambda)})
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
First need to prove that <span class="math inline">\(P_B^{(\lambda)}\)</span> is indeed
correctly normalized:
<span class="math display">\[\begin{align}
    \exp \left[
        -(\lambda - 1) D_\lambda(P_B \| Q_B)
    \right]
    &amp;= \mathbb E_Q \left[\left(\dfrac{dP}{dQ}\right)^\lambda\right]
    = \int P_B^\lambda(b) Q_B^{1-\lambda}(b)\, db
\end{align}\]</span>
Next up, computing the RHS explicitly, we have
<span class="math display">\[\begin{align}
    (\lambda - 1) [D_\lambda(P_B \| Q_B) +
    D_\lambda( P_{A|B} \| Q_{A|B} | P_B^{(\lambda)})]
    &amp;= \log \int_{\mathcal B} \left[
        P_B(b)^\lambda Q_B(b)^{1-\lambda}
        \cdot \int_{\mathcal A} P_{A|B=b}(a)^\lambda Q_{A|B=b}(a)^{1-\lambda}
    \right] \\
    &amp;= \log \mathbb E_{Q_{AB}} \left(\dfrac{dP_{AB}}{dQ_{AB}}\right)^\lambda
    = D_\lambda(P_{AB} \| Q_{AB})
\end{align}\]</span>
</details>
<div class="corollary">
<p><span id="cor:unlabeled-div-61" class="corollary"><strong>Corollary 8.1  (Rényi tensorization) </strong></span>Specializing the chain rule to independent joints,
<span class="math display">\[
    D_\lambda \left(\prod P_{X_j} \| \prod Q_{X_j}\right)
    = \sum_j D_\lambda(P_{X_j} \| Q_{X_j})
\]</span></p>
</div>
<div class="corollary">
<p><span id="cor:renyiTensorCor" class="corollary"><strong>Corollary 8.2  (tensorization of χ² and Hellinger) </strong></span>Applying tensorization and proposition <a href="f-divergence.html#prp:specialRenyi">8.8</a>:
<span class="math display">\[\begin{align}
    1 + \chi^2 \left(\prod_j P_j \| \prod_j Q_j\right)
    &amp;= \prod_j 1 + \chi^2(P_j \| Q_j)  \\
    1 - \dfrac 1 2 H^2 \left(\prod P_j, \prod Q_j\right)
    &amp;= \prod_j 1 - \dfrac 1 2 H^2(P_j, Q_j)
\end{align}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-62" class="proposition"><strong>Proposition 8.10  (variational characterization via KL) </strong></span>Show that <span class="math inline">\(\forall \alpha \in \mathbb R\)</span>:
<span class="math display">\[
    \bar \alpha D_\alpha(P \|Q)
    = \inf_R \left[
        \alpha D(R\|P) + \bar \alpha D(R\|Q)
    \right]
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The KL case <span class="math inline">\(\alpha=1\)</span> holds trivially with <span class="math inline">\(R=P\)</span>.
otherwise expand the LHS to
<span class="math display">\[\begin{align}
    \bar \alpha D_\alpha(P\|Q)
    &amp;= -\log \mathbb E_Q \left(\dfrac{dP}{DQ}\right)^\alpha
    = \log \mathbb E_Q \left(
        \dfrac{dQ}{dP}
    \right)^\alpha  
\end{align}\]</span>
Expand the RHS to
<span class="math display">\[\begin{align}
    \alpha D(R\|P) + \bar \alpha D(R\|Q)
    &amp;= \mathbb E_R \log \left[
        \left(\dfrac{dR}{dP}\right)^\alpha
        \left(\dfrac{dR}{dQ}\right)^{\bar \alpha}
    \right]
    = \mathbb E_R \log
        \dfrac{dR}{dP^\alpha dQ^{\bar \alpha}} \\
    &amp;= \mathbb E_R \log \dfrac{dR}{dQ} \cdot \left(\dfrac{dQ}{dP}\right)^\alpha
    = D(R \| Q) + \mathbb E_R \log \left(\dfrac{dQ}{dP}\right)^\alpha
\end{align}\]</span>
We wish to establish the bound
<span class="math display">\[\begin{align}
    \log \mathbb E_Q \left(
        \dfrac{dQ}{dP}
    \right)^\alpha
    \leq \mathbb E_R \log \left[
        \dfrac{dR}{dP^\alpha dQ^{\bar \alpha}}
    \right]
    &amp;= D(R \| Q) + \mathbb E_R \log \left(\dfrac{dQ}{dP}\right)^\alpha  \\
    D(R\|Q) &amp;\geq \mathbb E_R \log \left(\dfrac{dP}{dQ}\right)^\alpha
    - \log \mathbb E_Q \left(\dfrac{dP}{dQ}\right)^\alpha
\end{align}\]</span>
Comparison between <span class="math inline">\(\log \mathbb E\)</span> and <span class="math inline">\(\mathbb E\log\)</span> screams Donsker-Varadhan
<a href="variational-characterizations.html#thm:donskerVaradhan">5.5</a>:
<span class="math display">\[
    D(R \|Q) \geq  \mathbb E_R f - \log \mathbb E_Q \exp f
\]</span>
for <span class="math inline">\(f = \alpha \log(dP/dQ)\)</span> being the likelihood ratio.
Recall that the saturation constraint is
<span class="math display">\[
    \log \left(\dfrac{dP}{dQ}\right)^\alpha = f = \log \dfrac{dR}{dQ} + C \iff
    \dfrac{P(x)^\alpha}{Q(x)^\alpha} = \dfrac{R(x)}{Q(x)} \iff
    dR \propto dP^\alpha dQ^{\bar \alpha}
\]</span>
the proportionality freedom comes from invariance of <span class="math inline">\(f\mapsto f+C\)</span>
in Donsker-Varadhan and is determined by the normalization constraint.
The final result is simply the Renyi-tilt of <span class="math inline">\(P\)</span> towards <span class="math inline">\(Q\)</span> given by
<span class="math display">\[
    R(x) = P(x)^\alpha Q(x)^{\bar \alpha} \exp \left[
        -\bar \alpha D_\alpha(P \| Q)
    \right]
\]</span>
</details>
</div>
<div id="variational-characterizations-1" class="section level2 unnumbered hasAnchor">
<h2>Variational characterizations<a href="f-divergence.html#variational-characterizations-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a convex function <span class="math inline">\(f:(0, \infty)\to \mathbb R\)</span>,
recall that is convex conjugate <span class="math inline">\(f^*:\mathbb R\to \mathbb R\cup \{+\infty\}\)</span>
is defined by
<span class="math display">\[
    f^*(y) = \sup_{x\in \mathbb R_+} xy - f(x)
\]</span>
For a differentiable convex function,
<span class="math inline">\(\mathrm{dom}(f^*) = \{y: f^*(y)&lt;\infty\}\)</span> is the range of <span class="math inline">\(\nabla f\)</span>.
The Legendre transform is convex, involutary, and satisfies
<span class="math display">\[
    f(x) + f^*(y) \geq xy
\]</span>
Similarly, the convex conjugate of any convex
functional <span class="math inline">\(\Psi(P)\)</span> is, under suitable technical conditions
<span class="math display">\[
    \Psi^*(g) = \sup_P \int g\, dP - \Psi(P), \quad
    \Psi(P) = \sup_g \int g\, dP - \Psi^*(g)
\]</span>
When <span class="math inline">\(P\)</span> is a probability measure, <span class="math inline">\(\int g\, dP = \mathbb E_P[g]\)</span>.
In this section, we identify <span class="math inline">\(f:(0, \infty)\to \mathbb R\)</span>
with its convex extension <span class="math inline">\(f:\mathbb R\to \mathbb R\cup \{+\infty\}\)</span>
(one can just take <span class="math inline">\(f = \infty\)</span> for all inputs outside
the original domain); different choices of <span class="math inline">\(f\)</span>’s extension
yield different variational formulas:</p>
<div class="theorem">
<p><span id="thm:fVar" class="theorem"><strong>Theorem 8.9  (supremum characterization of f-divergences) </strong></span>Given probability measures <span class="math inline">\(P, Q\)</span> on <span class="math inline">\(\mathcal X\)</span>; fix
an extension of <span class="math inline">\(f\)</span> and corresponding Legendre transform <span class="math inline">\(f^*\)</span>;
denote <span class="math inline">\(\mathrm{dom}(f^*) = (f^*)^{-1}(\mathbb R)\)</span>, then
<span class="math display">\[
    D_f(P\|Q) = \sup_{g:\mathcal X\to \mathrm{dom}(f^*)}
    \mathbb E_P[g(X)] - \mathbb E_Q[(f^*\circ g)(X)]
\]</span></p>
</div>
<p>As a result of this variational characterization, we obtain:</p>
<ol style="list-style-type: decimal">
<li><u>Joint convexity</u>: <span class="math inline">\((P, Q)\mapsto D_f(P\|Q)\)</span> is convex since <span class="math inline">\(D_f\)</span>
is a supremum of affine functions.</li>
<li><u>Weak lower semicontinuity</u>: a simple counter-example to continuity
is <span class="math inline">\(\sum_{j=1}^n X_j/\sqrt n\to \mathcal N(0, 1)\)</span> for
<span class="math inline">\(X_j=2\mathrm{Ber}(1/2)-1\)</span> but each partial sum remains discrete.
By the same line of reasining in theorem <a href="variational-characterizations.html#thm:semiContinuity">5.6</a>:<br />
<span class="math display">\[
     \liminf_{n\to \infty} D_f(P_n\|Q_n) \geq D_f(P\|Q)
\]</span></li>
<li><u>Relation to DPI</u>: again, variational characterizations can be
used as a version of DPI when using <span class="math inline">\(1_E\)</span>
since it allows one to bound the event variation using <span class="math inline">\(f\)</span>-divergence.</li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-63" class="example"><strong>Example 8.1  (TV) </strong></span>Recall, for <span class="math inline">\(\mathrm{TV}, f(x)=|x-1|/2\)</span>; extending as such to <span class="math inline">\(\mathbb R\)</span> to obtain
the conjugate
<span class="math display">\[
    f^*(y) = \sup_x xy - \dfrac 1 2 |x-1|
    = \begin{cases}
        \infty &amp; |y|&gt;1/2 \\
        y &amp; |y| \leq 1/2
    \end{cases}
\]</span>
Applying theorem <a href="f-divergence.html#thm:fVar">8.9</a> recovers theorem <a href="f-divergence.html#thm:tvVarChar">8.5</a>:
<span class="math display">\[
    \mathrm{TV}(P, Q) = \sup_{g(x)\in [-1/2, 1/2]} \mathbb E_P[g]
    - \mathbb E_Q[g]
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-64" class="example"><strong>Example 8.2  (Hellinger) </strong></span>The conjugate for Hellinger <span class="math inline">\(f(x) = (1-\sqrt x)^2\)</span>
is <span class="math inline">\(f^*(y) = \frac 1 {1-y} - 1\)</span> with <span class="math inline">\(y\in (-\infty, 1)\)</span>, yielding
<span class="math display">\[\begin{align}
    H^2(P, Q)
    &amp;= \sup_{g(x)\in (-\infty, 1)} \mathbb E_P[g]
    - \mathbb E_Q\left[\dfrac 1 {1-g} - 1\right]\\
    &amp;= \sup_{g&gt; 0} \mathbb E_P[1-g] - \mathbb E_Q \dfrac 1 g + 1
    = 2 - \inf_{g&gt; 0} \mathbb E_P g + \mathbb E_Q \dfrac 1 g
\end{align}\]</span>
Given <span class="math inline">\(f:\mathcal X\to [0, 1]\)</span> and <span class="math inline">\(\tau \in (0, 1)\)</span>, we have
<span class="math inline">\(h=1-\tau f\)</span> and <span class="math inline">\(\frac 1 h \leq 1 + \frac{\tau}{1-\tau} f\)</span>, then
<span class="math display">\[
    \mathbb E_P[f] \leq \dfrac 1 {1-\tau} \mathbb E_Q f + \dfrac 1 \tau H^2(P, Q), \quad
    \forall f:\mathcal X\to [0, 1], \quad \tau \in (0, 1)
\]</span>
Note that for this inequality we don’t need <span class="math inline">\(h\)</span> to saturate the
<span class="math inline">\(\inf h&gt;0\)</span> condition: only a subset suffices.</p>
</div>
<div class="proposition">
<p><span id="prp:chiVariation" class="proposition"><strong>Proposition 8.11  (χ²-divergence) </strong></span>Given <span class="math inline">\(\chi^2\)</span> and <span class="math inline">\(f(x)=(x-1)^2\)</span>, the conjugate is
<span class="math inline">\(f^*(y) = y + \frac{y^2}{4}\)</span> and
<span class="math display">\[\begin{align}
    \chi^2(P\|Q)
    &amp;= \sup_{g:\mathcal X\to \mathbb R} \mathbb E_P g - \mathbb E_Q \left[
        g + \dfrac{g^2}{4} \right] \\
    &amp;= \sup_{g:\mathcal X\to \mathbb R} \sup_{\lambda \in \mathbb R} \left[
        \mathbb E_P g - \mathbb E_Q g
    \right] \lambda - \dfrac{\mathbb E_Q[g^2]}{4} \lambda^2
\end{align}\]</span>
Complete the square in the inner problem to obtain
<span class="math display">\[\begin{align}
    -a \lambda^2 + b\lambda
    &amp;= -a\left(\lambda - \dfrac b {2a}\right)^2
    + \dfrac{b^2}{4a} \implies
    \chi^2(P\|Q)
    = \sup_{g:\mathcal X\to \mathbb R}
    \dfrac{\left(\mathbb E_P g - \mathbb E_Q g\right)^2}{\mathbb E_Q[g^2]}
\end{align}\]</span>
The choice of <span class="math inline">\(g\)</span> is invariant under <span class="math inline">\(g\mapsto g+c\)</span>, so we obtain
<span class="math display" id="eq:chiStatVar">\[
    \chi^2(P\|Q) = \sup_{g:\mathcal X\to \mathbb R}
    \dfrac{\left(\mathbb E_P g - \mathbb E_Q g\right)^2}{\mathrm{Var}_Q[g^2]}
    \tag{8.5}
\]</span>
Given two hypotheses <span class="math inline">\(P, Q\)</span>, <span class="math inline">\(\chi^2\)</span> is the maximum ratio
between the squared-distance between the mean statistic <span class="math inline">\(g\)</span>
and its variance under <span class="math inline">\(Q\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:JSGAN" class="example"><strong>Example 8.3  (Jenson-Shannon divergence) </strong></span>For JS we have <span class="math inline">\(f(x) = x\log \dfrac{2x}{1+x} + \log \dfrac{2}{1+x}\)</span>
with conjugate and variational characterization
<span class="math display">\[\begin{align}
    f^*(s)
    &amp;= -\log(2 - e^s), \quad s\in (-\infty, \log 2) \\
    \mathrm{JS}(P, Q)
    &amp;= \sup_{g(x)&lt;\log 2} \mathbb E_P g + \mathbb E_Q \log(2 - e^g)
\end{align}\]</span>
Reparameterize with <span class="math inline">\(h=e^g/2\)</span> to obtain
<span class="math display">\[
    \mathrm{JS}(P, Q) = \log 2 + \sup_{h(x)\in (0, 1)} \mathbb E_P \log h
    + \mathbb E_Q\log(1-h)
\]</span>
In desity estimation, suppose <span class="math inline">\(P\)</span> is the data distribution
wish to approximate with <span class="math inline">\(P_{f_\theta(Z)}\)</span>, then we have
<span class="math display">\[
    \inf_\theta \sup_\phi
    \mathbb E_{X\sim P} [\log h_\phi(X)]
    + \mathbb E_{Z\sim \mathcal N} \log[1 - h_\phi(f_\theta(Z))]
\]</span>
Here <span class="math inline">\(f_\theta\)</span> is the generator, and <span class="math inline">\(h_\phi\)</span> is
the critic in GAN.</p>
</div>
<div class="proposition">
<p><span id="prp:klDV" class="proposition"><strong>Proposition 8.12  (KL) </strong></span>We have <span class="math inline">\(f(x\geq 0) = x\log x\)</span>. Extend to <span class="math inline">\(f(x&lt;0) = \infty\)</span>;
the convex conjugate is (in natural units) <span class="math inline">\(f^*(y) = e^y/e\)</span> thus
<span class="math display">\[
    D(P\|Q) = \sup_{g:\mathcal X\to R} \mathbb E_P[g] - \mathbb E_Q[e^g] + 1
\]</span>
Do the substitution <span class="math inline">\(g\mapsto g+c\)</span> to obtain the following expression,
and solve for the analytic optimal <span class="math inline">\(c\)</span>.
Fixing <span class="math inline">\(g\)</span>, let <span class="math inline">\(\alpha = \mathbb E_Q[e^g]\)</span> be a constant. The extremal value
of <span class="math inline">\(c - a\cdot e^c\)</span> is obtained at <span class="math inline">\(c=-\ln a\)</span> as <span class="math inline">\(-\ln a - 1\)</span>, yielding
<span class="math display">\[
    D(P\|Q) = \sup_{g:\mathcal X\to \mathbb R} \sup_{c\in \mathbb R}
    \mathbb E_P[g] + c - e^c \mathbb E_Q[e^g] - 1
    = \sup_{g:\mathcal X\to \mathbb R} \mathbb E_P[g] + \ln \mathbb E_Q[e^g]
\]</span>
This recovers the Donsker-Varadhan <a href="f-divergence.html#thm:fVar">8.9</a>.</p>
</div>
</div>
<div id="fisher-information-location-family" class="section level2 unnumbered hasAnchor">
<h2>Fisher information, location family<a href="f-divergence.html#fisher-information-location-family" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We consider a parameterized set of distributions
<span class="math inline">\(\{P_\theta: \theta\in \Theta\}\)</span> where <span class="math inline">\(\Theta\)</span> is an open
subset of <span class="math inline">\(\mathbb R^d\)</span>. Further suppose that <span class="math inline">\(P_\theta(dx)=p_\theta(x)\mu(dx)\)</span>
for some common dominating measure <span class="math inline">\(\mu\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-65" class="definition"><strong>Definition 8.8  (Fisher information, score) </strong></span>If for each fixed <span class="math inline">\(x\)</span>, the density <span class="math inline">\(p_\theta(x)\)</span> depends smoothly
on <span class="math inline">\(\theta\)</span>, we define the Fisher information matrix w.r.t. <span class="math inline">\(\theta\)</span> as
<span class="math display">\[
    \mathcal J_F(\theta) = \mathbb E_\theta(VV^T), \quad
    V = \nabla_\theta \ln p_\theta(X) = \dfrac 1 {p_\theta(X)} \nabla_\theta p_\theta(X)
\]</span>
The <em>random variable</em> <span class="math inline">\(V\)</span> is known as the score.</p>
</div>
<p>Under technical regularity conditions, we have:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb E_\theta[V]=0\)</span>: for an intuitive argument
<span class="math display">\[
\mathbb E_\theta[\nabla_\theta \ln p_\theta(X)]
= \int \nabla_\theta p_\theta(x) = \nabla_\theta \int p_\theta = 0
\]</span></li>
<li><span class="math inline">\(\mathcal J_F(\theta) = \mathrm{Cov}_\theta(V)  = -\mathbb E_\theta \left[\mathcal H_\theta \ln p_\theta(X)\right]\)</span>: differentiating (1)
<span class="math display" id="eq:fisherHessian">\[\begin{align}
\partial_{jk}^2 \ln p_\theta = \partial_{j} \left(
     \dfrac 1 {p_\theta} \partial_{k} p_\theta
\right) = -\dfrac{(\partial_{j} p_\theta)(\partial_{k} p_\theta)} {p_\theta^2}
+ \left[\dfrac 1 {p_\theta} \partial_{jk}^2 p_\theta\right]_{=0} = (VV^T)_{jk}
\tag{8.6}
\end{align}\]</span>
The second term vanishes when we take the expectation.</li>
<li><span class="math inline">\(D(P_{\theta} \| P_{\theta + \xi})  = \dfrac{\log e}{2} \xi^T \mathcal J_F(\theta)\xi + o(\|\xi\|^2)\)</span>:
this is obtained by integrating
<span class="math display">\[
\ln p_{\theta + \xi}(x) = \ln p_\theta(x) + \xi^T \nabla_\theta \ln p_\theta(x)
+ \dfrac 1 2 \xi^T [\mathcal H_\theta \ln p_\theta(x)]\xi + o(\|\xi\|^2)
\]</span></li>
<li>Under a smooth invertible map <span class="math inline">\(\theta \to \tilde \theta\)</span>, we have
<span class="math display">\[
\mathcal J_F(\tilde \theta) = J_{\tilde \theta \to \theta}^T
\mathcal J_F(\theta) J_{\tilde \theta\to \theta}
\]</span>
In fact, this allows one to define a Riemannian metric on the parameter space,
called the Fisher-Rao metric.</li>
<li>Tensorization: under i.i.d. observations, <span class="math inline">\(X^n\sim P_\theta\)</span> i.i.d, we have
<span class="math inline">\(\{P_\theta^{\otimes n}:\theta \in \Theta\}\)</span> whose Fisher information
<span class="math inline">\(\mathcal J_F^{\otimes n}(\theta) = n\mathcal J_F(\theta)\)</span>.</li>
</ol>
<p>We next consider the information properties of Fisher information
under regularity conditions. Consider a joint distribution <span class="math inline">\(P^{XY}_\theta\)</span>
parameterized by <span class="math inline">\(\theta\)</span> with marginals <span class="math inline">\(P^X_\theta, P^Y_\theta\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-66" class="definition"><strong>Definition 8.9  (conditional Fisher information) </strong></span>The conditional Fisher information <span class="math inline">\(\mathcal J^{Y|X}_\theta\)</span>
is the semidefinite matrix of size <span class="math inline">\(n\)</span> defined as
<span class="math display">\[
    \mathcal J^{Y|X}_F(\theta) = \mathbb E_X[\mathcal J^{Y|X=x}_F(\theta)]
    = \mathbb E_{XY}[V^{Y|X}_\theta(V^{Y|X}_\theta)^T], \quad
    V^{Y|X}_\theta(x, y) = \nabla_\theta \ln P^{Y|X}_\theta(x, y)
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:fisherChainRule" class="proposition"><strong>Proposition 8.13  (Fisher information chain rule) </strong></span><span class="math inline">\(\mathcal J^{XY}_F(\theta) = \mathcal J^X_F(\theta) + \mathcal J^Y_F(\theta)\)</span></p>
</div>
<details>
<summary>
Proof
</summary>
Note that <span class="math inline">\(V^{XY}_\theta, V^{Y|X}_\theta\)</span> are
functions of <span class="math inline">\(x, y\)</span>:
<span class="math display">\[\begin{align}
    V^{XY}_\theta(x, y)
    &amp;= \nabla_\theta \ln P^{XY}_\theta(x, y)
    = \nabla_\theta \ln P^{Y|X}_\theta(x, y) + \nabla_\theta \ln P^X_\theta(x, y) \\  
    \mathcal J^{XY}_F(\theta) = V^{Y|X}_\theta + V^X_\theta
    &amp;= \mathbb E_{XY} \left[
        (V^{Y|X}_\theta + V^X_\theta)(V^{Y|X}_\theta + V^X_\theta)^T
    \right] \\
    &amp;= \mathcal J^X_F + \mathcal J^{Y|X}_F + 2\mathbb E_{XY}\left[
        (V^X_\theta)^T V^{Y|X}_\theta
    \right]
\end{align}\]</span>
The last term vanishes since
<span class="math display">\[\begin{align}
    \mathbb E_{Y|X=x}[\nabla V^{Y|X}_\theta(x, y)]
    &amp;= \int P_{Y|X=x}(y) \dfrac 1 {P_{Y|X=x}(y)} \nabla_\theta P_{Y|X=x}(y)\, dy = 0 \\
    \mathbb E_{XY}\left[(V^X_\theta)^T V^{Y|X}_\theta\right]
    &amp;= \mathbb E_X \left[
        (V^X_\theta)^T \mathbb E_{Y|X=x} \left[
            V^{Y|X}_\theta
        \right]_{=0}
    \right] = 0
\end{align}\]</span>
</details>
<div class="corollary">
<p><span id="cor:unlabeled-div-67" class="corollary"><strong>Corollary 8.3  (monotonicity) </strong></span><span class="math inline">\(\mathcal J^{XY}_F(\theta) \geq \mathcal J^X_F(\theta)\)</span>, here <span class="math inline">\(\geq\)</span>
is in the sense of positive-semidefinite matrices.</p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-68" class="corollary"><strong>Corollary 8.4  (DPI) </strong></span><span class="math inline">\(\mathcal J^{X}_F(\theta) \geq \mathcal J^Y_F(\theta)\)</span> if <span class="math inline">\(\theta\to X\to Y\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-69" class="definition"><strong>Definition 8.10  (location family) </strong></span>For any density <span class="math inline">\(p_0\)</span> on <span class="math inline">\(\mathbb R^d\)</span>, one can define a location
family of distributions on <span class="math inline">\(\mathbb R^d\)</span> by setting
<span class="math inline">\(P_\theta(dx) = p_0(x-\theta)dx\)</span>. In this case the Fisher information is
translation-invariant, does not depend on <span class="math inline">\(\theta\)</span>, and its written instead
to emphasize its dependence on <span class="math inline">\(p_0\)</span>:
<span class="math display">\[
    J(p_0) = \mathbb E_{X\sim p_0} \left[(\nabla \ln p_0)(\nabla \ln p_0)^T\right]
    = -\mathbb E_{X\sim p_0} \left[
        \mathcal H_X \ln p_0
    \right]
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-70" class="proposition"><strong>Proposition 8.14  (properties of location family) </strong></span>Given distributions <span class="math inline">\(X, Y\)</span>, the following hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(J_F(XY) = J_F(X)+J_F(Y)\)</span>.</li>
<li><span class="math inline">\(J_F(a\theta + X) = a^2J_F(X)\)</span></li>
</ol>
</div>
</div>
<div id="local-χ²-behavior" class="section level2 unnumbered hasAnchor">
<h2>Local χ² behavior<a href="f-divergence.html#local-χ²-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The first theorem gives the linear coefficient of
<span class="math inline">\(D_f(\lambda P + \bar \lambda Q \| Q)\)</span> as <span class="math inline">\(\lambda\to 0^+\)</span>.
When <span class="math inline">\(P\ll Q\)</span>, the decay is always sublinear with
quadratic coefficient given by theorem <a href="f-divergence.html#thm:localChiExpansion">8.11</a>.
Else the linear coefficient is given by theorem <a href="f-divergence.html#thm:flocalLinearCoeff">8.10</a>.
When a distribution family satisfies additional regularity conditions
<a href="f-divergence.html#def:fisherRegularity">8.11</a>, the local behavior of <span class="math inline">\(\chi^2(P_\theta \|P_0)\)</span>
in $is governed
by the Fisher information matrix by
theorem <a href="f-divergence.html#thm:localFisherDiv">8.12</a>.</p>
<div class="theorem">
<p><span id="thm:flocalLinearCoeff" class="theorem"><strong>Theorem 8.10  (local linear coefficient) </strong></span>Suppose <span class="math inline">\(D_f(P\|Q)&lt;\infty\)</span> and <span class="math inline">\(f&#39;(1)\)</span> exists, then
<span class="math display">\[
    \lim_{\lambda\to 0} \dfrac 1 \lambda D_f(\lambda P + \bar \lambda Q \|Q)
    = (1 - P[Q\neq 0])f&#39;(\infty)
\]</span>
In particular, for <span class="math inline">\(P\ll Q\)</span>, the decay is always sublinear.</p>
</div>
<details>
<summary>
Proof
</summary>
Per proposition <a href="f-divergence.html#prp:fEquivProperties">8.4</a> we may assume
<span class="math inline">\(f(1)=f&#39;(1)=0\)</span> and <span class="math inline">\(f\geq 0\)</span>. Decompose <span class="math inline">\(p\)</span> into absolutely continuous
and singular parts
<span class="math display">\[
    P = \mu P_1 + \bar \mu P_0, \quad P_0\perp Q, \quad P_1\ll Q
\]</span>
Expand out the definition of divergence to obtain
<span class="math display">\[
    \dfrac 1 \lambda D_f(\lambda P + \bar \lambda Q \| Q)
    = \bar \mu f&#39;(\infty) + \int dQ \dfrac 1 \lambda f\left[
        1 + \lambda \left(
            \mu \dfrac{dP_1}{dQ} - 1
        \right)
    \right]
\]</span>
The function <span class="math inline">\(g(\lambda) = f(1 + \lambda t)\)</span> is positive and
convex for every <span class="math inline">\(\in \mathbb R\)</span>, then <span class="math inline">\(\dfrac 1 \lambda g(\lambda)\)</span>
decreases monotonically to <span class="math inline">\(g&#39;(0)=0\)</span> as <span class="math inline">\(\lambda \to 0^+\)</span>
(picture a zero-centered convex function). The integrand is
<span class="math inline">\(Q\)</span>-integrable at <span class="math inline">\(\lambda=1\)</span> (which dominates <span class="math inline">\(g(t, \lambda\leq 1)\)</span>,
then applying dominated covergence theorem yields the desired result.
</details>
<div class="theorem">
<p><span id="thm:localChiExpansion" class="theorem"><strong>Theorem 8.11  (local quadratic coefficient) </strong></span>Let <span class="math inline">\(f\)</span> be twice continuously differentiable on <span class="math inline">\((0, \infty)\)</span>
with <span class="math inline">\(\limsup_{x\to \infty} f&#39;&#39;(x)\)</span> finite.
If <span class="math inline">\(\chi^2(P\|Q)&lt;\infty\)</span> (in particular, <span class="math inline">\(P\ll Q\)</span>), then
<span class="math inline">\(D_f(\lambda P + \bar \lambda Q \|Q)\)</span> is finite for all
<span class="math inline">\(\lambda \in [0, 1]\)</span> and
<span class="math display">\[
    \lim_{\lambda \to 0} \dfrac 1 {\lambda^2} D_f(\lambda P + \bar \lambda Q \| Q)
    = \dfrac{f&#39;&#39;(1)}{2} \chi^2(P\|Q)
\]</span>
If <span class="math inline">\(\chi^2(P\|Q)=\infty\)</span> with <span class="math inline">\(f&#39;&#39;(1)&gt;0\)</span>, then
<span class="math inline">\(D_f(\bar \lambda Q + \lambda P \|Q) = \omega(\lambda^2)\)</span>.
This includes KL, SKL, <span class="math inline">\(H^2\)</span>, JS, LC, and all Renyi divergences
of orders <span class="math inline">\(\lambda &lt; 2\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
<p>Taylor expand with integral form, then apply the dominated
convergence theorem.</p>
Since <span class="math inline">\(P\ll Q\)</span>, we can use the expression <span class="math inline">\(D_f(P\|Q) = \mathbb E_Q[f(d_QP)]\)</span>.
W.l.o.g. again assume <span class="math inline">\(f&#39;(1) = f(1)=0\)</span>, then <span class="math inline">\(\chi^2\)</span> is written as
<span class="math display">\[\begin{align}
    D_f(\bar \lambda Q + \lambda P \| Q)
    &amp;= \int dQ f\left(1 - \lambda + \lambda \dfrac{dP}{dQ}\right)
    \, d\mu \\
    &amp;= \int dQ f\left(1 + \lambda \dfrac{dP - dQ}{DQ}\right)
    \, d\mu
\end{align}\]</span>
Next up, apply Taylor’s theorem with remainder (which is
integration by parts) to
<span class="math display">\[\begin{align}
    f(1+u) &amp;= u^2 \int_0^1 (1-t) f&#39;&#39;(1+tu)dt
\end{align}\]</span>
Apply to <span class="math inline">\(u=\lambda \dfrac{P-Q}{Q}\)</span> to obtain
<span class="math display">\[
    D_f(\bar \lambda Q + \lambda P \|Q) =
    \int dQ \int_0^1 dt (1-t) \lambda^2 \left(\dfrac{P-Q}{Q}\right)^2
    f&#39;&#39;\left(1 + t\lambda \dfrac{P-Q}{Q}\right)
\]</span>
Given the condition of the theorem, we can apply the dominated convergence
theorem and Fubini to obtain
<span class="math display">\[\begin{align}
    \lim_{\lambda \to 0} \dfrac 1 {\lambda^2}
    D_f(\bar \lambda Q + \lambda P \|Q)
    &amp;= \int_0^1 dt(1-t) \int dQ \left(\dfrac{P-Q}{Q}\right)^2
    \lim_{\lambda\to 0} f&#39;&#39;\left(1 + t\lambda \dfrac{P-Q}{Q}\right) \\
    &amp;= \int_0^1 (1-t)dt \int dQ \left(\dfrac{P-Q}{Q}\right)^2
    = \dfrac{f&#39;&#39;(1)}{2}\chi^2(P\|Q)
\end{align}\]</span>
</details>
<div class="definition">
<p><span id="def:fisherRegularity" class="definition"><strong>Definition 8.11  (regular single-parameter families) </strong></span>We call a single-parameter family <span class="math inline">\(\{P_t, t\in [0, \tau)\}\)</span>
regular at <span class="math inline">\(t=0\)</span> if the following holds:</p>
<ol style="list-style-type: lower-alpha">
<li>There is a dominating measure: <span class="math inline">\(P_t(dx)=p_t(x)\mu(dx)\)</span> for some
measurable <span class="math inline">\((t, x)\mapsto p_t(x)\)</span>.</li>
<li>Density varies smoothly in <span class="math inline">\(t\)</span>: there exists measurable <span class="math inline">\(\dot p_s(x)\)</span>
such that for <span class="math inline">\(\mu\)</span>-a.e. <span class="math inline">\(x_0\)</span> we have
<span class="math display">\[
\int_0^\tau |\dot p_s(x_0)|\, ds &lt; \infty, \quad p_
t(x_0) = p_0(x_0) + \int_0^t \dot p_s(x_0)\, ds
\]</span>
as well as <span class="math inline">\(\lim_{t\to 0^+} \dot p_t(x_0) = \dot p_0(x_0)\)</span>.</li>
<li><span class="math inline">\(p_0(x)=0\implies \dot p_t(x)=0\)</span>, and
<span class="math display">\[
\int \sup_{0\leq t&lt;\tau} \dfrac{\dot p_t(x)^2}{p_0(x)} \, d\mu &lt; \infty
\]</span></li>
<li>Condition (b) holds for <span class="math inline">\(h_t(x) = \sqrt{p_t(x)}\)</span>, and <span class="math inline">\(\{\dot h_t\}_{t\in [0, \tau)}\)</span>
is uniformly <span class="math inline">\(\mu\)</span>-integrable.</li>
</ol>
</div>
<p>In particular, conditions (b, c) implies that <span class="math inline">\(P_t\ll P_0\)</span>.</p>
<div class="theorem">
<p><span id="thm:localFisherDiv" class="theorem"><strong>Theorem 8.12  (local Fisher behavior of divergence) </strong></span>Given a family <span class="math inline">\(P_{t\in [0, \tau)}\)</span> in definition <a href="f-divergence.html#def:fisherRegularity">8.11</a>, we have
<span class="math display">\[\begin{align}
    \chi^2(P_t\|P_0) = J_F(0)t^2 + o(t^2), \quad
    D(P_t\|P_0) = \dfrac{\log e}{2}\mathcal J_F(0)t^2 + o(t^2)
\end{align}\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
<p>Idea: rewrite the difference in <span class="math inline">\(\chi^2\)</span> as an integral,
then apply the dominated convergence theorem.</p>
Recall that <span class="math inline">\(P_t\ll P_0\)</span>, then
<span class="math display">\[\begin{align}
    \dfrac 1 {t^2}\chi^2(P_t\|P_0)
    &amp;= \dfrac 1 {t^2} \int d\mu \dfrac{[p_t(x) - p_0(x)]^2}{p_0(x)} \\
    &amp;= \dfrac 1 {t^2} \int \dfrac{d\mu}{p_0(x)} \left(
        t\int_0^1 \dot p_{tu}(x)\, d\mu
    \right)^2 \\
    &amp;= \int d\mu \int_0^1 du_1 \int_0^1 du_2
    \dfrac{\dot p_{tu_1}(x) \dot p_{tu_2}(x)}{p_0(x)}
\end{align}\]</span>
By regularity condition (c), take the limit <span class="math inline">\(t\to 0\)</span> and
apply the monotone convergence theorem to yield
<span class="math display">\[\begin{align}
    \lim_{t\to 0}
    \dfrac 1 {t^2}\chi^2(P_t\|P_0)
    &amp;= \int d\mu \int_0^1 du_1 \int_0^1 du_2
    \lim_{t\to 0} \dfrac{\dot p_{tu_1}(x) \dot p_{tu_2}(x)}{p_0(x)} \\
    &amp;= \int d\mu \dfrac{\dot p_0(x)^2}{p_0(x)} = \mathcal J_F(0)
\end{align}\]</span>
</details>
<div class="proposition">
<p><span id="prp:unlabeled-div-71" class="proposition"><strong>Proposition 8.15  (variational characterization of Fisher information) </strong></span>For a density <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathbb R\)</span>, the location family Fisher information
<span class="math display">\[
    J(P) = \sup_h \dfrac{\mathbb E_P[h&#39;]^2}{\mathbb E_P[h]}
\]</span>
where the supremum is over test functions which are continuously
differentiable and compactly supported such that <span class="math inline">\(\mathbb E_P[h^2] &gt; 0\)</span>.</p>
</div>
<p>We defer details to textbook 7.13. This can be anticipated
from the variational <span class="math inline">\(\chi^2\)</span> formula <a href="f-divergence.html#eq:chiStatVar">(8.5)</a>
together with the local <span class="math inline">\(\chi^2\)</span> expansion <a href="f-divergence.html#thm:localChiExpansion">8.11</a>.
<span class="math display">\[
    \chi^2(P_t \| P)
    = \sup \dfrac{\left(\mathbb E[h(X+t) - h(X)\right)^2}{\mathbb E[h^2]}
    = \sup \dfrac{\mathbb E[h&#39;]^2}{\mathbb E[h^2]} t^2 + o(t^2)
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tensorization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="statistical-decision-applications.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
