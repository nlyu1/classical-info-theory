<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Lossless compression | 6.7480 Notes</title>
  <meta name="description" content="10 Lossless compression | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Lossless compression | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Lossless compression | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-decision-applications.html"/>
<link rel="next" href="hypothesis-testing-large-deviations.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html"><i class="fa fa-check"></i><b>12</b> Noisy Channel Coding</a>
<ul>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#optimal-decoder-weak-converse"><i class="fa fa-check"></i>Optimal decoder, weak converse</a></li>
<li class="chapter" data-level="" data-path="noisy-channel-coding.html"><a href="noisy-channel-coding.html#random-and-maximal-coding"><i class="fa fa-check"></i>Random and maximal coding</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="channel-capacity.html"><a href="channel-capacity.html"><i class="fa fa-check"></i><b>13</b> Channel Capacity</a></li>
<li class="chapter" data-level="14" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>14</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lossless-compression" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">10</span> Lossless compression<a href="lossless-compression.html#lossless-compression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section corresponds to Chapter 10 of the book.
Important takeaways:</p>
<ol style="list-style-type: decimal">
<li>Regimes for lossless data compression:
<ul>
<li>Single-shot: encode a symbol, then decode a symbol.
For single-shot encoding, one may consider grouping
several symbols into a super-symbol; a special case of this
is when the source is i.i.d.</li>
<li>Code extension (definition <a href="lossless-compression.html#def:uniqueDecodability">10.6</a>):
codewords are concatenated.</li>
</ul></li>
<li>Compression length of the optimal single-shot lossless
is related to the entropy density
(proposition <a href="lossless-compression.html#prp:entDensityDomination">10.2</a>) and is optimal in
every sense by stochastic dominance
(proposition <a href="lossless-compression.html#prp:optimalEncodingOptimality">10.1</a>).
<ul>
<li>Theorem <a href="lossless-compression.html#thm:optimalEncodingAvg">10.1</a>
for the the optimal average-length <span class="math inline">\(\mathbb E[L]\)</span>.</li>
<li>The code length distribution is also
related to that of entropy density
(theorem <a href="lossless-compression.html#thm:varCompOptDistribution">10.2</a>), resulting
in optimal-coding asymptotics for i.i.d. source grouped into
a supersymbol (corollary <a href="lossless-compression.html#cor:iidAsymp">10.2</a>).</li>
</ul></li>
<li>Kraft-McMillan (theorem <a href="lossless-compression.html#thm:kraftMcMillan">10.3</a>):
constructive bijection between inequality-satisfying code
lengths and prefix-free codes. In particular, in terms
of encoding length, the uniquely decodable codes do not
offer any advantage over prefix codes.</li>
<li>Theorem <a href="lossless-compression.html#thm:optimalPrefixLength">10.4</a>:
bound on optimal prefix code.</li>
<li>Main inequality: theorems
<a href="lossless-compression.html#thm:optimalEncodingAvg">10.1</a> and <a href="lossless-compression.html#thm:optimalPrefixLength">10.4</a>.
<span class="math display">\[
H(X) - \log_2[e(H(X)+1)] \leq \mathbb E[L_{\mathrm{single-shot}}^*(X)]
\leq H(X) \leq \mathbb E[L_{\mathrm{prefix}}^*(X)] \leq H(X)+1
\]</span><br />
</li>
<li>In fixed-length almost-lossless compression (definition <a href="lossless-compression.html#def:flal">10.7</a>),
a single emission of the source is compressed
to <span class="math inline">\(\{0, 1\}^k\)</span> instead of <span class="math inline">\(\{0, 1\}^*\)</span>.</li>
<li>Shannon’s noiseless coding theorem (corollary <a href="lossless-compression.html#cor:shannonNoiselessCoding">10.3</a>):
holds in the limit of asymptotically large copies of
the i.i.d. source <span class="math inline">\(R^n\)</span> v.s. the compression length <span class="math inline">\(nR\)</span>.</li>
<li>Asymptotic equipartition property (proposition <a href="lossless-compression.html#prp:aep">10.3</a>):
for asymptotically large copies of i.i.d. source, the dominant
occurences have equiprobability.</li>
</ol>
<p>To motivate later studies, we note two limitations of Huffmann codes
(optimal prefix-free variable-length code):</p>
<ol style="list-style-type: decimal">
<li>It requires knowing the source distribution. This is
addressed by the study of universal compression.
<ul>
<li>Lempel-Ziv is universal, has low complexity, and is provably
optimal for all ergodic sources.</li>
</ul></li>
<li>Often <span class="math inline">\(H(S^n)\ll nH(S)\)</span> (e.g. not all arrangements of letters are
words, and sentences are subject to semantic constraints).
Thus applying Huffmann to a block <span class="math inline">\((S_1, \cdots, S_n)\)</span> is much
more rate-efficient. However, this construction is exponential
in <span class="math inline">\(n\)</span>.
<ul>
<li>Arithmetic coding: complexity linear in the block-length
while attaining <span class="math inline">\(H(S_1^n)\)</span> length.</li>
</ul></li>
</ol>
<div id="variable-length-source-coding" class="section level2 unnumbered hasAnchor">
<h2>Variable-length source coding<a href="lossless-compression.html#variable-length-source-coding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Fixing a source domain <span class="math inline">\(\mathcal X\)</span>, we can define:</p>
<ul>
<li>A <strong>compressor</strong> <span class="math inline">\(f:\mathcal X\to \{0, 1\}^*\)</span>.</li>
<li>A <strong>decompressor</strong> <span class="math inline">\(g:\{0, 1\}^*\to \mathcal X\)</span>.</li>
</ul>
<p>The single-shot in the definition below refers to the fact that
we are compressing and decompressing each symbol, instead of
compressing many then decompressing them; this we do not
need to impose any constraints on <span class="math inline">\(f\)</span>, such as prefix-freeness or
unique-decodability.</p>
<div class="definition">
<p><span id="def:unlabeled-div-78" class="definition"><strong>Definition 10.1  (variable-length single-shot lossless compression) </strong></span>A pair <span class="math inline">\((f, g)\)</span> is a variable-length single-shot lossless compressor if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(f\)</span> is of the type <span class="math inline">\(f:\mathcal X\to \{0, 1\}^*\)</span>. The image of <span class="math inline">\(f\)</span>
is a codebook, and an element <span class="math inline">\(f(x)\)</span> in the image of <span class="math inline">\(f\)</span> is a codeword.</li>
<li>There exists a decompressor such that <span class="math inline">\(g\circ f = 1_{\mathcal X}\)</span>.<br />
</li>
</ol>
</div>
<p>Two immediate results of this definition:</p>
<ul>
<li>Lossless compression is only possible for discrete <span class="math inline">\(X\)</span>.</li>
<li>Without loss of generality, we can sort <span class="math inline">\(\mathcal X\)</span>
so that <span class="math inline">\(P(0)\geq P(1)\geq \cdots\)</span>.</li>
</ul>
<p>We begin with several definitions.</p>
<div class="definition">
<p><span id="def:unlabeled-div-79" class="definition"><strong>Definition 10.2  (entropy density) </strong></span>Given a finite alphabet and fixing binary units,
the entropy density function of a source <span class="math inline">\(X\)</span> is
<span class="math display">\[
    i_X(a) = \log_2 \dfrac 1 {P_X(a)}\implies H(X) = \mathbb E[i_X(x)]
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-80" class="definition"><strong>Definition 10.3  (varentropy) </strong></span>The varentropy <span class="math inline">\(V(S)\)</span> of a source <span class="math inline">\(S\)</span> is
<span class="math display">\[
    V(S) = \mathrm{Var}[i_S]
\]</span>
Note that <span class="math inline">\(H(S)^2 + V(S)^2 = \mathbb E[i_S^2]\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-81" class="definition"><strong>Definition 10.4  (stochastic dominance) </strong></span>Given two real-valued stochastic variables <span class="math inline">\(X, Y\)</span>, we say that <span class="math inline">\(X\preceq Y\)</span>,
or <span class="math inline">\(Y\)</span> stochastically dominates <span class="math inline">\(X\)</span>, if the CDF of <span class="math inline">\(X\)</span> dominates that of <span class="math inline">\(Y\)</span>
everywhere; this formalizes the notion that
<span class="math inline">\(X\)</span> is concentrated on smaller values.</p>
</div>
<div class="definition">
<p><span id="def:singleShotOpt" class="definition"><strong>Definition 10.5  (optimal single-shot lossless compressor) </strong></span>For a down-sorted PMF <span class="math inline">\(P(X)\)</span>, the optimal single-shot lossless compressor
assigns strings with increasing lengths to symbol <span class="math inline">\(j\in \mathcal X=\mathbb N\)</span>.
In particular, <span class="math inline">\(l(f^*(j)) = \lfloor \log_2 j\rfloor\)</span>, where <span class="math inline">\(l\)</span> is the
string-length function. <span style="color:blue"> Let <span class="math inline">\(L(j) = \lfloor \log_2 j \rfloor\)</span>
denote the compression length of input <span class="math inline">\(j\)</span></span>.</p>
</div>
<p>An immediate corollary of the following result is that
<span class="math inline">\(\mathbb E[(l\circ f^*)(X)] \leq \mathbb E[(l\circ f)(X)]\)</span> for any <span class="math inline">\(X\)</span>.</p>
<div class="proposition">
<p><span id="prp:optimalEncodingOptimality" class="proposition"><strong>Proposition 10.1  (optimality of optimal encoding) </strong></span>For any lossless single-shot compressor <span class="math inline">\(f\)</span> and source distribution <span class="math inline">\(X\)</span>,
<span class="math inline">\(l\circ f^*\preceq l\circ f\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Let <span class="math inline">\(A_k = \{x:l(f(x)) \leq k\}\)</span> denote the set of inputs which
are encoded to length at most <span class="math inline">\(k\)</span>. Note that, since
encoding is lossless and there are at most <span class="math inline">\(2^{k+1}-1\)</span> binary
strings of length <span class="math inline">\(k\)</span>, we have
<span class="math display">\[
    |A_k| \leq \sum_{j=0}^k 2^j = 2^{k+1}-1 = |A_k^*|
\]</span>
The stochastic dominance follows from the fact that <span class="math inline">\(f^*\)</span>
sorts the PMF in descending order:
<span class="math display">\[
    \mathrm{Pr}[(l\circ f)(X)\leq k]
    = \sum_{x\in A_k} P_X(x)
    \leq \sum_{x\in A^*_k} P_X(x) = \mathrm{Pr}[(l\circ f^*)(X) \leq k]
\]</span>
</details>
<div class="proposition">
<p><span id="prp:entDensityDomination" class="proposition"><strong>Proposition 10.2  (entropy density dominates encoding length) </strong></span>When <span class="math inline">\(X\)</span> is sorted (as in the case of optimal compressor), <span class="math inline">\(L\preceq i_X\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Note that <span class="math inline">\(1/x \leq P_X(x)\)</span>, then <span class="math inline">\(L(x)\leq i_X(x)\implies L\preceq i_X\)</span>
based on
<span class="math display">\[
    L(x) = \lfloor \log_2 x\rfloor\leq -\log_2 \dfrac 1 x \leq
    -\log_2 P_X(x) = i_X(x)
\]</span>
</details>
<p>A coding theorem is one that relates an operational,
intractable compression quantity (e.g. <span class="math inline">\(\mathbb E[L(X)]\)</span>)
to an information measure (e.g. <span class="math inline">\(H(X)\)</span>).</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-82" class="lemma"><strong>Lemma 10.1  </strong></span><span class="math inline">\(H(X|L=k)\leq k\)</span>. This follows from that <span class="math inline">\(X|L=k\)</span> can
take at most <span class="math inline">\(2^k\)</span> values, the uniform distribution
on which has entropy <span class="math inline">\(k\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:finiteExpectationExtremality" class="lemma"><strong>Lemma 10.2  </strong></span>For <span class="math inline">\(X\)</span> taking values on <span class="math inline">\(\mathbb N=\{1, 2, \cdots\}\)</span> and <span class="math inline">\(\mathbb E[X]&lt;\infty\)</span>,
we have
<span class="math display">\[
    H(X) \leq h\left(\mathbb E[X]^{-1}\right)\mathbb E[X]
\]</span>
The inequality is saturated by the geometric distribution.</p>
</div>
<details>
<summary>
Proof
</summary>
Let <span class="math inline">\(p = 1 / \mathbb E[X]\)</span> and note that <span class="math inline">\(\mathbb E[xp] = 1, \mathbb E[x\bar p] = \mathbb E[x(1-p)] = \mathbb E[x - 1]\)</span>, then
<span class="math display">\[\begin{align}
    H(X) - \mathbb E[X] h(p)
    &amp;= \mathbb E\left[-\log p(x) - x[-p\log p - \bar p\log \bar p]\right] \\
    &amp;= \mathbb E\left[-\log p(x) + xp \log p + x\bar p\log \bar p\right] \\
    &amp;= \mathbb E\left[-\log p(x) + (x-1) \log \bar p + \log p\right] \\
    -D(P_X\|\mathrm{Geom}_p)
    &amp;= -\mathbb E\log \dfrac{p(x)}{p{\bar p}^{x-1}} \\
    &amp;= \mathbb E\left[-\log p(x) + (x-1) \log \bar p + \log p\right] \\
    -D(P_X\|\mathrm{Geom}_p) &amp;= H(X) - \mathbb E[X] h(p) \\
    H(X) &amp;= \mathbb E[X] h(p) + D(P_X\|\mathrm{Geom}_p)_{\geq 0}
\end{align}\]</span>
</details>
<div class="theorem">
<p><span id="thm:optimalEncodingAvg" class="theorem"><strong>Theorem 10.1  (average length of optimal encoding) </strong></span><span class="math inline">\(H(X) - \log_2[e(H(X)+1)] \leq \mathbb E[L(X)] \leq H(X)\)</span></p>
</div>
<details>
<summary>
Proof
</summary>
<p>Assume <span class="math inline">\(X\)</span> sorted w.l.o.g,
in light of proposition <a href="lossless-compression.html#prp:entDensityDomination">10.2</a>,
we have <span class="math inline">\(L\preceq i_X\implies \mathbb E[L] \leq H(X)\)</span>.
On the other hand, first note that <span class="math inline">\(H(X|L=k)\leq k\)</span> since <span class="math inline">\(X|L=k\)</span> can
take at most <span class="math inline">\(2^k\)</span> values,
the uniform distribution on which has entropy <span class="math inline">\(k\)</span>.
Next apply lemma <a href="lossless-compression.html#lem:finiteExpectationExtremality">10.2</a>.</p>
<span class="math display">\[\begin{align}
    H(X)
    &amp;= H(X, L) = H(X|L) + H(L) \\
    &amp;\leq \mathbb E[L] + (\mathbb E[L] + 1) h\left(\dfrac 1 {1+\mathbb E[L]}\right) \\
    &amp;= \mathbb E[L] + \log(1+\mathbb E[L]) + \mathbb E[L] + \log \left(1 + \dfrac 1 {\mathbb E[L]}\right) \\
    &amp;= \mathbb E[L] + \log_2(1+\mathbb E[L]) + \log_2 e \leq \mathbb E[L] + \log_2 e(1+H(X))
\end{align}\]</span>
In the last tep steps, we used <span class="math inline">\(x\log(1+1/x) \leq \log e\)</span> and <span class="math inline">\(H(X)\leq \mathbb E[L]\)</span>.
</details>
<div class="theorem">
<p><span id="thm:varCompOptDistribution" class="theorem"><strong>Theorem 10.2  (code length distribution of optimal compression) </strong></span><span class="math inline">\(\forall \tau&gt;0, k\geq 0\)</span>:
<span class="math display">\[
    \mathrm{Pr}\left[i_X(X)\leq k \right] \leq
    \mathrm{Pr}[L(X) \leq k] \leq
    \mathrm{Pr}\left[i_X(X) \leq k + \tau\right] + 2^{1-\tau}
\]</span>
In other words, <span class="math inline">\(-\log_2 P_X\)</span> stochastically dominates <span class="math inline">\(L\)</span> but
only up to a constant factor.</p>
</div>
<span style="color:green">
Proof idea: <span class="math inline">\(\mathrm{Pr}[A]= \mathrm{Pr}[A, B] + \mathrm{Pr}[A, B^c] \leq \mathrm{Pr}[B] + \mathrm{Pr}[A, B^c]\)</span>
</span>
<details>
<summary>
Proof
</summary>
<ul>
<li>Lower bound (achievability, or “compression length is smaller than”):
proposition <a href="lossless-compression.html#prp:entDensityDomination">10.2</a>.</li>
<li>Upper bound (converse, or “compression length cannot be greater than”):
<span class="math display">\[\begin{align}
  \mathrm{Pr}[L\leq k]
  &amp;= \mathrm{Pr}[L \leq k, i_X(X)\leq k+\tau] + \mathrm{Pr}[L\leq k, i_X(X)&gt; k+\tau] \\
  &amp;= \mathrm{Pr}[i_X\leq k+\tau] + (2^{k+1}-1) \sup_{i_X(X)&gt;k+\tau} P_X(x) \\
  &amp;= \mathrm{Pr}[i_X\leq k + \tau] + 2^{1-\tau}
\end{align}\]</span>
To bound the first quantity, we used <span class="math inline">\(\mathrm{Pr}[A, B]\leq \mathrm{Pr}[B]\)</span>. For the second,
there are at most <span class="math inline">\(2^{k+1}-1\)</span> atoms of mass at most <span class="math inline">\(2^{-k-\tau}\)</span>.</li>
</ul>
</details>
<p>We can consider the source as a random process <span class="math inline">\((S_1, S_2, \cdots)\)</span>,
group the first <span class="math inline">\(n\)</span> (blocklength) symbols into a supersymbol <span class="math inline">\(S^n\)</span>.</p>
<div class="corollary">
<p><span id="cor:unlabeled-div-83" class="corollary"><strong>Corollary 10.1  (asymptotic optimal-coding properties) </strong></span>Let <span class="math inline">\((S_1, S_2, \cdots)\)</span> be a random process (such that <span class="math inline">\(S^n\)</span> is sorted)
and <span class="math inline">\(U, V\)</span> real-valued r.vs, then
<span class="math display">\[\begin{align}
    \dfrac 1 n i_{S^n(S^n)}\xrightarrow d U
    &amp;\iff \dfrac 1 n L(S^n)\xrightarrow d U  \\
    \dfrac 1 {\sqrt n} \left[
        i_{S^n}(S^n) - H(S^n)
    \right] \xrightarrow d V &amp; \iff \dfrac 1 {\sqrt n} \left[
        L(S^n) - H(S^n)
    \right] \xrightarrow d V
\end{align}\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
To obtain the first result, apply <a href="lossless-compression.html#thm:varCompOptDistribution">10.2</a>
with <span class="math inline">\(k=un\)</span> and <span class="math inline">\(\tau = \sqrt n\)</span>. For the second,
apply with <span class="math inline">\(k=H(S^n) + \sqrt n u\)</span> and <span class="math inline">\(\tau = n^{1/4}\)</span>.
<span class="math display">\[\begin{align}
\mathrm{Pr}[i_X \leq H + \sqrt n u]  &amp;\leq \mathrm{Pr}[L \leq H + \sqrt n u]
\leq \mathrm{Pr}[i_X \leq H + \sqrt n u + n^{1/4}] + 2^{1-n^{1/4}} \\
\mathrm{Pr}\left[
    \dfrac{i_X - H} {\sqrt n} \leq u
\right] &amp;\leq \mathrm{Pr}\left[
    \dfrac{L-H}{\sqrt n} \leq u
\right] \leq \mathrm{Pr}\left[
    \dfrac{i_X - H}{\sqrt n} \leq u + n^{-1/4}
\right] + 2^{1-n^{1/4}}
\end{align}\]</span>
</details>
<div class="corollary">
<p><span id="cor:iidAsymp" class="corollary"><strong>Corollary 10.2  (i.i.d asymptotics) </strong></span>When <span class="math inline">\(S_j\)</span> are i.i.d, we have</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(L(S^n)/n \xrightarrow P H(S)\)</span>.</li>
<li>If varentropy <span class="math inline">\(V(S)&lt;\infty\)</span>, then <span class="math inline">\(V\)</span> is Gaussian by CLT and
<span class="math display">\[
\dfrac 1 {\sqrt n V(S)} [L(S^n) - nH(S)] \xrightarrow d \mathcal N(0, 1)
\]</span>
equivalently, in shorthand <span class="math inline">\(L(S^n) \sim nH(S) + \sqrt{nV(S)} \mathcal N(0, 1)\)</span>.</li>
</ol>
</div>
</div>
<div id="uniquely-decodable-codes" class="section level2 unnumbered hasAnchor">
<h2>Uniquely decodable codes<a href="lossless-compression.html#uniquely-decodable-codes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now focus on compressors whose output stream can be uniquely decoded.
We consider a finite alphabet <span class="math inline">\(\mathcal A\)</span> and let
<span class="math inline">\(\mathcal A^*=\bigcup_{j=1}^\infty A^j\)</span>.</p>
<div class="definition">
<p><span id="def:uniqueDecodability" class="definition"><strong>Definition 10.6  (code extension, unique decodability, prefix code) </strong></span>The symbol-by-symbol extension of <span class="math inline">\(f:\mathcal A\to \{0, 1\}^*\)</span> to
<span class="math inline">\(A^*\to \{0, 1\}^*\)</span> is obtained by concatenating outputs.
A compressor <span class="math inline">\(f\)</span> is uniquely decodable if its extension is injective;
it is a prefix (-free) code if no codeword is a prefix of each other.</p>
</div>
<p>Lossless codes <span class="math inline">\(\supsetneq\)</span>
uniquely decodable codes <span class="math inline">\(\supsetneq\)</span> prefix codes. Consider</p>
<ul>
<li><span class="math inline">\(f(a)=0, f(b)=1, f(c)=10\)</span>: lossless but not uniquely decodable.</li>
<li><span class="math inline">\(f(a)=0, f(b)=01, f(c)=011, f(d)=0111\)</span>:
uniquely decodable but not prefix; decoder needs to look for the next <span class="math inline">\(0\)</span>
to know when token ends.</li>
<li><span class="math inline">\(f(a)=0, f(b)=10, f(c)=11\)</span>: prefix code.</li>
</ul>
<p>Prefix codes are in bijective correspondence with binary trees.</p>
<div class="theorem">
<p><span id="thm:kraftMcMillan" class="theorem"><strong>Theorem 10.3  (Kraft-McMillan) </strong></span>Given a uniquely decodable <span class="math inline">\(f:\mathcal A\to \{0, 1\}^*\)</span>
and let <span class="math inline">\(L=l\circ f\)</span> be the code length function. Then <span class="math inline">\(f\)</span>
satisfies the Kraft inequality
<span class="math display">\[
    \sum_{a\in \mathcal A} 2^{-L(a)} \leq 1
\]</span>
With <span class="math inline">\(2\mapsto D\)</span> for general size-<span class="math inline">\(D\)</span> alphabets.
Conversely, for any set of code length <span class="math inline">\(\{L(a):a\in \mathcal A\}\)</span>
satisfying the Kraft inequality, there exists an efficiently
computable prefix code with length function <span class="math inline">\(L\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
<p>Let <span class="math inline">\(f\)</span> be a uniquely decodable code. Assuming <span class="math inline">\(\mathcal A\)</span> finite
and define <span class="math inline">\(L^*=\max_{a\in \mathcal A} L(a)\)</span>. Let
<span class="math display">\[
    G(f, z) = \sum_{a\in \mathcal A} z^{L(a)}
    = \sum_{l=0}^{L^*} A(f, l) z^l
\]</span>
here <span class="math inline">\(A(f, l)\)</span> is the number of codewords of length <span class="math inline">\(l\)</span> in <span class="math inline">\(f\)</span>.
Consider the extension <span class="math inline">\(f^{k\geq 1}\)</span> and note that
<span class="math display">\[
    G(f^k, z) = \sum_{a^k \in \mathcal A^k} z^{L(a_1)+\cdots + L(a_k)}
    = G(f, z)^k = \sum_{l=0}^{kL} A(f^k, l)z^l
\]</span>
The key step here is the combinatorial equation
<span class="math inline">\(G(f^k, z) = G(f, z)^k\)</span>. Since <span class="math inline">\(f^k\)</span> is lossless, we have
<span class="math inline">\(A(f^k, \forall l)\leq 2^l\)</span>, then</p>
<ul>
<li><p><span class="math inline">\(G(f^k, 1/2) \leq kL\)</span> for all <span class="math inline">\(k\)</span>, then
<span class="math inline">\(G(f, 1/2) = G(f^k, 1/2)^{1/k} \leq (kL)^{1/k}\)</span>.</p></li>
<li><p>Take the limit <span class="math inline">\(k\to \infty (kL)^{1/k}=1\)</span>
to obtain <span class="math inline">\(G(f, k)\leq 1\)</span>.
The countably infinite case concludes by the arbitrariness of
a finite subset <span class="math inline">\(\mathcal A&#39;\subset \mathcal A\)</span>.
For the converse, without loss of generality relabel <span class="math inline">\(\mathcal A\)</span>
to <span class="math inline">\(\mathcal N\)</span> and assume <span class="math inline">\(1\leq L(1)\leq L(2) \leq \cdots\)</span>.
Given a set of lengths <span class="math inline">\(\{L(a\in \mathcal A)\}\)</span>
satisfying <span class="math inline">\(\sum_{a\in \mathcal A} 2^{-L(a)} \leq 1\)</span>, define for each <span class="math inline">\(j\)</span>
<span class="math display">\[
  a_j = \sum_{k=1}^{j-1} 2^{-L(k)} \leq 1, \quad a_1 = 0
\]</span>
Define <span class="math inline">\(f(j)\)</span> as the first <span class="math inline">\(L(j)\)</span> bits in the
binary expansion of <span class="math inline">\(a_j\)</span>. In other words, to code <span class="math inline">\(i\)</span>:</p></li>
<li><p>Start with a <span class="math inline">\(l_i\)</span> string of zeros.</p></li>
<li><p>Flip positions <span class="math inline">\(l_1, \cdots, l_{i-1}\)</span> to <span class="math inline">\(1\)</span>; this is the codeword.</p></li>
</ul>
To show that this is prefix-free, note that the codeword has the
interpretation of the binary expansion in <span class="math inline">\([0, 1]\)</span>.
Then <span class="math inline">\(f(i)\)</span> being a prefix of <span class="math inline">\(f(j\neq i)\)</span> implies that the codings
agree on the most significant <span class="math inline">\(L(i)\)</span> bits so <span class="math inline">\(a_j - a_i \leq 2^{-L(i)}\)</span>.
However, by definition of the encoding
<span class="math inline">\(a_j - a_i = 2^{-L(i)} + 2^{-L(i+1)} + \cdots &gt; 2^{-L(i)}\)</span>.
</details>
<p>In particular, the Kraft inequality implies that the sorted prefix-code
lengths can be at most <span class="math inline">\(1, 2, \cdots\)</span>. One can also thus formulate
the prefix-code with <em>optimal length</em> as the following integer-programming
problem:
<span class="math display" id="eq:optimalPrefix">\[
    L^*(X) = \min_{L:\mathcal A\to \mathbb N}
    \sum_{a\in \mathcal A} P_X(a) L(a) \text{  s.t.  }
    \sum_{a\in \mathcal A}2^{-L(a)} \leq 1
    \tag{10.1}
\]</span></p>
<p>We now proceed to analyze the bound on uniquely decodable codes.</p>
<div class="theorem">
<p><span id="thm:optimalPrefixLength" class="theorem"><strong>Theorem 10.4  (length of optimal prefix code) </strong></span><span class="math inline">\(H(X) \leq L^*(X) \leq H(X)+1\)</span> in units of bits.</p>
</div>
<details>
<summary>
Proof
</summary>
Achievability is established by the Shannon code with
<span class="math display">\[
    l_a = \lceil \log_2 \dfrac 1 {P_X(a)}\rceil \implies
    \sum_{a\in \mathcal A} 2^{-l_a} \leq \sum_{a\in \mathcal A} P_X(a) = 1
\]</span>
For the converse, for encoding <span class="math inline">\(f\)</span> with lengths <span class="math inline">\(\{l_a\}\)</span>
satisfying the Kraft inequality, define a normalized probability
measure <span class="math inline">\(Q_X(a) = 2^{-l_a} / Z\)</span> with <span class="math inline">\(Z=\sum 2^{-l_a} \leq 1\)</span>. Then
<span class="math display">\[\begin{align}
    \mathbb E_P[l\circ f] - H(X)
    &amp;= \sum P_X(a) l_a + \sum P_X(a) \log P_X(a) \\
    &amp;= \sum P_X(a) \log 2^{l_a} P_X(a)
    = \sum P_X(a) \log \dfrac{P_X(a)}{2^{-l_a}} \\
    &amp;= \sum P_X(a) \log \dfrac{P_X(a)}{Q_X(a)} - \sum P_X(a) \log Z \\
    &amp;= D(P\|Q) - (\log Z)_{\geq 0} \geq 0
\end{align}\]</span>
</details>
<p>The optimal encoding is achieved by the Huffmann code:
given PMF <span class="math inline">\(P_X(a\in \mathcal A)\)</span>:
For each step, choose the two least probable symbols in the alphabet,
merge into a super-symbol with combined probability, then
update the tree representation with super-symbol at the node
and merged symbols as leaves. Repeat for <span class="math inline">\(|\mathcal A| - 1\)</span> steps.</p>
</div>
<div id="fixed-length-source-coding" class="section level2 unnumbered hasAnchor">
<h2>Fixed-length source coding<a href="lossless-compression.html#fixed-length-source-coding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To be precise, we consider
<strong>fixed-length almost-lossless</strong> compression.
A single emission
of the source is compressed to
<span class="math inline">\(\{0, 1\}^k\)</span> instead of <span class="math inline">\(\{0, 1\}^*\)</span>.</p>
<div class="definition">
<p><span id="def:flal" class="definition"><strong>Definition 10.7  (fixed-length almost-lossless compression) </strong></span>A compressor-decompressor pair
<span class="math inline">\(f:\mathcal X\to \{0, 1\}^k, g:\{0, 1\}^k\to\mathcal X\cup \{e\}\)</span>
is a fixed-length almost-lossless
<span class="math inline">\((k, \epsilon)\)</span> source-code for <span class="math inline">\(X\)</span> if
<span class="math display">\[
    (g\circ f)(\forall x) \in \{x, e\}
    \text{ and } \mathrm{Pr}[g\circ f = e] \leq \epsilon
\]</span>
Fixing <span class="math inline">\(X, k\)</span>, the fundamental limit of fixed-length compression is
<span class="math display">\[
    \epsilon^*(X, k) = \inf\{\epsilon:
    \exists (k, \epsilon)\text{-code for } X\}
\]</span></p>
</div>
<p>Recall the optimal single-shot (variable-length) lossless
compressor length <span class="math inline">\(L\)</span> in definition <a href="lossless-compression.html#def:singleShotOpt">10.5</a>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-84" class="theorem"><strong>Theorem 10.5  (fundamental limit of fixed-length compression) </strong></span>Assuming pmf-sorted alphabet on <span class="math inline">\(\mathbb N\)</span>, then
<span class="math display">\[
    \epsilon^*(X, k) = \mathrm{Pr}[L(X) \geq k] = \sum_{x\geq 2^k} P_X(x)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The compressor reserves <span class="math inline">\(1^k\)</span> for the error message.
To suppress the error rate, we choose to encode the top <span class="math inline">\(2^k-1\)</span>
symbols with the highest probabilities.
</details>
<div class="corollary">
<p><span id="cor:shannonNoiselessCoding" class="corollary"><strong>Corollary 10.3  (Shannon's noiseless source coding theorem) </strong></span>Given i.i.d. discrete source <span class="math inline">\(S^n\)</span>, for any rate <span class="math inline">\(R&gt;0\)</span>
and <span class="math inline">\(\gamma\in \mathbb R\)</span> we have
<span class="math display">\[
    \lim_{n\to \infty} \epsilon^*(S^n, nR) = \begin{cases}
        0 &amp; R &gt; H(S) \\
        1 &amp; R &lt; H(S)
    \end{cases}
\]</span>
If varentropy <span class="math inline">\(V(S) &lt; \infty\)</span> then for <span class="math inline">\(Q\)</span> being one minus
the CDF of <span class="math inline">\(\mathcal N(0, 1)\)</span>:
<span class="math display">\[
    \lim_{n\to \infty} \epsilon^*(S^n, nH(S)+ \sqrt{nV(S)}\gamma) = Q(\gamma)
\]</span></p>
</div>
<p>The second distribution claim follows from the asymptotic
normality of the optimal encoding length (corollary <a href="lossless-compression.html#cor:iidAsymp">10.2</a>).
This implies that if we allow a non-vanishing error <span class="math inline">\(\epsilon\)</span>, then
compression is possible down to
<span class="math display">\[
    k = nH(S) + \sqrt{nV(S)} Q^{-1}(\epsilon)
\]</span></p>
<div class="theorem">
<p><span id="thm:finiteLengthBounds" class="theorem"><strong>Theorem 10.6  (finite block-length bounds) </strong></span>Placeholder.</p>
</div>
<div class="proposition">
<p><span id="prp:aep" class="proposition"><strong>Proposition 10.3  (asymptotic equipartition (AEP)) </strong></span>Given i.i.d. <span class="math inline">\(S^n\)</span> and <span class="math inline">\(\delta&gt;0\)</span>, define
the entropy <span class="math inline">\(\delta\)</span>-typical set
<span class="math display">\[
    T_n^\delta = \left\{
        s^n : \left| \dfrac 1 n \log \dfrac 1 {P_{S^n}(s^n)}
        - H(S) \right| \leq \delta  
    \right\}
\]</span>
Then <span class="math inline">\(\lim_{n\to \infty} \mathrm{Pr}[S^n\in T_n^\delta]\to 1\)</span>
and <span class="math inline">\(|T_n^\delta| \leq \exp[n(H(S)+\delta)]\)</span>.</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-decision-applications.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-testing-large-deviations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
