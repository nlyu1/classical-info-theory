[["index.html", "6.7480 Notes Preface Administrative trivia Motivation", " 6.7480 Notes Nicholas Lyu 2024-10-11 Preface Fundamentally, information and entropy captures how our uncertainty in a quantity changes after observations. Some recurring themes in the book are: Fundamental properties of information measures (e.g. entropy, mutual information, divergence, Fisher information) are: Nonnegativity. Monotonicity: joint provides more information than marginals. For especially well-behaved ones (entropy, KL, Fisher information) we expect additive decomposition, i.e. chain rule. Data-processing inequality. Divergence as a fundamental concept in information theory. Quantifies difference between distributions; the choice of this quantification depends on our problem. Convexity of \\(f\\) for \\(f\\)-divergence is mathematically equivalent to our information intuitions: data-processing inequality, monotonicity, etc. KL-divergence (and Rényi) is special because its \\(\\log\\) form admits additive decomposition of joint divergence. Convexity of information measures correspond to variational characterizations, which are extremely useful because: They provide bounds: choose the varied quantity to be our friend! Provide tractable variational approximations using numerical optimization methods (e.g. VAE, GAN). Mutual information as information radius, or center of gravity; capacity as a minimax saddle point. Divergence measures are extremely useful for bounding events: bound the event for a tractable distribution, bound the divergence between the tractable and given distribution, and we can bound the event for the given distribution. The foundational results in this book include: Additive decomposition of KL. Golden formula: variational characterization of mutual information. Saddle point characterization of mutual information. Donsker-Varadhan (theorem 5.5): variational characterization of KL, extended to \\(f\\)-divergences in theorem 8.9. Finite-partition approximation theorem for \\(f\\)-divergence. Harremoës-Vadja (theorem 8.8): one theorem to rule them all for \\(f\\)-divergence inequalities. Most \\(f\\)-divergences are locally \\(\\chi^2\\)-like about \\(\\lim_{\\lambda \\to 0^+} D_f(\\lambda P + \\bar \\lambda Q \\| Q)\\) and decays quadratically (theorem 8.12). Useful proof ideas: Asymptotic analysis proofs: rewrite a difference as an integral, then apply the monotone convergence theorem. Theorems 8.13, 8.12. Unique extremality proofs: reduce the desired inequality to the information inequality (theorem 3.1) or a nonnegative information quantity (e.g. entropy). Corollary 3.1, theorem 3.3, lemma 9.2; the latter two characterized Gaussian and geometric distributions. Occasionally we need to introduce a tilted distribution based on both inputs, see Donsker-Varadhan (5.5) or Bounds on conditional quantities: consider the joint quantity and decompose in \\(2\\) different ways. KL-DPI theorem 3.5. Meta-converse: instead of studying \\(P_{X\\hat X}\\) for arbitrary \\(\\hat X\\), introduce a tractable \\(Q_{X\\hat X}\\), to bound an event, then use DPI (or divergence inequalities like Donsker-Varadhan) to bound the same event for \\(P_{X\\hat X}\\) together with some divergence quantity. Fano’s inequality 4.4. Convexity proofs: introduce an additional parameter \\(\\theta \\sim \\mathrm{Ber}(\\lambda)\\) to rewrite \\(\\lambda f(x)+\\bar \\lambda f(y)\\) as a conditional quantity, then use decomposition. See section on convexity. Counting bounds: to bound \\(|\\mathcal C|\\), draw an element uniformly from \\(\\mathcal C\\), then upper-bound the joint-entropy by marginal bound, Han’s inequality, or chain rule. Typicality methods (compression): break region into tractable region of large probability and difficult region with low probability. Break a single \\(\\sup\\) into nested ones. Solve the inner \\(\\sup\\) analytically: useful characterization of \\(\\chi^2\\) (8.4). Alternate inner and alter closed-form solution: EM algorithm. Administrative trivia Professor email: yp@mit.edu. Office Hours: LIDS (\\(6\\)th floor of \\(D\\)-tower, lobby). Motivation Why is training GPT \\(\\iff\\) building a great text compressor? Given \\(2\\) families of distributions \\(P_\\theta, \\theta\\in \\Theta_1, \\Theta_2\\). To have \\(P_{\\mathrm{err}} \\leq \\delta\\), the number of samples is asymptotically \\(\\left(\\min D(P_{\\theta_1}\\|P_{\\theta_2})\\right)^{-1} \\log(1/\\delta)\\) Noisy channel coding. Cramer-Rau bound \\[ \\min_{\\hat \\theta} \\max_{\\theta\\in \\Theta} \\mathbb E[(\\theta - \\hat \\theta^2] \\asymp \\dfrac 1 {n\\min \\mathcal I_F(\\theta)} \\] How many bits to represent a random function / signal / picture with fidelity \\(\\epsilon\\). Key concepts: entropy, KL-divergence, mutual information, fisher information, mutual information. Todo list Mandatory: Gaussian saddle-point. Variational characterizations of \\(f\\)-divergences. Refine TV and Hellinger. Optional: Entropy-power inequality Sufficient statistic. "],["entropy.html", "1 Entropy Basics Combinatorial properties", " 1 Entropy In my interpretation, the fundamental properties of entropy are: Additive under independent joint: \\(H(X, Y) = H(X)+H(Y)\\) when \\(X\\perp\\!\\!\\!\\perp Y\\), with subadditivity when \\(X, Y\\) are not independent. Subtractive under conditioning: \\(H(X|Y) = H(X, Y) - H(X)\\) (chain rule). Conditioning decreases entropy (implication of 1, 2) Strong subadditivity is the mathematical equivalent of “conditioning decreases entropy,” which is further equivalent to submodularity. The two tricks in the proofs below are: The chain \\(H_n/n \\leq \\cdots \\leq H_k/k \\leq \\cdots H_1\\) is equivalent to \\(H_{k+1}-H_k \\leq H_k - H_{k-1}\\). (Han’s inequality). Expand “probability” to empirical lists and take appropriate limit (Shearer’s lemma). When reduction inequalities are present (e.g. submodularity), reason about an easy case (e.g. nested chain) then reduce using the reduction inequality (Shearer’s lemma). Basics Definition 1.1 (Entropy) The shannon entropy of a discrete random variable with probability mass function \\(P_x(x\\in \\mathcal X)\\) is \\[ H(X) = \\mathbb E\\left[\\log \\dfrac 1 {P_X(X)}\\right] \\] The conditional entropy is \\[ H(X|Y) = \\mathbb E_{y\\sim P_Y}\\left[ H(P_{X|Y=y}) \\right] \\] Definition 1.2 (convexity) A subset \\(S\\) of a vector space is convex if \\[ x, y\\in S\\implies \\alpha x + \\bar \\alpha y \\in S, \\quad \\alpha \\in [0, 1], \\quad \\bar \\alpha = 1 - \\alpha \\] A function \\(f:S\\to \\mathbb R\\) is convex if \\[ f(\\alpha x + \\bar \\alpha y) \\leq \\alpha f(x) + \\bar \\alpha f(y) \\] For any \\(S\\)-valued r.v. \\(X\\), \\(f\\) is convex implies \\(f(\\mathbb E[X]) \\leq \\mathbb E[f(X)]\\). Theorem 1.1 (properties of entropy) Positivity \\(H(X)\\geq 0\\) with equality iff \\(X\\) is constant. Uniform distribution maximizes entropy: for finite \\(\\mathcal X, H(X)\\leq \\log |\\mathcal X|\\) with equality iff \\(X\\) is uniform on \\(X\\). Chain rule: \\(H(X, Y)=H(X)+H(Y|X) \\leq H(X)+H(Y)\\). Conditioning decreases entropy: \\(H(X|Y)\\leq H(X)\\) with equality iff \\(X, Y\\) are independent. Deterministic transformation: \\(H(X)=H(X, f(X))\\geq H(f(X))\\) with equality iff \\(f\\) is injective on the support of \\(P_X\\). Chain rule: equality holds iff \\(X_1, \\cdots, X_n\\) are mutually independent. \\[ H(X_1, \\cdots, X_n) = \\sum_{j=1}^n H(X_j|X^{j-1}) \\leq \\sum_{j=1}^n H(X_j) \\] Proof: (b) \\(x\\mapsto \\log x\\) is concave, so \\[ H(X) = \\mathbb E\\log \\dfrac 1 {P_X(X)} \\leq \\log \\mathbb E\\left[\\dfrac 1 {P_X(X)}\\right] = \\log |\\mathcal X| \\] (c) Telescoping \\[ H(X, Y) = \\mathbb E\\left[\\log \\dfrac 1 {P_{X, Y}(X, Y)}\\right] = \\mathbb E\\left[\\log \\dfrac 1 {P_X(X)}\\right] + \\mathbb E\\left[\\log \\dfrac 1 {P_{Y|X}(Y|X)}\\right] \\] (d) Apply Jensen’s inequality to the strictly concave (over \\([0, 1]\\)) function \\(-x\\log x\\). \\[ H(X|Y) = \\sum_x \\mathbb E_Y\\left[P(x|Y) \\log \\dfrac 1 {P(x|Y)}\\right] \\leq \\sum_x P(x)\\log \\dfrac 1 {P(x)} = H(X) \\] Jenson’s equality is saturated when \\(\\forall x: \\mathbb E[P(x|Y)] = P(x) \\iff X \\perp\\!\\!\\!\\perp Y\\). (e) \\(H(X, f(X)) = H(f(X)) + H(f(X)|X) = H(X) + H(f(X)|X)\\), and \\(H(f(X)|X)=0\\iff f(X)\\) is constant given \\(X\\). Theorem 1.2 (axiomatic characterization of entropy) Denote a distribution \\(P=(p_1, \\cdots, p_m)\\) on \\(m\\) letters and a functional \\(H_m(p_1, \\cdots, p_m)\\). \\(H_m(P) = H(P)\\) has to be the Shannon entropy if it obeys the following axioms: Permutation invariance: \\(H(P\\circ \\pi) = H(P)\\) Expandibility: \\(H_{m+1}(P\\oplus [0]) = H_m(P)\\) Subadditivity: \\(H(X, Y)\\leq H(X)+H(Y)\\), with equality if \\(X\\perp\\!\\!\\!\\perp Y\\). The saturation constraint is equivalently \\[ H_{mn}(p_1q_1, \\cdots, p_mq_n) = H_m(P) + H_n(Q) \\] Normalization: \\(H_2(1/2, 1/2)=\\log 2)\\) Continuity: \\(H_2(p, 1-p)\\to 0\\) as \\(p\\to 0\\). Definition 1.3 (Renyi entropy) The Renyi entropy of order \\(\\alpha\\) is \\[ H_\\alpha(P) = \\begin{cases} \\dfrac 1 {1-\\alpha} \\log \\sum_j p_j^\\alpha &amp; \\alpha \\in (0, \\infty)-\\{1\\} \\\\ \\min_j \\log(1/p_j) &amp; \\alpha = \\infty \\end{cases} \\] Renyi entropy is non-increasing in \\(\\alpha\\) and tends to the Shannon entropy as \\(\\alpha\\to 1\\). The “type” of a sequence refers to the empirical distribution of symbols in the sequence (physical “macrostate”, and the exact sequence corresponds to the “microstate”). The following proposition shows that the multinomial coefficient can be approximated up to a polynomial term by \\(\\exp(nH(P))\\). Proposition 1.1 (method of types) Given non-negative integers \\((n_j)_{1\\leq j\\leq k}\\) with \\(\\sum n_j=n\\) and denote the distribution \\((p_j) = (n_j/n)\\), then the multinomial coefficient satisfies \\[ \\dfrac 1 {(1+n)^{k-1}} e^{nH(P)} \\leq \\binom{n}{(n_j)} \\leq e^{nH(P)}, \\quad \\binom{n}{(n_j)} = \\dfrac{n!}{\\prod n_j!} \\] Proof: Fix \\(P\\), let \\((X_j)\\sim P\\) i.i.d and let \\(N_j\\) denote the number of occurences of \\(j\\), then \\((N_j)\\) has a multinomial distribution \\[ \\mathrm{Pr}((N_j)) = \\binom{n}{(n_j)} \\prod_j p_j^{n_j} = \\binom{n}{(n_j)} e^{-nH(P)} \\leq 1 \\] The upper bound follows from \\(\\mathrm{Pr}((N_j))\\leq 1\\). For the lower-bound, note that \\((N_j)\\) takes at most \\((n+1)^{k-1}\\) values (each entry takes values in \\(\\{0, \\cdots, n\\}\\), and there are \\(k-1\\) degrees of freedom). Now \\[ 1 = \\sum_{(N_j)} \\mathrm{Pr}((N_j)) \\leq (n+1)^{k-1} \\max \\mathrm{Pr}((N_j)) = (n+1)^{k-1} \\binom{n}{(N_j)} e^{-nH(P)} \\] Combinatorial properties Definition 1.4 (submodularity, monotone) Define \\([n]=\\{1, \\cdots, n\\}\\) and let \\(\\binom{S}{k}\\) denote subsets of \\(S\\) of size \\(k\\), \\(2^S\\) all subsets of \\(S\\). A set function \\(f:2^S\\to \\mathbb R\\) is submodular if \\(\\forall T_1, T_2\\subset S\\) \\[ f(T_1\\cup T_2) + f(T_1\\cap T_2) \\leq f(T_1) + f(T_2) \\] This captures the sense that “adding elements yield diminishing returns” by rearranging the equation \\[ f(T_1\\cup T_2) - f(T_2)\\leq f(T_1) - f(T_1\\cap T_2) \\] The set function \\(f\\) is monotone if \\(T_1\\subset T_2\\implies f(T_1)\\leq f(T_2)\\). Theorem 1.3 (strong subadditivity of entropy) Let \\(X^n\\) be a discete RV, then \\(T\\mapsto H(X_T)\\) is monotone and submodular. Proof: Let \\(A=X_{T_1-T_2}, B=X_{T_1\\cap T_2}, C-X_{T_2-T_1}\\), we need to show that \\[ H(A, B, C) + H(B) \\leq H(A, B) + H(B, C) \\] Monotonicity is apparant. Submodularity follows from the fact that conditioning decreases entropy \\[\\begin{align} H(A, B, C) - H(A, B) &amp;\\leq H(B, C) - H(B) \\\\ H(C|A, B) &amp;\\leq H(C|B) \\end{align}\\] Remark (interpretation on submodularity). Entropy subadditivity is the foralization of the fact that adding information decreases entropy. Consider an example: \\(Z\\) is the result of a fair coin flip (\\(0\\) or \\(1\\)). \\(X\\) is the result of an independent fair coin flip. \\(Y=X\\oplus Z\\) denotes whether \\(Z=X\\) (\\(0\\) if this is true, else \\(1\\)). Then \\(H(X) = H(Y) = H(Z) = \\log 2\\); also \\(H(X, Z)=H(Y, Z)=H(X, Y, Z)=2\\log 2\\). Then \\[\\begin{aligned} H(Z|X, Y) &amp;= H(X, Y, Z) - H(X, Y) = 2\\log 2 - 2\\log 2 = 0 \\\\ H(Z|Y) &amp;= H(Y, Z) - H(Y) = 2\\log 2 - \\log 2 = \\log 2 = H(Z) \\end{aligned}\\] The first equation makes precise that “\\(X, Y\\) tells us everything we need to know about \\(Z\\)”, while the last two that “\\(X\\) alone tells us nothing about \\(Z\\).” The mathematical implication of this statement is that \\[ H(Z|Y) &gt; H(Z|X, Y) \\iff H(Y, Z) - H(Y) \\geq H(X, Y, Z) - H(X, Y) \\] which is exactly the strong subadditivity of entropy: adding information about \\(X\\) increases \\(H(X)\\mapsto H(X, Y)\\) more than it increases \\(H(X, Z)\\mapsto H(X, Y, Z)\\). Theorem 1.4 (Han's inequality) Let \\(X^n\\) be a discrete \\(n\\)-dimensional RV. Let \\[ \\bar H_k(X^n) = \\binom{n}{k}^{-1} \\sum_{T\\in \\binom{[n]}{k}} H(X_T) \\] denote the average entropy of a \\(k\\)-subset of coordinates. Then \\(\\bar H_k/k\\) is decreasing in \\(k\\): \\[ \\dfrac 1 n \\bar H_n \\leq \\cdots \\leq \\dfrac 1 k \\bar H_k \\leq \\cdots \\leq \\bar H_1 \\] Furthermore, \\(\\bar H_k\\) is monotonically increasing but concave \\[ \\bar H_{k+1} - \\bar H_k \\leq \\bar H_k - \\bar H_{k-1} \\] Proof: Let \\(\\bar H_0=0\\). The second claim implies the first since each term in the first equation is an average of the (diminishing) terms in the second: \\[ \\dfrac 1 m \\bar H_m = \\dfrac 1 m \\sum_{k=1}^m (\\bar H_k - \\bar H_{k-1}) \\] To prove the second claim, use submodularity \\[ H(X_1, \\cdots, X_{k+1}) - H(X_1, \\cdots, X_{k}) \\leq H(X_2, \\cdots, X_{k+1}) - H(X_2, \\cdots, H_{k}) \\] Average this over all \\(n!\\) permutations of \\([n]\\) to get \\[ \\bar H_{k+1} - \\bar H_k \\leq \\bar H_k - \\bar H_{k-1} \\] Equivalently, use the fact that conditioning decreases entropy, average this expression over \\(n!\\) permutations \\[ H(X_{k+1}|X_1, \\cdots, X_k) \\leq H(X_{k+1}|X_2, \\cdots, X_k) \\] Corollary 1.1 (joint entropy pairwise bound) \\(H(X) \\leq \\dfrac 1 {n-1} \\sum_{i&lt;j} H(X_i, X_j)\\) Corollary 1.2 (a cute geometric result) Consider placing \\(N\\) points in \\(\\mathbb R^3\\) arbitrarily. Let \\(N_1, N_2, N_3\\) denote the number of distinct points projected onto the \\(xy, xz, yz\\)-planes, then \\(N_1N_2N_3\\geq N^2\\). Proof: Let \\(\\mathcal C\\subset (\\mathbb R^3)^N\\) denote the set of coordinates of the \\(N\\) points, and let \\((A, B, C)\\sim \\mathcal C\\) denote the three components of the points drawn from \\(\\mathcal C\\), then using Han’s inequality: \\[ \\log N = H(A, B, C) \\leq \\dfrac 1 2(\\log N_1 + \\log N_2 + \\log N_3) \\] Remark. The core to the proof above is that combinatorics (size-counting) related to components are subject to the submodularity of entropy. Theorem 1.5 (Shearer's lemma) Let \\(X^n\\) be discrete \\(n\\)-dimensional RV and \\(S\\subset [n]\\) be a RV independent of \\(X^n\\), then \\[ H(X_S|S) \\geq H(X^n) \\min_{i\\in [n]} \\mathrm{Pr}(i\\in S) \\] This holds for any submodular set-function \\(H\\) with \\(H(\\emptyset) = 0\\). Proof: Consider the equivalent statement by “unrolling the probability”: if \\(\\mathcal C=(S_1, \\cdots, S_M)\\) is a list of subsets of \\([n]\\) then \\[ \\sum_j H(S_j) \\geq H(X^n) \\min_j \\deg(j), \\quad \\deg(j) = |\\{j: i\\in S_j\\}| \\] Call \\(\\mathcal C\\) a chain if all subsets are such that \\(S_1\\subset S_2\\cdots \\subset S_M\\). For a chain, the proposition is trivial since \\(\\min_j \\deg(j)\\) is zero of \\(S_M\\neq [n]\\) or equals the multiplicity of \\(S_M\\) in \\(\\mathcal C\\), which equals \\(\\min_j \\deg(j)\\), in which case \\[ \\sum_j H(X_{S_j}) \\geq H(X_{S_M})\\min_j \\deg(j) \\] When \\(\\mathcal C\\) is not a chain, consider a pair of sets \\(S_1, S_2\\) incomparable by inclusion and replace with \\(S_1\\cap S_2, S_1\\cup S_2\\). Submodularity implies that \\(\\sum_j H(X_{S_j})\\) does not increase under this transform, and \\(\\deg j\\) are not changed. Repeat this until we have a chain. Main idea of the proof: (1) expand “probability” to empirical lists. (2) Reason about an easy case (chain), then reduce to easy case by submodularity. "],["entropy-method-in-combinatorics-and-geometry.html", "2 Entropy method in combinatorics and geometry Binary vectors of average weights Counting subgraphs", " 2 Entropy method in combinatorics and geometry To bound the cardinality of a set \\(\\mathcal C\\), consider drawing an element uniformly at random from \\(\\mathcal C\\), whose entropy is \\(\\log |\\mathcal C|\\). To bound \\(|\\mathcal C|\\) from above, we can describe this random object by a random vector \\(X=(X_j)_{1\\leq j\\leq n}\\) and compute or upper-bound the joint-entropy by one of the following methods: Marginal bound: \\(H(X) \\leq \\sum H(X_j)\\). Pairwise bound: \\(H(X) \\leq \\dfrac 1 {n-1}\\sum_{i&lt;j} H(X_i, X_j)\\) Exact calculation: \\(H(X) = \\sum_j H(X_j|X_1\\leq k &lt;j)\\) Binary vectors of average weights Let \\(\\mathcal C\\) denote a set of \\(n\\)-bitstrings. If the average number of \\(1\\)’s in \\(\\mathcal C\\) is close to \\(0\\) or \\(1\\), then \\(\\mathcal C\\) cannot contain too many elements. Lemma 2.1 Let \\(\\mathcal C\\subset \\{0, 1\\}^n\\) and \\(p\\) be the average fraction of \\(1\\)’s in \\(\\mathcal C\\), then \\[ |\\mathcal C|\\leq e^{nh(p)}, \\quad h(p) = -p\\log p - (1-p)\\log(1-p) \\] Proof: Let \\((X_j)_{1\\leq j\\leq n}\\) be drawn uniformly random from \\(\\mathcal C\\), then \\[ \\log |C| = H(X_1, \\cdots, X_n) \\leq \\sum_j H(X_j) = \\sum h(p_j) \\] Note that \\((X_j)\\) are not independent. Uniform distribution over \\(\\mathcal C\\) does not imply independence of the individual components. Now, by concavity \\[ \\sum h(p_j) \\leq n h\\left(\\dfrac 1 n \\sum p_j\\right) = nh(p) \\] The volume of a Hamming ball of radius \\(k\\) centered at a binary vector \\(x\\) is the number of binary vectors which lie within that ball. Since the space \\(\\{0, 1\\}^n\\) is symmetric, the volume of the Hamming ball of radius \\(k\\) is the same for any center \\(x\\) and bounded by the following theorem Theorem 2.1 (binomial bound) \\[ \\sum_{j=0}^k \\binom{n}{j} \\leq e^{nh(k/n)}, \\quad k\\leq n/2 \\] Proof: Let \\(w_H(x)\\) denote the Hamming weight and take \\(\\mathcal C = \\{x|w_H(X)\\leq k\\}\\), then the previous lemma gives, for \\(x\\mapsto h(x)\\) increasing for \\(x\\leq 1/2\\) \\[ |\\mathcal C| = \\sum_{j=0}^k \\binom{n}{j} \\leq \\sum_{j=0}^k e^{n h(j/n)} \\leq e^{nh(k/n)}, \\quad k\\leq n/2 \\] Counting subgraphs For graphs \\(H, G\\), let \\(N(H, G)\\) be the number of subgraphs (subset of edges) of \\(G\\) which are isomorphic to \\(H\\). Let \\(G\\) have \\(m\\) edges, consider the maximal number of \\(H\\) which are contained in \\(G\\). Define \\[ N(H, m) = \\max_{G:|E(G)|\\leq m|} N(H, G) \\] This is the maximum number of \\(H\\) which can be contained in a graph with \\(m\\) edges. Proposition 2.1 (triangle in graphs) \\(N(K_3, m) \\asymp m^{3/2}\\). Proof: To show the lower-bound, \\(G=K_n\\) has \\(\\Theta(n^2)\\) edges and \\(\\Theta(n^3)\\) triangles. To show the upper bound, fix \\(G=(V, E)\\) with \\(m\\) edges and draw a labeled triangle at random with vertices \\((X_1, X_2, X_3)\\), then by Shearer’s lemma \\[ \\log(3!N(K_3, G)) = H(X_1, X_2, X_3) \\leq \\dfrac 1 2 [H(X_1, X_2)+H(X_1, X_3)+H(X_2, X_3)] \\leq \\dfrac 3 2 \\log(2m) \\] Theorem 2.2 (linear programming dual) Given a linear program \\(\\max c^Tx: Ax\\leq b, x\\geq 0\\), its dual is \\[ \\min b^Ty, \\quad A^Ty\\geq c, y\\geq 0 \\] This can be viewed as looking for a vector of coefficients \\(y\\) to combine the constraints such that the resulting RHS constraint \\(b^Ty\\) upper-bounds \\(c^Tx\\) (which happens when \\(A^Ty\\geq c\\)). Theorem 2.3 (Friedgut and Kahn) The fractional covering number of a graph \\(G=(V, E)\\) is the value of the linear program \\[ \\rho^*(H) = \\min_w \\left[ \\sum_{e\\in E}w(e): \\sum_{v\\in e\\in E} w(e)\\geq 1, \\forall v\\in V, w(e)\\in [0, 1] \\right] \\] Then \\(c_0(H) m^{\\rho^*(H)} \\leq N(H, m) \\leq c_1(H)m^{\\rho^*(H)}\\). Proof: Consider the upper bound first. Label \\(V(H)=[n]\\) and let \\(w^*(e)\\) be the solution for \\(\\rho^*(H)\\). For any \\(G\\) with \\(m\\) edges, draw a subgraph of \\(G\\) uniformly at random from all those isomorphic to \\(H\\). Let \\(X_i\\in V(G)\\) be the vertex corresponding to the \\(i\\)-th vertex of \\(H\\). Define a random \\(2\\)-subset of \\([n]\\) by sampling \\(e\\in E(H)\\) with probability \\(w^*(e)/\\rho^*(H)\\), then by definition of \\(\\rho^*(H)\\) \\[ \\mathrm{Pr}(i\\in S) \\geq \\dfrac 1 {\\rho^*(H)} \\] since the minimal probability of selecting any edge (and its associated vertices) is \\(1/\\max w(e) \\leq 1/\\sum w(e) = \\rho^*(H)\\). Then applying Shearer’s lemma 1.5 \\[ \\log N(H, G) = H(X) \\leq H(X_S|S)\\rho^*(H) \\leq \\log(2m)\\rho^*(H) \\implies N(H, G)\\leq (2m)^{\\rho^*(H)} \\] For the other bound, the idea is to explode the graph \\(H\\). Consider the dual LP corresponding to the fractional packing number \\[ \\alpha^*(H) = \\max_\\psi \\left[\\sum_{v\\in V(H)} \\psi(v): \\psi(v)\\in [0, 1] \\text{ and } \\forall (vw)\\in E: \\psi(v)+\\psi(w)\\leq 1 \\right] \\] Construct the graph \\(G\\) as follows: for each \\(v\\in H\\), replicate it \\(m(v)\\) times (here \\(m\\) stands for multiplicity, to be defined later). Each edge \\(e=(vw)\\) of \\(H\\) is then replaced by \\(K_{m(v), m(w)}\\) so that \\[ |E(G)| = \\sum_{(vw)\\in E(H)} m(v)m(w) \\] We also have \\(N(G, H)\\prod_{v\\in V(H)} m(v)\\). Fix a large number \\(M\\) and let \\(m(v) = \\lceil M^{\\psi(v)}\\rceil\\), then \\[\\begin{align} |E(G)| &amp;\\leq \\sum_{(vw)\\in E(H)} 4M^{\\psi(v)+\\psi(w)} \\leq 4M|E(H)| \\\\ N(G, H) &amp;\\geq \\prod_{v\\in V(H)} M^{\\psi(v)} = M^{\\alpha^*(H)} \\end{align}\\] "],["divergence.html", "3 Divergence KL-Divergence Differential entropy Channel, conditional divergence Chain Rule, DPI", " 3 Divergence Divergence is a fundamental object in information theory. Shannon-entropy is related to KL-divergence by equation 3.3. It sheds light on the following relations: Entropy subadditivity \\(H(A, B)\\leq H(A)+H(B)\\) corresponds to super-additivity of divergence when the sampling distribution is factorizable \\[ D(P_{AB} \\|Q_AQ_B) \\geq D(P_AP_B \\| Q_AQ_B) = D(P_A\\|Q_A) + D(P_B \\|Q_B) \\] with equality iff \\(P_{AB} = P_AP_B\\) (special case of full chain rule 3.5). Conditional factorization: the definition of \\(H(Y|X)\\) satisfying \\[ H(Y|X) = H(X, Y) - H(X) \\] corresponds to the conditional factorization of divergence in 3.2 \\[ D(P_{Y|X} \\| Q_{Y|X} \\, | \\, P_X) = D(P_{XY}|Q_{XY}) - D(P_Y \\| P_X) \\] Conditioning reduces entropy (recall equivalence to entropy strong subadditivity iff submodularity) \\[ H(A|B) \\leq H(A|B, C) \\] corresponds to conditioning increasing divergence \\[ D(P_Y \\| Q_Y) \\leq D(P_{Y|X} \\| Q_{Y|X}\\, |\\, P_X) \\] with equality iff \\(D(P_{X|Y} \\| Q_{X|Y} \\, | \\, P_Y) = 0\\). Some other ideas: \\(D(P\\|Q)\\) is the un-likelihood of sampling \\(P\\) from \\(Q\\). It’s insightful to view \\(P_{XY}\\) as generative processes \\(P_{Y|X}P_X\\) by disintegration. Markov-kernels are randomized functions. A standalone distribution is a randomized function from a singleton domain. Discrete Markov kernels are transition matrices and functions. Composition \\(K_{A|B}\\circ K_{B|C} = K_{A|C}\\) corresponds to the matrix product (einsum) \\(K^{A|B}_{ab} K^{B|C}_{bc} \\mapsto K^{A|C}_{ac}\\). Multiplication \\(K_{A|B} K_{B|1} = K_{AB|1}\\) corresponds to the einsum \\(K^{A|B}_{ab} K^{B|C}_{bc} \\mapsto K^{AB|C}_{abc}\\). In the function picture, composition is function composition \\(f, g\\mapsto (x\\mapsto f(g(x))\\), while multiplication corresponds to a more complicated operation. The singular most important tool for divergence is chain rule 3.2. To produce conditional-quantity bounds, consider the joint quantity and decompose in \\(2\\) different ways; this underpins the proof of KL-DPI and monotonicity of divergence under conditioning. The large deviations estimate 3.3 demonstrates how KL gives a uniform (across all events) KL-Divergence Definition 3.1 (KL-Divergence) Given distributions \\(P, Q\\) on \\(\\mathcal A\\) with \\(Q\\) being the reference measure, the (Kullback-Leibler) divergence (relative entropy) between \\(P, Q\\) is \\[ D(P\\|Q) = \\begin{cases} \\mathbb E_Q \\left[d_QP \\log d_QP\\right] &amp; P \\ll Q \\\\ +\\infty &amp; \\text{otherwise} \\end{cases} \\] By expanding the expectation, the first quantity is seen to be equivalent to \\[ \\mathbb E_Q \\left[d_QP \\log d_QP\\right] = \\mathbb E_P\\left[ \\log d_QP \\right] \\] Here \\(d_QP\\) is the Radon-Nikodym derivative, which in the case of standard alphabet is \\(P(X)/Q(X)\\). The relation \\(P\\ll Q\\) is read as “\\(P\\) is absolutely continuous w.r.t. \\(Q\\)”. Theorem 3.1 (information inequality) For all \\(P\\ll Q\\), \\(D(P\\|Q) \\geq 0\\), with equality iff \\(P=Q\\). Proof: Applying Jenson’s inequality to the convex function \\(\\varphi(x)=x\\log x\\): \\[\\begin{align} D(P\\|Q) = \\mathbb E_Q[\\varphi(d_QP)] \\geq \\varphi \\mathbb E_Q[d_QP] = \\varphi(1) = 0 \\end{align}\\] The following corollary shows that minimizing divergence recovers the true distribution. Corollary 3.1 (minimal entropy recovers true distribution) Given any discrete R.V. \\(X\\) such that \\(H(X)&lt;\\infty\\). Then \\[ \\min_Q \\mathbb E_{X\\sim P_X} \\left[\\log \\dfrac 1 {Q(X)} \\right] = H(X) \\] where \\(Q\\) is over valid probability distributions. The unique minimizer is \\(Q=P_X\\). Proof: Using the previous theorem: \\[\\begin{align} \\mathbb E_Q \\left[\\log \\dfrac 1 {Q(X)} - H(X)\\right] &amp;= \\mathbb E_{P_X} \\left[\\log \\dfrac{P_X(X)} {Q(X)}\\right] = D(P_X\\|Q) \\end{align}\\] Remark (perspective on KL-divergence). One should think of \\(D(P\\|Q)\\) as the un-likelihood of producing the “candidate” distribution \\(P\\) by samping from \\(Q\\). Definition 3.2 (binary divergence) Binary divergence is defined by \\[ d(p, q) = D(\\mathrm{Ber}_p\\| \\mathrm{Ber}_q) = p \\log \\dfrac p q + \\bar p \\log \\dfrac{\\bar p}{\\bar q} \\] Differential entropy The differential entropy generalizes entropy to non-probability measures; it does not have many of the desirable properties of divergence, however (in particular, lack of invariance under bijective transform). Definition 3.3 (differential entropy) The differential entropy of a random vector \\(X\\) is \\[ h(X) = -D(P_X\\|\\mathrm{Leb}) \\] where \\(\\mathrm{Leb}\\) is the Lebesgue measure (just think of it as the constant \\(1\\) everywhere). In particular, if \\(X\\) has probability \\(P_X\\), then \\[ h(X) = \\mathbb E_{P_X}\\left[\\log P_X(x)\\right] \\] Theorem 3.2 (properties of differential entropy) Uniform distribution maximizes \\(h\\): given \\(\\mathrm{Pr}(X\\in S\\subset \\mathbb R^n)=1\\), then \\(h(X^n \\leq \\log \\mathrm{Leb}(S)\\) with equality given by uniform. Linear transform: \\(h(AX+c) = h(X) + \\log|\\det A|\\) for invertible \\(A\\). Conditioning reduces entropy: \\(h(X|Y) \\leq h(X)\\). Proposition 3.1 (differential entropy of Gaussian) Recall the multivariate Gaussian pdf: \\[ f(x) = \\dfrac{1}{\\sqrt{(2\\pi )^d |\\Sigma|}} \\exp \\left[-\\dfrac 1 2 (x-\\mu)^T \\Sigma^{-1}(x-\\mu)\\right] \\] the differential entropy of a multivariate Gaussian \\(X=\\mathcal N(\\mu, \\Sigma)\\) is \\[ h(X) = \\dfrac 1 2 \\log[(2\\pi e)^d |\\Sigma|] \\] Proof: Direct computation: logarithm of the constant term yields \\(\\dfrac 1 2 \\log[(2\\pi )^d |\\Sigma|]\\), while the exponential term yields \\[\\begin{align} \\mathbb E\\left[ (x-\\mu)^T \\Sigma^{-1}(x-\\mu) \\right] &amp;= \\mathbb E\\mathrm{tr}\\left[ (x-\\mu)(x-\\mu)^T \\Sigma^{-1} \\right]\\\\ &amp;= \\mathrm{tr}\\left( \\mathbb E[(x-\\mu)(x-\\mu)^T]\\Sigma^{-1} \\right) = \\mathrm{tr}(I) = d \\end{align}\\] Recall that for semidefinite matrices, the positive semidefinite order is \\[ A\\prec B \\iff B - A\\text{ semi-definite.} \\] Theorem 3.3 (entropy extremality under covariance constraint) For any \\(d\\times d\\) covariance \\(\\Sigma\\), differential entropy is saturated by multivariate Gaussian \\[ \\max_{\\mathrm{Cov}(X) \\preceq \\Sigma} h(X) = h(\\mathcal N(0, \\Sigma)) = \\dfrac 1 2 \\log[(2\\pi e)^d |\\Sigma|] \\] Expected power constraint is saturated by independent Gaussian \\[ \\max_{\\mathbb E[\\|X\\|^2] \\leq a} h(X) = h\\left(\\mathcal N\\left(0, \\dfrac a d I_d\\right)\\right) = \\dfrac d 2 \\log \\dfrac{2\\pi e a}{d} \\] Proof: let \\(\\mathbb E[X]=0\\) w.l.g and \\(X_G = \\mathcal N(0, \\Sigma)\\) with pdf \\(P_G\\); apply information inequality \\[\\begin{align} 0 &amp;\\leq D(P_X \\| X_G) = \\mathbb E_P \\left[\\log \\dfrac{P_X(x)}{P_G(x)}\\right] = \\mathbb E_P \\left[\\log P_X(x)\\right] - \\mathbb E_P \\left[\\log P_G(x)\\right] \\\\ &amp;= -h(X) - \\dfrac 1 2 \\log[(2\\pi)^d |\\Sigma|] + \\dfrac{\\log e} e \\mathbb E_P[X^T\\Sigma^{-1}X] \\leq -h(X) + h(X_G) \\end{align}\\] On the last step, we apply the usual trick \\(\\mathbb E_P[X^T\\Sigma^{-1}X] = \\mathrm{tr}\\, \\mathbb E[\\Sigma_X \\Sigma^{-1}] \\leq \\mathrm{tr}(I) = d\\). Corollary 3.2 \\(\\Sigma \\mapsto \\log \\det \\Sigma = \\mathrm{tr}\\log \\Sigma\\) is concave on the space of real positive definite \\(n\\times n\\) matrices. Proof: Let \\(\\lambda\\sim \\mathrm{Ber}(1/2)\\) and \\(X = \\lambda \\mathcal N(0, \\Sigma_1) + \\bar \\lambda \\mathcal N(0, \\Sigma_2)\\), convexity follows from \\(h(X) \\leq (X|\\lambda)\\). Channel, conditional divergence Information theorists see Markov kernels everywhere! Definition 3.4 (Markov kernel (channel)) A Markov kernel is a function \\(K(-|-)\\), whose first argument is a measurable subset of \\(\\mathcal Y\\) and the second an element of \\(\\mathcal X\\) such that \\(E\\to K(E|x)\\) is a probability measure for every \\(x\\in \\mathcal X\\). \\(x\\to K(E|x)\\) is measurable. A markov kernel is the concept of a randomized function, where an input \\(x\\) results in a random measure (distribution) on \\(\\mathcal Y\\). Common operations include: Joint multiplication: Maps \\(P_{Y|X} P_X\\mapsto P_{X, Y}\\). Composition: a probability distribution \\(P_X\\) on \\(\\mathcal X\\) and \\(K:\\mathcal X\\to \\mathcal Y\\), then one can consider joint distribution \\(P_X\\times K\\) on \\(\\mathcal X\\times \\mathcal Y\\) where \\[ P_{X, Y}(x, y) = P_X(x) K(\\{y\\}|X) \\] This corresponds to matrix multiplication in the transition-matrix picture and function composition in the function picture. It is also the partial trace of joint-multiplication. (Tensor / Cartesian) product: \\(P_X, P_Y\\mapsto P_X \\times P_Y\\). We also overload this with joint multiplication: for example: \\(P_{Y|X}P_{Z|X}P_X\\) should be understood as \\((P_{Y|X}\\times P_{Z|X})P_X\\). Disintegration (standard Borel spaces): every \\(P_{X, Y}\\) on \\(\\mathcal X\\times \\mathcal Y\\) can be decomposed into \\[ P_{X, Y} = P_X\\times P_{Y|X} \\] Definition 3.5 (Binary symmetric channel) The channel \\(\\mathrm{BSC}_\\delta\\{0, 1\\}\\to \\{0, 1\\}\\) is defined as \\[ Y = (X + Z)\\mod 2, \\quad Z\\sim \\mathrm{Ber}(\\delta) \\] This has the transition matrix in basis \\(\\{0, 1\\}\\): \\[ \\begin{pmatrix} 1 - \\delta &amp; \\delta \\\\ \\delta &amp; 1 - \\delta \\end{pmatrix} \\] Matrix multiplication shows that \\(\\mathrm{BSC}_\\delta \\circ \\mathrm{BSC}_\\delta = \\mathrm{BSC}_{2\\delta \\bar \\delta} = \\mathrm{BSC}_{\\delta \\ast \\bar \\delta}\\). We will next see that the joint divergence can be written as the sum of marginal and conditional divergences; the latter is defined below: Definition 3.6 (Conditional divergence) Given distribution \\(P_X\\) and two markov kernels \\(P_{Y|X}, Q_{Y|X}\\), the divergence between \\(P, Q\\) given \\(P_X\\) is defined as \\[ D(P_{Y|X}\\|Q_{Y|X}\\,|\\, P_X) \\equiv \\mathbb E_{x_0\\sim P_X} \\left[ D(P_{Y|X=x_0} \\| Q_{Y|X=x_0}) \\right] = \\mathbb E_{X, Y\\sim P_{Y|X}P_X} \\left[\\log \\dfrac{P_{Y|X}}{Q_{Y|X}}\\right] \\] Chain Rule, DPI Proposition 3.2 (chain rule) The joint divergence is (1) the sum of marginal plus conditional divergences, and (2) the sum of marginal divergence and joint divergence upon replacing the source of \\(Q_{Y|X}\\) with \\(P_X\\). \\[ D(P_{X, Y}\\|Q_{X, Y}) = D(P_X, Q_X) + D(P_{Y|X}\\|Q_{Y|X} \\, |P_X) = D(P_X, Q_X) + D(P_{XY} \\| Q_{Y|X}P_X) \\] Proof: Expand the definition and factor joint into conditionals \\[\\begin{align} D(P_{X, Y}\\|Q_{X, Y}) &amp;= \\mathbb E_P\\left[\\log \\dfrac{P_{XY}}{Q_{XY}}\\right] = \\mathbb E_P\\left[\\log \\dfrac{P_{Y|X}P_X}{Q_{Y|X}Q_X}\\right] \\\\ &amp;= \\mathbb E_P \\left[\\log \\dfrac{P_{Y|X}}{Q_{Y|X}}\\right] + \\mathbb E_P \\left[\\log \\dfrac{P_X}{Q_X}\\right] \\\\ &amp;= D(P_{Y|X}\\|Q_{Y|X}\\, | P_X) + D(P_X\\|Q_X) \\end{align}\\] The second equality follows from the first equality (see property 1 below). Remark (care in factoring Q). In the expression above, note that \\(Q_{XY}=Q_{Y|X}Q_X\\) should be factored according to the marginal of \\(Q\\), not \\(P\\). This is important in the proof of the Kolmogorov identities in theorem 4.2, where \\[ P_{XY}P_Z \\text{ factored against $XZ$} = P_{Y|X} \\] Almost every property of Shannon entropy has a counterpart in KL-divergence. The following relation provides some intuition as to why: Proposition 3.3 (entropy-divergence relation) Given a distribution \\(P\\) supported on a finite set \\(\\mathcal A\\) \\[ H(P) = \\log |\\mathcal A| - D(P\\|U_{\\mathcal A}) \\] where \\(U_{\\mathcal A}\\) is the uniform distribution of \\(\\mathcal A\\). Given \\(X, Y\\) on \\(\\mathcal A, \\mathcal B\\), the conditional entropy is written as \\[ H(Y|X) = \\log |\\mathcal B| - D(P_{Y|X} \\| U_{\\mathcal B} | P_X) \\] Proof The first claim follows by direct computation: let \\(n=|\\mathcal A|\\), then \\[ \\log |\\mathcal A| - D(P\\|U_{\\mathcal A}) = \\log n - \\mathbb E_{x\\sim P} \\log n P(x) =\\mathbb E_{x\\sim P} \\left[\\log n + \\log \\dfrac 1 {n P(x)}\\right] =H(X) \\] Let \\(m=|\\mathcal B|\\), the second claim follows from \\[\\begin{align} H(Y|X) &amp;= H(X, Y) - H(X)\\\\ &amp;= \\log (nm) - \\log n - \\left[ D(P_{XY} \\| U_{\\mathcal A\\times \\mathcal B}) - D(P_X \\| U_{\\mathcal A}) \\right]\\\\ &amp;= \\log |\\mathcal B| - D(P_{Y} \\| U_{\\mathcal B}\\, | U_{\\mathcal A}) \\end{align}\\] Theorem 3.4 (properties of divergence) Given standard Borel \\(\\mathcal X, \\mathcal Y\\), then Unconditional expression for divergence \\(D(P_{Y|X}\\|Q_{Y|X}\\, |\\, P_X) = D(P_{Y|X}P_X \\| Q_{Y|X} P_X)\\). Monotonicity: \\(D(P_{X, Y}\\|Q_{X, Y}) \\geq D(P_Y\\|Q_Y)\\) (follows from chain rule + information inequality). Full chain rule: \\(D(P_{X_1\\cdots X_n} \\| Q_{X_1\\cdots X_n}) = \\sum_{j=1}^n D( P_{X_j|X_1, \\cdots, X_{j-1}} \\| Q_{X_j|X_1, \\cdots, X_{j-1}} \\, | \\, P_{X_1, \\cdots, X_{j-1}} )\\). Tensorization : \\(D\\left(\\prod P_{X_j} \\| \\prod Q_{X_j} \\right) = \\sum D(P_{X_j} \\| Q_{X_j})\\) Proof: The unconditional expression follows from chain rule 3.2 \\[ D(P_{Y|X} P_X\\|Q_{Y|X} P_X) = D(P_{Y|X} P_X\\|Q_{Y|X} P_X) - D(P_X\\|P_X) = D(P_{Y|X} P_X\\|Q_{Y|X} P_X) \\] The full chain rule follows from inductive application of the chain rule. For tensorization, see proposition 3.5 below. Proposition 3.4 (conditioning increases divergence) Given \\(P_{Y|X}, Q_{Y|X}, P_X\\), we have \\[ D(P_{Y|X}\\circ P_X \\| Q_{Y|X} \\circ P_X) \\leq D(P_{Y|X} \\| Q_{Y|X} \\, | \\, P_X) = D(P_{Y|X} P_X \\| Q_{Y|X} P_X) \\] Equality is saturated iff \\(D(P_{X|Y} \\| Q_{X|Y} \\, |\\, P_Y) = 0\\). Proof: Written in this form, this is apparant since \\(A\\circ B\\) loses information from the joint \\(AB\\). To see this explicitly, let \\[ P_Y = P_{Y|X} \\circ P_X, \\quad Q_Y = Q_{Y|X} \\circ P_X, \\quad P_{XY} = P_XP_Y, \\quad Q_{XY} = Q_XQ_Y \\] Using the chain rule yields \\[\\begin{align} D(P_{XY} \\| Q_{XY}) &amp;= D(P_{X|Y} \\| Q_{X|Y} \\, | \\, P_Y) + D(P_Y \\| Q_Y) \\\\ &amp;= D(P_{Y|X} \\| Q_{Y|X} \\, | \\, P_X) + D(P_X \\| Q_X)_{=0} \\end{align}\\] Here \\(D(P_X \\| Q_X)=0\\); the equality condition can also be seen. Proposition 3.5 (chain rule: independent sampling distribution) Consider the chain rule with independent \\(Q\\)’s, then \\[ D(P_{X_1\\cdots X_n} \\|Q_{X_1}\\cdots Q_{X_n}) = D(P_{X_1\\cdots X_n} \\| P_{X_1}\\cdots P_{X_n}) + \\sum_{j=1}^n D(P_{X_j} \\|Q_{X_j}) \\geq \\sum_{j=1}^n D(P_{X_j} \\|Q_{X_j}) \\] with equality saturated iff \\(P\\) is factorizable. Proof: Use the second equality in the chain rule 3.2 inductively to swap out \\(Q_{X_j}\\). Theorem 3.5 (data-processing inequality (DPI)) Given a Markov kernel \\(K:\\mathcal X\\to \\mathcal Y\\) and a chain \\((P_X, Q_X) \\xrightarrow{K} (P_Y, Q_Y)\\) so that \\(P_Y = K_{Y|X} \\circ P_X, \\quad Q_Y = K_{Y|X} \\circ Q_X\\), then processing reduces the ability to distinguish between the distributions \\[ D(P_X\\|Q_X) \\geq D(P_Y\\|Q_Y) = D(K_{Y|X} \\circ P_X \\| K_{Y|X} \\circ Q_X) \\] Proof: Decompose the joint in \\(2\\) different ways: \\[\\begin{align} D(P_{XY}\\|Q_{XY}) &amp;= D(P_{X|Y} \\| Q_{X|Y} |P_Y)_{\\geq 0} + D(P_Y \\| Q_Y) \\\\ &amp;= D(P_{Y|X} \\| Q_{Y|X}|P_X)_{=0} + D(P_X\\|Q_X) \\end{align}\\] Using nonnegativity on the first line and equality of the channel on the second line yields that the processing inequality is saturated iff the reverse inference distribution is the same: \\[ D(P_Y\\|Q_Y) \\leq D(P_X\\|Q_X) \\text{ with equality } \\iff D(P_{X|Y} \\| Q_{X|Y} |P_Y)_{\\geq 0} \\] Corollary 3.3 (large deviations estimate) For any subset \\(E\\subset \\mathcal X\\) we have \\[ D(P_X \\| Q_X) \\geq d(P_X[E] \\| Q_X[E]) \\] Proof: Apply to the binary-output channel \\(1_E\\). "],["mutual-information.html", "4 Mutual Information Definition, properties Causal graphs Conditional MI Sufficient statistic Probability of error, Fano’s inequality Entropy-power Inequality", " 4 Mutual Information Key takeaways: The graph direction in Markov models are somewhat redundant: \\[ X\\to Y\\to Z \\iff X\\leftarrow Y\\to Z\\iff Z\\to Y\\to X \\] The key is \\(X\\perp\\!\\!\\!\\perp Z\\|Y\\). In light of how divergence-DPI implies mutual information-DPI (proposition 4.2), given a divergence \\(\\mathcal D\\) satisfying DPI, we can define a mutual-information like quantity \\[ I_{\\mathcal D}(X; Y) = \\mathbb E_{x\\sim P_X} \\left[\\mathcal D(P_{Y|X=x} \\| P_Y)\\right] = D(P_{Y|X} \\| P_Y | P_X) \\] The key factorization result for mutual information is 4.2. Mutual information is the weighted divergence from the marginal to the conditionals (definition 4.1). General proof technique (see 4.4); to derive bounds on general statements, derive one for a tractable statement then apply divergence inequalitites (DPI, Donsker-Varadhan etc). Definition, properties Definition 4.1 (Mutual information) The mutual information between \\(X, Y\\) is (Fano’s definition) \\[ I(X; Y) \\equiv D(P_{X, Y}\\|P_XP_Y) = D(P_{Y|X} \\| P_Y | P_X) \\] Shannon’s definition is not general enough for infinite cases, even when the alphabet is discrete. \\[ I(X; Y) = H(X) - H(X|Y) \\] The conditional mutual-information is \\[ I(X; Y|Z) = \\mathbb E_{z\\in P_Z} I(X; Y|Z=z) \\] Theorem 4.1 (properties of mutual information) Mutual information satisfies: Conditional expression: \\(I(X; Y) = D(P_{XY}\\|P_XP_Y) = D(P_{Y|X} \\| P_Y\\,|\\, P_X)\\). Symmetry: \\(I(X; Y) = I(Y; X)\\). Positivity: \\(I(X; Y)\\geq 0\\) with equality saturated iff \\(X\\perp\\!\\!\\!\\perp Y\\). Transformations destroy information: \\(I(f(X); Y) \\leq I(X; Y)\\) with equality iff \\(f\\) is injective. Data create information: \\(I(X, Y; Z) \\geq I(X; Z)\\) with equality iff \\(Y\\) is a deterministic transform of \\(X\\). Proof: The conditional expression follows from the decomposition in theorem 3.4. For symmetry, consider a swap-channel on the joint which maps \\(P_XP_Y\\mapsto P_YP_X\\), then applying the KL-DPI 3.5 to the joint \\(K\\circ P_{XY} = P_{YX}\\) and the marginal \\(K\\circ P_XP_Y = P_YP_X\\) yields \\[ D(P_{XY} \\| P_XP_Y) = D(P_{YX} \\| P_YP_X) \\iff I(X; Y) = I(Y; X) \\] the DPI inequality is made into an equality by a symmetry argument. Positivity is established by the information inequality 3.1. For claim (4), consider the kernel \\(K\\circ (X, Y) = (f(X), Y)\\) and apply KL-DPI \\[ D(P_{f(X)Y} \\| P_{f(X)}P_Y) \\leq D(P_{XY} \\| P_XP_Y) \\iff I(f(X); Y) \\leq I(X; Y) \\] For injective \\(f\\), apply the argument to the inverse. For the last claim, apply (4) to the projection transform \\((X, Y)\\mapsto X\\). Proposition 4.1 (Mutual information and entropy) \\(I(X; X) = \\begin{cases} H(X) &amp; X \\text{ discrete.} \\\\ \\infty \\text{ otherwise.}\\end{cases}\\) Given discrete \\(X\\), \\(H(X) = I(X; Y) + H(X|Y)\\). Given both \\(X, Y\\) discrete, \\(H(X)+H(Y) = I(X; Y) + H(X; Y)\\). Proof: (2) and (3) follow from direct computation. Given \\(X\\) discrete with alphabet-size \\(n\\), we have \\[ I(X; X) = D(P_{XX} \\| P_XP_X) = \\mathbb E_{XX\\sim P_{XX}} \\log \\dfrac{P_X(X)}{P_X(X)^2} = H(X) \\] note that \\(XX\\sim P_{XX}\\cong X\\sim P_X\\). For infinite-alphabet, consider a kernel \\(K_m\\) which projects \\(X\\) onto the univariate \\(\\mathrm{Unif}(0, 1)\\) distribution then takes the first \\(m\\) decimals, then \\[ I(X; X) &gt; I(K_m(X); X) = H(K_m(X)) - H(K_m(X)|X)_{=0} = m\\log 2 \\] this holds for all \\(m\\). Causal graphs Consider a Markov graph \\(X\\to Y\\to Z\\). This unrolls to \\[\\begin{align} X\\to Y\\to Z &amp;\\iff P_{X, Y, Z} = P_{Z|Y}P_{Y|X}P_X \\\\ &amp;\\iff P_{X, Z}|Y = P_{X|Y} P_{Z|Y} \\iff X\\perp\\!\\!\\!\\perp Z | Y \\\\ &amp;\\iff P_{X, Y, Z} = P_{X, Z|Y} P_Y = P_{X|Y} P_{Z|Y} P_Y \\\\ &amp;\\iff X\\leftarrow Y \\rightarrow Z \\\\ &amp;\\iff Z\\rightarrow Y\\rightarrow X \\end{align}\\] A variable \\(V\\) is a collider on some undirected path if \\[ \\cdots \\to V \\leftarrow \\cdots \\] Two subsets of vertices \\(A, B\\) are \\(d\\)-connected by a subset \\(C\\) if there exists an undirected path from \\(a\\in A\\to b\\in B\\) such that: There are no non-colliders in \\(C\\). Every collider is either in \\(C\\) or has a descendent in \\(C\\). \\(A\\perp\\!\\!\\!\\perp B|C\\) in every distribution satisfying the graphical model iff \\(A, B\\) are not \\(d\\)-connected by \\(C\\). Conditional MI Definition 4.2 (conditional mutual information) Given standard Borel \\(\\mathcal X, \\mathcal Y\\), define \\[ I(X; Y|Z) = D(P_{X, Y|Z} \\| P_{X|Z} P_{Y|Z} \\, | \\, P_Z) = \\mathbb E_{z\\sim P_Z}\\left[I(X; Y|Z=z)\\right] \\] Theorem 4.2 (more properties of mutual information) Given standard Borel R.Vs, then \\(I(X; Z|Y)\\geq 0\\) with equality iff \\(X\\perp\\!\\!\\!\\perp Z | Y\\). Chain rule (Kolmogorov identities): \\[ I(X, Y; Z) = I(X; Z) + I(Y; Z|X) = I(Y; Z) + I(X; Z|Y) \\] DPI for mutual information: given \\(X\\to Y\\to Z\\), then \\[ I(X; Z) \\leq I(X; Y) \\] with equality iff \\(X\\to Z\\to Y\\). Full chain rule: \\(I(X^n; Y) = \\sum_{k=1}^n I(X_k; Y|X^{k-1})\\). Bijective invariance: given bijective \\(f, g\\), then \\(I(fX; gY) = I(X; Y)\\). Proof: Non-negativity follows from the divergence definition and the information inequality: \\[ I(X; Z|Y) = D(P_{XZ}P_Y \\| P_XP_ZP_Y) \\geq 0 \\] For the chain rule, factor the LHS using proposition 3.2 against \\(X, Z\\) to obtain \\[\\begin{align} I(X, Y; Z) &amp;= D(P_{XYZ} \\| P_{XY}P_Z) = D(P_{XZ} \\| P_XP_Z) + D(P_{XYZ} \\| P_{Y|X} P_{XZ}) \\\\ &amp;= I(X; Z) + D(P_{XY|Z} P_Z \\| P_{Y|X} P_{Z|X} P_X) = I(X; Z) + I(X; Y |Z) \\end{align}\\] where factoring against \\(XZ\\) yields \\(P_{XY}P_Z = P_{Y|X}(P_XP_Z)\\) and \\(P_XP_Z\\) is replaced with the marginal \\(P_{XZ}\\) of \\(P_{XYZ}\\). The other identity follows from symmetry. For the DPI, apply the Kolmogorov identity to \\(I(Y, Z;X)\\) to obtain \\[\\begin{aligned} I(Y, Z ; X) &amp;= I(Y; X) + I(Z; X | Y)_{=0} \\\\ &amp;= I(Z; X) + I(Y; X | Z) \\end{aligned}\\] equality is fulfilled iff \\(X\\to Z\\to Y\\). Bijective invariance follows from applying DPI to \\(f\\) then \\(f^{-1}\\): consider the equally valid chains under bijection: \\[ Y\\to X\\to fX \\implies I(fX; Y) \\leq I(X; Y), \\quad Y\\to fX\\to X \\implies I(X; Y) \\leq I(fX; Y) \\] Remark (mutual information chain rule). Mutual information between \\(X; Y\\) is a resource which can be consumed when we condition on an additional variable \\(Z\\). Additionally, one should remember the Kolmogorov identities as a decomposition of \\(I(X, Y; Z)\\) into an individual component \\(I(X; Z)\\) plus the second term \\(I(Y; Z|X)\\), which is \\(I(Y; Z)\\) adjusted for over-counting. Corollary 4.1 If \\(X\\to Y\\to Z\\to W\\), then \\(I(X; W) \\leq I(Y; Z)\\). Proof: Several applications of chain rule: \\(I(Y; Z) \\geq I(X; Z) \\geq I(X; W)\\). Remark (incomparable mutual information under conditioning). Under a Markov chain \\(X\\to Y\\to Z\\), mutual information decreases upon conditioning. To find an example such that \\(I(X; Y|Z) &gt; I(X; Y)\\), the only non-isomorphic graph is \\[ X\\to Y\\leftarrow Z \\] Let \\(X, Z\\sim \\mathrm{Ber}(1/2)\\) i.i.d and \\(Y=X\\oplus Z\\), then \\(X\\perp\\!\\!\\!\\perp Y \\implies I(X; Y)=0\\) and \\[ I(X; Y|Z) = H(X, Y|Z) - H(X|Y, Z) = \\log 2 \\] Proposition 4.2 (alternate proof of mutual information DPI) Mutual information DPI is implied by the divergence-DPI theorem 3.5. Given a Markov chain \\(X\\to Y\\to Z\\), we have \\[ I(X; Z) = D(P_{XZ} \\| P_XP_Z) = D(P_{Z|X} \\| P_Z | P_X) \\leq D(P_{Y|X} \\| P_Y | P_X) = I(X; Y) \\] Proof: The markov kernel in question is simply \\(P_{Z|Y}\\) applied to \\((P_{Y|X=x}, P_Y) \\mapsto (P_{Z|X=x}, P_Z)\\). Sufficient statistic Consider the following definitions: \\(P^\\theta_X\\) is a class of distributions for \\(X\\) parameterized by \\(\\theta\\in \\Theta\\). Given kernel \\(P_{T|X}\\), let \\(P_T^\\theta = P_{T|X} \\circ P^\\theta_X\\) be the induced distribution on \\(T\\). Definition 4.3 (sufficient statistic) \\(T\\) is a sufficient statistic of \\(X\\) for \\(\\theta\\) if there exists a kernel \\(P_{X|T}\\) such that \\(P_{T|X} P_X^\\theta = P_{X|T} P^\\theta_T\\). In other words, \\(P_{X|T}\\) can be chosen not to depend on \\(\\theta\\). Theorem 4.3 (characterization of sufficiency) The following claims are equivalent: \\(T\\) is a sufficient statistic of \\(X\\) for \\(\\theta\\). \\(\\forall P_\\theta, \\theta \\to T\\to X \\iff \\forall P_\\theta, I(\\theta; X|T) = 0\\). To finish. Probability of error, Fano’s inequality Given a R.V. \\(W\\) and our prediction \\(\\hat W\\), we can Guess randomly: \\(W\\perp\\!\\!\\!\\perp\\hat W\\). Guess with data: \\(W\\to X\\to \\hat W\\), where \\(X=f(W)\\) is a deterministic function of \\(W\\). \\(W\\to X\\to Y\\to \\hat W\\), where \\(X\\to Y\\) is some noisy channel. The following statement upper bounds the discrete entropy based on the probability of correctness for random guessing and the maximum probability. We would guess that: For the maximum probability of the distribution, the bound should be largest when \\(P_0 = 1/M\\) and decay as \\(P_0\\) increases. The maximum collision probability is also \\(1/M\\) across all distributions. The second statement in the following theorem can be viewed as a bound on entropy using the \\(\\infty\\)-Renyi entropy. Theorem 4.4 (entropy bound using independent guess) Given a finite alphabet \\(|\\mathcal X|=M&lt;\\infty\\), for any \\(\\hat X\\perp\\!\\!\\!\\perp X\\) \\[ H(X) \\leq F_M(\\mathrm{Pr}[X=\\hat X]), \\quad F_M(x) = (1-x)\\log(M-1)+h(x) \\] and \\(h(x)=-x\\log x - \\bar x \\log \\bar x\\) is the binary entropy. Let \\(P_0 = \\max_{x\\in \\mathcal X} P_X(x)\\), then \\[ H(X) \\leq F_M(P_0) \\] with equality iff \\(P_X=(P_0, \\alpha, \\cdots)\\) where \\(\\alpha=(1-P_0)/(M-1)\\). Note that \\(F_M(0)=\\log(M-1)\\), \\(F_M(1/M)=\\log M\\), and \\(F_M(1)=0\\). Proof technique: instead of studying \\(P_{X\\hat X}\\) for arbitrary \\(\\hat X\\), introduce a tractable \\(Q_{X\\hat X}\\), to bound an event, then use DPI (or divergence inequalities) to bound the same event for \\(P_{X\\hat X}\\) together with some divergence quantity. Proof For the first inequality, consider \\(Q_{X\\hat X} = U_XP_{\\hat X}\\) with \\(U_X\\) uniform, then \\(Q[X=\\hat X]=1/M\\); let \\(P[X=\\hat X]=p\\), the applying divergence DPI to \\((X, \\hat X)\\mapsto 1[X=\\hat X]\\) yields \\[\\begin{align} D(P_{X\\hat X} \\| Q_{X\\hat X}) &amp;= \\mathbb E_{X, \\hat X\\sim P} \\log M \\dfrac{P(x)^2}{P(x)} = \\log M - H(P_X) \\\\ &amp;\\geq d(p\\|1/M) = p \\log pM + \\bar p \\log \\dfrac{M \\bar p}{M-1} \\\\ &amp;= -h(p) + p\\log M + (1-p) \\log M - \\bar p\\log(M-1) \\\\ &amp;= -h(p) + \\log M + \\bar p \\log(M-1) \\\\ H(P_X)&amp;\\leq h(p) + \\bar p \\log(M-1) \\end{align}\\] To prove the second claim, choose \\(\\hat X\\) to be the mode of \\(X\\). Theorem 4.5 (Fano's inequality) Let \\(|\\mathcal X|=M\\) and \\(X\\to Y\\to \\hat X\\). Let \\(p_e = P[X\\neq \\hat X]\\), then \\[ H(X|Y) \\leq F_M(1-p_e) = p_e\\log(M-1) + h(p_e) \\] Furthermore, for \\(p_0 = \\max P_X(x)\\), then regardless of \\(M\\) we have \\[ I(X; Y) \\geq (1 - p_e) \\log \\dfrac 1 {p_0} - h(p_e) \\] Proof: Apply the same proof as the previous theorem with \\(U_X\\) uniform. Entropy-power Inequality To finish. "],["variational-measures-of-information.html", "5 Variational Measures of Information Geometric interpretations of MI Lower variational bounds Continuity", " 5 Variational Measures of Information Key takeaways: Variational characterizations are important because they provide convenient bounds when we choose an easily-computable candidate. For each convex quantity there is a dual variational characterization per the Legendre transformation. To analyze mutual information with a mixture distribution: add a latent variable as an upper bound, then use the Kolmogorov identities to analyze the gap / decompose in another way. Mutual information is “average divergence” achieved using center of gravity 5.1. Geometric interpretations of MI Theorem 5.1 (golden formula) For any \\(Q_Y\\) we have \\[ D(P_{Y|X} \\| Q_Y|P_X) = I(X; Y) + D(P_Y\\|Q_Y) \\] In particular, if \\(P_Y \\ll Q_Y\\), then \\[ I(X; Y) = D(P_{Y|X} \\| Q_Y|P_X) - D(P_Y\\|Q_Y) \\] Proof The two sides expand to \\[\\begin{align} D(P_{Y|X} \\| Q_Y|P_X) &amp;= D(P_{XY} \\| P_XQ_Y) \\\\ I(X; Y) + D(P_Y \\|Q_Y) &amp;= D(P_{XY} \\| P_XP_Y) + D(P_Y \\|Q_Y) \\end{align}\\] and are equal by the divergence chain rule 3.2. Corollary 5.1 (center of gravity formula) \\(I(X; Y) = \\min_{Q_Y} D(P_{Y|X} \\| Q_Y|P_X)\\), the unique minimizer is \\(Q_Y=P_Y\\). Proof: Follows from the golden formula and information inequality. To properly interpret this corollary as the center of gravity: consider the simplex of conditionals \\(\\{P_{Y|X=x}\\}\\). Given a candidate marginal \\(Q_Y\\), we can compute its divergence from each conditional, weighted by their incidence probability \\[ D(P_{Y|X} \\| Q_Y|P_X) = \\mathbb E_{x\\sim P_X} \\left[D(P_{Y|X=x} \\| Q_Y)\\right] \\] The unique minimizer of this is \\(P_Y\\), with minimized value \\(I(X; Y)\\). One can also see that mutual information is small if masses are concentrated: this coincides with \\(I(X; Y) = H(Y) - H(Y|X)\\). Theorem 5.2 (minimum distance from independence) \\(I(X; Y) = \\min_{Q_X, Q_Y} D(P_{XY} \\|Q_XQ_Y)\\). The unique minimizer is \\(Q_X, Q_Y = P_X, P_Y\\). Proof Decompose the divergence on the RHS \\[\\begin{align} D(P_{XY} \\|Q_XQ_Y) &amp;= D(P_{XY} \\| Q_X P_Y) + D(P_Y \\|Q_Y) \\\\ &amp;= D(P_{XY} \\| P_X P_Y) + D(P_Y \\|Q_Y) + D(P_X \\| Q_X) \\\\ &amp;= I(X; Y) + D(P_Y \\|Q_Y) + D(P_X \\| Q_X) \\end{align}\\] Theorem 5.3 (minimum distance from markov chain) Minimizing over all \\(Q_{XYZ} = Q_XQ_{Y|X}Q_{Z|Y}\\), \\[ I(X; Z|Y) = \\min_{Q_{XYZ}:X\\to Y\\to Z} D(P_{XYZ} \\| Q_{XYZ}) \\] Lower variational bounds Theorem 5.4 (lower variational bound of MI) For any Markov kernel \\(Q_{X|Y}\\) such that \\(Q_{X|Y=y} \\ll P_X\\) for \\(P_Y\\) almost everywhere we have \\[ I(X; Y) \\geq \\mathbb E_{P_{X, Y}} \\left[\\log \\dfrac{d Q_{X|Y}}{dP_X}\\right] \\] if \\(I(X; Y) &lt; \\infty\\), then \\[ I(X; Y) = \\sup_{Q_{X|Y}} \\mathbb E_{P_{XY}} \\left[ \\log \\dfrac{dQ_{X|Y}}{dP_X} \\right] \\] equality is saturated when \\(Q_{X|Y} = P_{X|Y}\\). Proof Without dealing with limit arguments, \\[\\begin{align} \\mathbb E_{P_{X|Y=y}} \\left[ \\log \\dfrac{Q_{X|Y}(x|y)}{P_X(x)} \\right] &amp;= \\mathbb E_{P_{X|Y=y}} \\left[ \\log \\left( \\dfrac{Q_{X|Y}(x|y)}{P_{X|Y}(x|y)} \\dfrac{P_{X|Y}(x|y)}{P_X(x)} \\right) \\right] \\\\ &amp;= \\mathbb E_{P_{X|Y=y}} \\left[ -D(P_{X|Y=y} \\| Q_{X|Y=y}) + D(P_{X|Y=y} \\| P_X) \\right] \\end{align}\\] Take expectation over \\(y\\) to obtain \\[\\begin{align} \\mathbb E_{P_{XY}} \\left[ \\log \\dfrac{dQ_{X|Y}}{dP_X} \\right] &amp;= D(P_{X|Y=y} \\| P_X | P_Y) - D(P_{X|Y} \\| Q_{X|Y} | P_Y) \\\\ &amp;= I(X; Y) - D(P_{X|Y} \\| Q_{X|Y} | P_Y) \\end{align}\\] Theorem 5.5 (Donsker-Varadhan) Given probability measures \\(P, Q\\) over \\(\\mathcal X\\), denote a class of functions \\[\\begin{align} \\mathcal C_Q &amp;= \\{ f:\\mathcal X\\to \\mathbb R\\cup \\{-\\infty\\} | 0 &lt; \\mathbb E_Q[e^{f(X)}] &lt; \\infty \\} \\\\ D(P \\| Q) &amp;= \\sup_{f\\in \\mathcal C_Q} \\mathbb E_P f(X) - \\log \\mathbb E_Q\\left[e^{f(X)}\\right] \\end{align}\\] If \\(\\mathcal X\\) is a metric space with the Borel \\(\\sigma\\)-algebra, then the supremum can be taken over the class of all bounded continuous functions. Proof Again, for clarity we’re ignoring measurability (or infinity) subtleties. The key idea is, given \\(f\\in \\mathcal C_Q\\), to define a tilted distribution \\(Q^f\\) such that \\[ Q^f(dx) = e^{f(dx) - \\psi_f} Q(dx), \\quad \\psi_f = \\log \\mathbb E_Q\\left(e^{f(X)}\\right) \\] Here \\(\\psi_f\\) is just a \\(f\\)-dependent normalization constant. Then \\[\\begin{align} \\mathbb E_P[f(X) - \\psi_f] &amp;= \\mathbb E_P[\\log e^{f(X) - \\psi_f}] = \\mathbb E_P\\left[\\log \\dfrac{dQ^f}{dQ}\\right]\\\\ &amp;= \\mathbb E_P\\left[\\log \\dfrac{dP}{dQ}- \\log \\dfrac{dP}{dQ^f}\\right] = D(P\\|Q) - D(P\\|Q^f) \\leq D(P\\|Q) \\end{align}\\] Equality is saturated when \\(Q^f = P \\iff f = \\log \\dfrac{dP}{dQ}\\). Corollary 5.2 The supremum in Donsker-Varadhan is equivalent to, \\(\\forall f\\in \\mathcal C_Q\\) \\[ \\mathbb E_P f(X) \\leq D(P\\|Q) + \\psi_f, \\quad \\psi_f = \\log \\mathbb E_Q\\left(e^{f(X)}\\right) \\] This allows us to upper-bound \\(\\mathbb E_P f(X)\\) for difficult \\(P\\) by substituting with an easier \\(Q\\). Recall the Legendre transform: given a convex function \\(f(x)\\), its Legendre transform is \\[ g(p) = \\sup_x\\left[\\langle x, p\\rangle- f(x)\\right] \\] Here, \\(g(p)\\) is the function \\(P\\mapsto D(P\\|Q)\\), \\(x\\) is a well-behaved function \\(f\\), and \\(f(x)\\) corresponds to the functional \\(f\\mapsto \\psi_f\\). This is a functional Legendre transform. The following result will then not be too surprising. Proposition 5.1 (Gibbs variational principle) Given measurable \\(\\mathcal X:\\mathbb R\\cup \\{-\\infty\\}\\) and a probability measure \\(Q\\) on \\(\\mathcal X\\) \\[ \\log \\mathbb E_Q\\left[e^{f(X)}\\right] = \\sup_P \\mathbb E_P [f(X)] - D(P\\|Q) \\] If the LHS is finite, then the unique maximizer in the RHS is \\(P=Q^f\\). Continuity Proposition 5.2 (Single-argument continuity of divergence for finite alphabets) Fixing finite \\(\\mathcal X\\) with a strictly positive distribution \\(Q\\) over \\(\\mathcal X\\), then \\(P\\mapsto D(P\\|Q)\\) is continuous; in particular, \\(P\\mapsto H(P)\\) is continuous. For finite alphabets, divergence is never continuous in the pair: consider \\[ \\lim_{n\\to \\infty} d(1/n \\| 2^{-n}) = \\dfrac 1 n \\log \\left(\\dfrac{2^n} n\\right) + \\dfrac{n-1} n \\log \\left(\\dfrac{1-1/n}{1-2^{-n}}\\right) = \\log 2 \\neq 0 \\] while the arguments converge to \\(0\\): information can be destroyed at the limit. Remark. \\(D(P\\|Q)\\) is not continuous in either \\(P\\) or \\(Q\\) for general alphabets: consider \\(X_j\\sim 2\\mathrm{Ber}_{1/2}-1\\), then \\(\\bar X_j\\to \\mathcal N(0, 1)\\) but \\(D(\\bar X_j \\|\\mathcal N(0, 1))=\\infty\\) because \\(\\bar X_j\\) is discrete for all \\(n\\). Theorem 5.6 (Lower semicontinuity of divergence) Given a metric space \\(\\mathcal X\\) with Borel \\(\\sigma\\)-algebra \\(\\mathcal H\\). If \\(P_n, Q_n\\) converge weakly to \\(P, Q\\), respectively, then \\[ D(P\\|Q) \\leq \\liminf_{n\\to \\infty} D(P_n\\|Q_n) \\] Recall that weak convergence is pointwise convergence in distribution. Proof This follows from Donsker-Varadhan by \\(\\mathbb E_{P_n}[f]\\to \\mathbb E_P[f]\\) and \\(\\mathbb E_{Q_n}[e^f]\\to \\mathbb E_Q[e^f]\\). To reason about \\(\\liminf\\): choose a subsequence such that \\(D(P_n\\|Q_n)\\) decreases monotonically, then \\(\\liminf\\mapsto \\lim\\); equality cannot be established due to the the previous example: a more fundamental reason is because weak convergence guarantees the convergence of expectations for each individual function but not convergence for the supremum. Convergence in the weaker metric topology only implies weak semi-continuity in the stronger Jensen metric. Proposition 5.3 (continuity of MI) Given finite alphabets \\(\\mathcal X, \\mathcal Y\\), \\(P_{XY}\\mapsto I(X; Y)\\) is continuous. If \\(\\mathcal X\\) is finite, then \\(P_X\\mapsto I(X; Y)\\) is continuous. For general \\(\\mathcal X, \\mathcal Y\\), let \\(P_X\\in \\Pi = \\mathrm{co}(P_1, \\cdots, P_n)\\) be in the convex hull of \\((P_j\\); if \\(I(P_j; P_{Y|X}&lt;\\infty\\) for each \\(P_j\\), then the map \\(P_X\\mapsto I(X; Y)\\) is continuous. Proof For the first statement, \\(I(X; Y)=H(X)+H(Y) - H(X, Y)\\) and entropy is continuous on finite alphabets. For the second statement, define the uniform mixture of conditionals \\(Q_Y = |\\mathcal X|^{-1}\\sum P_{Y|X=x}\\), then \\[ D(P_Y \\|Q_Y) = \\mathbb E_{Q_Y} \\left[\\varphi \\left(\\sum P_X(x)h_x(Y)\\right)\\right], \\quad \\varphi(t) = t\\log t \\] Here \\(h_x(y) = \\dfrac{dP_{Y|X=x}}{dQ_Y}(y)\\) is nonnegative and bounded by \\(|\\mathcal X|\\), so using the bounded convergence theorem we have $P_X\\(D(P_Y\\|Q_Y)\\) continuous. By the golden formula, \\[ I(X; Y) = D(P_{Y|X} \\|Q_Y|P_X) - D(P_Y\\|Q_Y) \\] the first term is linear in \\(P_X\\). For the third claim, form a chain \\(Z\\to X\\to Y\\) with \\(Z\\in [n]\\) mapping \\(P_{X|Z=j}=P_j\\). Then \\[ I(X; Y) = I(Z; Y) + I(X; Y|Z) \\] the first term is continuous in \\(P_Z\\) while the second is linear in \\(P_Z\\). Thus \\(P_Z\\mapsto I(X; Y)\\) is continuous, and so is \\(P_X\\mapsto I(X; Y)\\). The bounded convergence theorem applies in proof of (b) because we have pointwise convergence of \\(\\sum_{x} P_X(x)h_x(y)\\) as \\(P_X\\) approaches its limit, and \\(\\varphi\\) is continuous and bounded, so this allows us to pass the limit through composition with \\(\\varphi\\). "],["extremization.html", "6 Extremization Convexity Minimax and saddle-point Capacity, Saddle point of MI Capacity as information radius Gaussian saddle point", " 6 Extremization Key perspectives: Capacity: Given channel \\(P_{Y|X}\\), maximize \\(I(X; Y)\\) over a convex set of inputs \\(P_X\\). Rate-distortion: Given \\(P_X\\), minimize \\(I(X; Y)\\) over a convex set of \\(P_{Y|X}\\). Maximum likelihood: Given \\(P\\), minimize \\(D(P\\|Q)\\) over a class of \\(Q\\). Information projection: Given \\(Q\\), minimize \\(D(P\\|Q)\\) over a convex class of \\(P\\). The first two minimax objectives correspond to the concavity of \\(I(P_X; P_{Y|X})\\) in the first argument and convexity in the second argument. The last two objectives correspond to the convexity of \\(D(P\\|Q)\\) in both arguments. Key mathematical ideas: Convexity of \\(D(P\\|Q)\\) is equivalent to “conditioning increases divergence” (proposition 3.4) \\(\\iff\\) “mixing decreases divergence.” For convexity proofs, introduce a Bernoulli latent variable, then use chain rule to decompose the quantity (MI, KL, entropy, etc) into the two compared components. The saddle point of mutual information yields a game-theoretic perspective to the duality between achieving capacity and rate distortion: corollary 6.2. The capacity of a channel is the radius of posteriors under divergence. Convexity Theorem 6.1 (KL convexity) The map \\((P, Q)\\mapsto D(P\\|Q)\\) is convex: \\[ \\forall \\lambda \\in [0, 1]: \\lambda D(P_0 \\| Q_0) + \\bar \\lambda D(P_1\\|Q_1) \\geq D(\\lambda P_0 + \\bar \\lambda P_1 \\| \\lambda Q_0 + \\bar \\lambda Q_1) \\] Proof: The first quantity can be seen as an expected divergence over \\(R\\sim \\mathrm{Ber}(\\lambda)\\): \\[ \\lambda D(P_0 \\| Q_0) + \\bar \\lambda D(P_1\\|Q_1) = D(P\\|Q | R), \\quad (P, Q)_{R=j} = (P_j, Q_j) \\] The second quantity is simply the divergence of the marginal, then by chain rule we have \\[\\begin{align} D(P\\|Q | R) = D(PR \\|QR) \\geq D(P\\|Q) \\end{align}\\] Theorem 6.2 (concavity of entropy) \\(P_X\\mapsto H(X)\\) is concave. Fixing channel \\(P_{Y|X}\\), \\(P_X\\mapsto H(X|Y)\\) is concave and continuous if \\(\\mathcal X\\) finite. Proof: The first proof is complete by the KL-entropy relation \\(H(X) = \\log |\\mathcal X| - D(P_X\\|U_X)\\). The second proof follows by a similarl latent variable argument: consider \\(U\\sim \\mathrm{Ber}(\\lambda)\\) and \\(U\\to X\\to Y\\); let \\(f(P_X) = H(X|Y)\\), then \\[\\begin{align} f(\\lambda P_0 + \\bar \\lambda P_1) = H(X|Y), \\quad \\lambda f(P_0) + \\bar \\lambda f(P_1) = H(X|Y, U) \\end{align}\\] concavity follows from \\(H(X|Y) \\leq H(X|Y, U)\\). Continuity follows \\(H(Y|X) = H(Y) - I(X; Y)\\) both continuous. Theorem 6.3 (extremality of MI) Fixing channel \\(P_{Y|X}\\), \\(P_X\\mapsto I(P_X, P_{Y|X})\\) is concave. Fixing input distribution \\(P_X\\), \\(P_{Y|X} \\mapsto I(P_X, P_{Y|X})\\) is convex. Proof Consider 3 proofs for the first statement: \\[ \\lambda I(P^0_X; P_{Y|X} \\circ P^0_X) + \\bar \\lambda I(P^1_X; P_{Y|X} \\circ P^1_X) \\geq I(P_X; P_{Y|X}\\circ P_X), \\quad P_X = \\lambda P^0_X + \\bar \\lambda P^1_X \\] Standard latent variable proof: Consider the standard latent variable proof \\(Z\\to X\\to Y\\) with \\(Z\\sim \\mathrm{Ber}(\\lambda)\\perp\\!\\!\\!\\perp Y\\), then the RHS is \\(I(X; Y)\\) while the LHS is \\[ I(Y; X, Z) = I(Y; X | Z) + I(Y; Z)_{=0} \\] the result follows from the fact that mutual information increases with more deta. Center of gravity formula: Use corollary 5.1: \\(I(X; Y) = \\min_{Q_Y} D(P_{Y|X} \\|Q_Y | P_X)\\); this is the pointwise minimum of affine functions in \\(P_X\\) hence concave. Golden formula: Since \\(I(X; Y) = D(P_{Y|X} \\|Q_Y|P_X) - D(P_Y\\|Q_Y)\\), the map \\(P_X\\mapsto D(P_{Y|X} \\circ P_X \\|Q_Y)\\) is convex and the first term is affine, so the combination is concave. To prove the second argument, note that \\(I(X; Y) = D(P_{Y|X} \\| P_Y|P_X)\\); here \\(D(P_{Y|X=x} \\| P_Y)\\) is jointly convex, and \\(P_Y\\) is linear function of \\(P_{Y|X}\\). Minimax and saddle-point Proposition 6.1 (minimax inequality) \\[ \\inf_y \\sup_x f(x, y) \\geq \\sup_x \\inf_y f(x, y) \\] Whichever operation acts first strictly dominates. Proof: Fixing \\(y=y_0\\) on the LHS, \\(\\sup_x f(x, y_0) \\geq \\sup_x \\inf_y f(x, y)\\) by \\(f(x, y_0)\\geq \\inf_y f(x, y)\\). Minimax equality is implied by the existence of a saddle point \\((x^*, y^*)\\) such that \\[ f(x, y^*) \\leq f(x^*, y^*) \\leq f(x^*, y), \\quad \\forall x, y \\] here \\(x^*\\) is the dominant strategy given \\(y^*\\), and \\(y^*\\) is the dominant strategy given \\(x^*\\). If \\(\\inf, \\sup\\mapsto \\min, \\max\\), then equality implies the existence of a saddle point. von Neumann: Given \\(A, B\\) with finite alphabets and \\(g(A, B)\\) arbitrary, then \\[ \\min_{P_A} \\max_{P_B} \\mathbb E[g(A, B)] = \\max_{P_B} \\min_{P_A} \\mathbb E[g(A, B)] \\] this is a special case of minimax with \\(f(x, y) = \\sum_{ab} P_A(a)P_B(b) g(a, b)\\). Theorem 6.4 (minimax theorem) If \\(\\mathcal X, \\mathcal Y\\) are compact domains in \\(\\mathbb R^n\\), and \\(f(x, y)\\) is continuous in \\((x, y)\\), concave in \\(x\\) and convex in \\(y\\), then \\[ \\max_x \\min_y f(x, y) = \\min_y \\max_x f(x, y) \\] In particular, this implies the existence of a saddle point. Capacity, Saddle point of MI Definition 6.1 (capacity, caid, caod) The capacity of a channel \\(P_{Y|X}\\) over a set \\(\\mathcal P\\) of (usually convex) input distributions is \\[ C = \\sup_{P_X\\in \\mathcal P} I(P_X, P_{Y|X}) = \\sup_{P_X} D(P_{Y|X}P_X \\| (P_{Y|X}\\circ P_X)P_X) \\] If equality is saturated by \\(P_X^*\\in\\mathcal P\\), then \\(P_X^*\\) is a capacity-achieving input distribution (caid) and \\(P_Y^* = P_{Y|X}\\circ P_X^*\\) is the capacity-achieving output distribution (caod). Theorem 6.5 (implications of MI saddle point) Fixing a convex \\(\\mathcal P\\) on \\(\\mathcal X\\). The existence of a caid implies, for every \\(P_X\\in \\mathcal P, P_Y\\in P_{Y|X} \\circ \\mathcal P\\), \\[ D(P_{Y|X} \\| P_Y^* |P_X) \\leq D(P_{Y|X} \\| P_Y^* | P_X^*) \\leq D(P_{Y|X} \\| Q_Y | P_X^*) \\] Proof: The second inequality is simply center of gravity formula 5.1 applied to the middle quantity \\(C=I(X^*, Y^*) = D(P_{Y|X} \\| P_Y^* | P_X^*)\\). For the first inequality, let \\(C&lt;\\infty\\) and let \\[ P_{X_\\lambda} = \\lambda P_X + \\bar \\lambda P_X^*, \\quad P_{Y_\\lambda} = P_{Y|X} \\circ P_{X_\\lambda} \\implies P_{Y_\\lambda} = \\lambda P_Y + \\bar \\lambda P_Y^* \\] The following chain yields (the third line applies the center of gravity characterization again) \\[\\begin{align} C &amp;\\geq I(X_\\lambda; Y_\\lambda) = D(P_{Y|X} \\| P_{Y_\\lambda} | P_{X_\\lambda}) \\\\ &amp;= \\lambda D(P_{Y|X} \\| P_{Y_\\lambda} | P_X) + \\bar \\lambda D(P_{Y|X} \\| P_{Y_\\lambda} | P_{X^*}) \\\\ &amp;\\geq \\lambda D(P_{Y|X} \\| P_{Y_\\lambda} | P_X) + \\bar \\lambda C \\\\ C &amp;\\geq D(P_{Y|X} \\| P_{Y_\\lambda} | P_X) = D(P_{Y|X} P_X \\| P_{Y_\\lambda} P_X) \\end{align}\\] Apply lower semi-continuity 5.6 by taking \\(\\liminf_{\\lambda\\to 0}\\) to obtain the desired quantity \\[ \\liminf_{\\lambda\\to 0} P_{Y_\\lambda} P_X = P_{Y^*}P_X \\implies D(P_{Y|X} \\| P_{Y^*} | P_X) \\leq \\liminf_{\\lambda\\to 0} D(P_{Y|X} P_X \\| P_{Y_\\lambda} P_X) \\leq C \\] Corollary 6.1 (uniqueness of caod) In addition to the assumptions in theorem 6.5, if capacity is finite, then the caod \\(P_Y^*\\) is unique and satisfies \\[ D(P_{Y|X} \\circ P_X \\| P_Y^*) \\leq C &lt; \\infty, \\quad \\forall P_X\\in \\mathcal P \\] In particular, KL-finite implies \\(P_Y \\ll P_Y^*\\). Proof: Recognize the capacity as a decomposed component in the divergence chain rule 3.2: \\[\\begin{align} C = D(P_{X^*Y^*} \\| P_{Y^*}P_{X^*}) &amp;= D(P_{X^*Y^*} \\| P_{Y^*} P_X) - D(P_{Y^*} \\| P_Y) \\\\ &amp;= D(P_{X|Y} \\| P_X | P_{Y^*}) - D(P_{Y^*} \\| P_Y) \\\\ &amp;\\geq D(P_{X|Y} \\| P_{X^*} | P_{Y^*})_{=C} - D(P_{Y^*} \\| P_Y) \\end{align}\\] Saturated equality implies \\(D(P_{Y^*} \\| P_Y)=0\\iff P_Y=P_{Y^*}\\). Note that the caid need not be unique. Corollary 6.2 (minimax MI) Under saddle point assumptions, we additionally have \\[\\begin{align} C = \\max_{P_X\\in \\mathcal P} I(X; Y) &amp;= \\max_{P_X\\in \\mathcal P} \\min_{Q_Y} D(P_{Y|X} \\| Q_Y | P_X) \\\\ &amp;= \\min_{Q_Y} \\sup_{P_X\\in \\mathcal P} D(P_{Y|X} \\| Q_Y|P_X) \\end{align}\\] Proof: The first \\(\\max\\min\\) equation comes from the center of gravity characterization 5.1. The second equation comes from applying center of gravity to the left inequality of theorem 6.5 \\[ C = D(P_{Y|X} \\| P_Y^* | P_X^*) = \\max_{P_X\\in \\mathcal P} D(P_{Y|X} \\| P_Y^* | P_X) = \\min_{Q_Y} \\sup_{P_X\\in \\mathcal P} D(P_{Y|X} \\| P_Y^* | P_X) \\] Capacity as information radius Some review: given metric space \\((X, d)\\) and bounded set \\(A\\). Definition 6.2 (radius, diameter) The (Chebyshev) radius is the radius of the smallest ball that covers \\(A\\). \\[ \\mathrm{rad}(A) = \\inf_{y\\in X}\\sup_{x\\in A} d(x, y) \\] The diameter of \\(A\\) is the least upper bound on two distances in the set \\[ \\mathrm{diam}(A) = \\sup_{x, y\\in A} d(x, y) \\] Compare the radius to the second equation in corollary 6.2: channel capacity is the information radius of the conditionals. Corollary 6.3 Given finite \\(\\mathcal X\\) and channel \\(P_{Y|X}\\), the maximal mutual information over all input distributions satisfies \\[ \\max_{P_X} I(P_X; P_{Y|X}) = \\max_{x\\in \\mathcal X} D(P_{Y|x=x} \\| P_{Y^*}) \\] Proof: This follows from the left equality \\(C=\\max_{P_X\\in \\mathcal P} D(P_{Y|X} \\| P_Y^* | P_X)\\): for finite alphabets, concentrate weight in the single \\(x\\in \\mathcal X\\) which maximizes \\(D(P_{Y|X=x} \\| P_Y^*)\\). Gaussian saddle point Theorem 6.6 (extremality of Gaussian channels) Let \\(X_g\\sim \\mathcal N(0, \\sigma_X^2), N_g\\sim \\mathcal N(0, \\sigma_N^2)\\perp\\!\\!\\!\\perp X_g\\), then Gaussian capacity: \\[ C = I(X_g; X_g + N_g) = \\dfrac 1 2 \\log \\left( 1 + \\dfrac{\\sigma_X^2}{\\sigma_N^2} \\right) \\] Gaussian input is the caid for Gaussian noise: under the power constraint \\(\\mathrm{Var}(X) \\leq \\sigma_X^2\\) and \\(X\\perp\\!\\!\\!\\perp N_g\\) \\[ I(X; X+N_g) \\leq I(X_g; X_g+N_g) \\] with equality saturated iff \\(X=X_g\\) (in distribution). Gaussian noise is the worst for Gaussian input: under the power constraint \\(\\mathbb E[N^2] \\leq \\sigma_N^2\\) and \\(\\mathbb E[X_gN] = 0\\) \\[ I(X_g; X_g+N) \\geq I(X_g; X_g+N_g) \\] with equality iff \\(N=N_g\\) (in distribution and \\(N\\perp\\!\\!\\!\\perp X_g\\). Proof: Placeholder. "],["tensorization.html", "7 Tensorization", " 7 Tensorization Placeholder. "],["f-divergence.html", "8 f-Divergence Definition Information properties, MI TV and Hellinger, hypothesis testing Joint range Rényi divergence Variational characterizations Empirical distribution and χ² Fisher information, location family Local χ² behavior", " 8 f-Divergence Properties of \\(f\\)-divergences: \\(f\\)-divergence is equivalent modulo \\(c(t-1)\\); convexity plus \\(f(1)=f&#39;(1)=1\\) (which implies \\(f\\geq 0\\)) implies locally \\(\\chi^2\\). Convexity of \\(D_f\\) 8.3 \\(\\iff\\) DPI 8.2 \\(\\iff\\) monotonicity 8.1 \\(\\iff\\) convexity of \\(f\\). There is no chain rule for \\(f\\)-divergences other than KL-divergence. Rényi divergence enjoys tensorization and chain rule (up to tilting); it includes KL, \\(\\chi^2\\) and Hellinger as special cases. TV as binary hypothesis testing: to reason about TV, break spaces into \\(dP&gt;dQ\\) and \\(dP&lt;dQ\\). Several divergences are metrics (JS, \\(H^2\\)); one fast way to compare is to compare to metric, use triangle inequality, then use comparison inequality again. TV does not enjoy tensorization properties; Hellinger divergence is both a metric and tensorizes well. A powerful approximation theorem 8.4 reduces arbitrary spaces to finite ones. Similarly, the Harremoës-Vajda theorem 8.8 concludes the problem of joint range. Locality and Fisher information: Most \\(f\\)-divergences (with bounded \\(f&#39;&#39;\\)) are locally \\(\\chi^2\\): theorem 8.12 with respect to mixture interpolations. Fisher information is only defined with respect to a family of distributions parameterized by \\(\\theta\\). It is the expectation of the Hessian of the log-pdf about \\(\\theta\\): equation (8.5). For regular families, \\(\\chi^2\\) (mixture interpolations, in particular) is locally determined by the Fisher information 8.13. Fisher information matrix is non-negative, monotonic, has its own chain rule 8.10 and variational characterization. Definition Definition 8.1 (f-divergence) Given a convex function \\(f:(0, \\infty)\\to \\mathbb R\\) with \\(f(1)=0\\), for every two probability distributions over \\(\\mathcal X\\), if \\(P\\ll Q\\) then the \\(f\\) divergence is \\[ D_f(P\\|Q) = \\mathbb E_Q \\left[ f\\left(\\dfrac{dP}{dQ}\\right) \\right] \\] Here \\(f(0) = f(0_+)\\) per limit. More generally, define \\(f&#39;(\\infty) = \\lim_{x\\to 0^+} xf(1/x)\\), we have \\[ D_f(P\\|Q) = \\int_{q&gt;0} q(x) f\\left[ \\dfrac{p(x)}{q(x)} \\right] \\, d\\mu + f&#39;(\\infty) P[Q=0] \\] this last generalization is needed to account for divergences like total variation. Intuitively, sums for terms with \\(dQ=0\\) are like \\[ \\int_{dQ=0} dQ f\\left(\\dfrac{dP}{dQ}\\right) = \\lim_{t=dP/dQ\\to 0^+} \\int \\dfrac{dP}{t} f(1/t) \\] Examples of \\(f\\)-divergences: KL-divergence: \\(f(x)=x\\log x\\) to recover KL-divergence. Total variation (TV): \\(f(x) = \\dfrac 1 2 |x - 1|\\): \\[ \\mathrm{TV}(P, Q) = \\dfrac 1 2 \\mathbb E_Q \\left|\\dfrac{dP}{dQ} - 1\\right| = \\dfrac 1 2 \\int |dP - dQ| = 1 - \\int d(P\\wedge Q) \\] Recall that \\(P\\wedge Q\\) is the pointwise minimum measure so \\(\\int d(P\\wedge Q)\\) is the overlap measure. \\(\\chi^2\\)-divergence: \\(f(x)=(x-1)^2\\). \\[ \\chi^2(P\\|Q) = \\mathbb E_Q \\left(\\dfrac{dP}{dQ} - 1\\right)^2 = \\int \\dfrac{(dP - dQ)^2}{dQ} = \\int \\dfrac{dP^2}{dQ} + dQ - 2dP = \\int \\dfrac{dP^2}{dQ} - 1 \\] Squared Hellinger distance: \\(f(x) = \\left(1 - \\sqrt x\\right)^2\\). \\[ H^2(P, Q) = \\int \\left(\\sqrt{dP} - \\sqrt{dQ}\\right)^2 = 2 - 2\\int \\sqrt{dPdQ} \\tag{8.1} \\] The quantity \\(B(P, Q) = \\int \\sqrt{dPdQ}\\) is the Bhattacharyya coefficient (Hellinger affinity). Note that \\(H(P, Q) = \\sqrt{H^2(P, Q)}\\). The Hellinger distance is \\(H(P, Q) = \\sqrt{H^2(P, Q)}\\). Le Cam divergence: \\(f(x) = \\dfrac{1-x}{2x+2}\\), \\[ \\mathrm{LC}(P, Q) = \\dfrac 1 2 \\int \\dfrac{(dP - dQ)^2}{dP + dQ} \\] The square root \\(\\sqrt{\\mathrm{LC}(P, Q)}\\), the Le Cam distance, is a metric. Jensen-Shannon divergence take \\(f(x) = x\\log \\dfrac{2x}{x+1} + \\log \\dfrac 2 {x+1}\\). \\[ \\mathrm{JS}(P, Q) = D\\left(P \\| \\dfrac{P+Q}{2}\\right) + D\\left(Q \\| \\dfrac{P+Q}{2}\\right) \\] Proposition 8.1 \\(\\mathrm{TV}(P, Q) = \\dfrac 1 2 \\int |dP - dQ| = 1 - \\int d(P\\wedge Q)\\). Proof Proof: Let \\(E = \\{x:dP &gt; dQ\\}\\), then \\[\\begin{align} \\int |dP - dQ| &amp;= \\int_E dP - d(P\\wedge Q) + \\int_{E^c} dQ - d(P\\wedge Q) = \\int_E dP + \\int_{E^c} dQ - \\int d(P\\wedge Q) \\\\ &amp;= \\int_E dP + \\int_{E^c} d(P\\wedge Q)_{=dP} + \\int_{E^c} dQ + \\int_E d(P\\wedge Q)_{=dQ} - 2\\int d(P\\wedge Q) \\\\ &amp;= 2 - 2\\int d(P\\wedge Q) \\end{align}\\] Proposition 8.2 The following quantities derived from divergences are metrics on the space of probability distributions \\[ \\mathrm{TV}(P, Q), \\quad H(P, Q), \\quad \\sqrt{\\mathrm{JS}(P, Q)}, \\quad \\sqrt{\\mathrm{LC}(P, Q)} \\] Proposition 8.3 (closure properties) \\(D_f(Q \\| P) = D_g(P \\|Q)\\) for \\(g(x) = xf(1/x)\\). \\(D_f(P\\|Q)\\) is a \\(f\\)-divergence, then \\[ D_f(\\lambda P + \\bar \\lambda Q \\|Q), \\quad D_f(P \\| \\lambda P + \\bar \\lambda Q), \\quad \\forall \\lambda\\in [0, 1] \\] are \\(f\\)-divergences. Linearity: \\(D_{f+g} = D_f + D_g\\). Distinguishability: \\(D_f(P \\| P) = 0\\) Proof For the first claim, \\[ D_g(P \\|Q) = \\int dQ \\left(\\dfrac{dP}{dQ}\\right) f\\left(\\dfrac{dQ}{dP}\\right) = D_f(Q \\| P) \\] For the second claim, the other case can be obtained by using the equation above. \\[ D_f(\\lambda P + \\bar \\lambda Q \\|Q) = \\int dQ f\\left(\\lambda \\dfrac{dP}{dQ} + \\bar \\lambda\\right) \\implies \\tilde f(x) = f\\left(\\lambda x + \\bar \\lambda\\right) \\] Proposition 8.4 (equivalence properties) \\(D_f(P\\|Q) = 0\\) for all \\(P\\neq Q\\) iff \\(f(x)=c(x-1)\\) for some \\(c\\). \\(D_f = D_{f+c(x-1)}\\); thus we can always assume \\(f\\geq 0\\) and \\(f&#39;(1)=0\\). Proof Claim \\(1\\) proceeds by computation (assuming continuity) \\[ D_f(P \\|Q) = c \\int (dP / dQ - 1) dQ = 0 \\] This means that \\(c(x-1)\\) is in the kernel of the linear operator \\(f\\mapsto D_f\\). Pick \\(c=-f&#39;(1)\\), then \\(f(1)=f&#39;(1)=0\\); by convexity \\(f\\geq 0\\). Proposition 8.5 (special case of monotonicity) Joint divergence is unchanged through the same channel \\[ D_f(P_XP_{Y|X} \\| Q_XP_{Y|X}) = D_f(P_X \\| Q_X) \\] in particular, for the source-agnostic channel we have \\[ D_f(P_XP_Y \\| Q_XP_Y) = D_f(P_X \\| Q_X) \\] Proof Direct computation \\[\\begin{align} D_f(P_XP_{Y|X} \\| Q_XP_{Y|X}) &amp;= \\int Q_X(x) dx \\int P_{Y|X=x}(y)dy\\, f\\left[\\dfrac{P_X(x) P_{Y|X=x}(y)}{Q_X(x) Q_{Y|X=x}(y)}\\right]\\\\ &amp;= \\int Q_X(x) dx f\\left(\\dfrac{P_X(x)}{Q_X(x)}\\right) = D_f(P_X \\| Q_X) \\end{align}\\] Information properties, MI Theorem 8.1 (monotonicity) The joint is more distinguishable than the marginal: \\[ D_f(P_{XY} \\| Q_{XY}) \\geq D_f(P_X \\| Q_X) \\] inequality is saturated when \\(P_{Y|X} = Q_{Y|X}\\). Proof Assume \\(P_{XY} \\ll Q_{XY}\\), then expand and apply Jensen’s inequality \\[\\begin{align} D_f(P_{XY} \\| Q_{XY}) &amp;= \\mathbb E_{X\\sim Q_X} \\mathbb E_{Y\\sim Q_{Y|X}} f\\left( \\dfrac{dP_{Y|X} P_X}{dQ_{Y|X}Q_X} \\right) \\geq \\mathbb E_{X\\sim Q_X} f\\left(\\mathbb E_{Y\\sim Q_{Y|X}} \\dfrac{dP_{Y|X} P_X}{dQ_{Y|X}Q_X} \\right) \\\\ &amp;\\geq \\mathbb E_{X\\sim Q_X} f\\left( \\dfrac{dP_X}{dQ_X} \\right) = D_f(P_X \\| Q_X) \\end{align}\\] To be more careful, on the first line we have \\[\\begin{align} \\mathbb E_{Y\\sim Q_{Y|X=x}} \\dfrac{P_{Y|X=x}(y) P_X(x)}{Q_{Y|X=x}(y)Q_X(x)} &amp;= \\dfrac{P_X(x)}{Q_X(x)} \\sum_y Q_{Y|X=x}(y) \\dfrac{P_{Y|X=x}(y)}{Q_{Y|X=x}(y)} = \\dfrac{P_X(x)}{Q_X(x)} \\end{align}\\] Inequality is saturated when, for every \\(x\\), \\(P_{Y|X=x}=Q_{Y|X=x}\\). Definition 8.2 (conditional f-divergence) Given \\(P_{Y|X}, Q_{Y|X}\\) and \\(P_X\\), the conditional \\(f\\)-divergence is \\[ D_f(P_{Y|X} \\| Q_{Y|X} | P_X) = D_f(P_{Y|X} P_X \\| Q_{Y|X} P_X) = \\mathbb E_{x\\sim P_X}\\left[ D_f(P_{Y|X=x}\\| Q_{Y|X=x}) \\right] \\] Proof The second statement requires some justification: \\[\\begin{align} D_f(P_{Y|X} P_X \\| Q_{Y|X} P_X) &amp;= \\mathbb E_{x\\sim P_X} \\mathbb E_{y\\sim Q_{Y|X=x}}\\left[ f\\left(\\dfrac{P_{Y|X=x}(y)}{Q_{Y|X=x}(y)}\\right) \\right] \\end{align}\\] Theorem 8.2 (data-processing inequality) Given a channel \\(P_{Y|X}\\) with two inputs \\(P_X, Q_X\\) \\[ D_f(P_{Y|X} \\circ P_X \\| P_{Y|X} \\circ Q_X) \\leq D_f(P_X \\| Q_X) \\] Proof \\(D_f(P_X \\| Q_X) = D_f(P_{XY} \\| Q_{XY}) \\geq D_f(P_Y \\| Q_Y)\\), with two equalities given by proposition 8.5 and theorem 8.1. Inequality is saturated by the monotonicity condition \\(P_{X|Y} = Q_{X|Y}\\). Theorem 8.3 (information properties of f-divergences) Non-negativity: \\(D_f(P\\|Q) \\geq 0\\). If \\(f\\) is strictly convex at 1, i.e.  \\[ \\forall s, t\\in [0, \\infty), \\alpha \\in (0, 1) \\text{ with } \\alpha s + \\bar \\alpha t = 1: \\quad \\alpha f(s)+ \\bar \\alpha f(t) &gt; f(1) = 0 \\] then \\(D_f(P\\|Q) = 0 \\iff P=Q\\). Conditional \\(f\\)-divergence; conditioning increases divergence: \\(D_f(P_{Y|X} \\circ P_X \\| Q_{Y|X} \\circ P_X) \\leq D_f(P_{Y|X} \\| Q_{Y|X} | Q_X) = D_f(P_{Y|X} P_X \\| Q_{Y|X} P_X)\\). Joint-convexity: \\((P, Q)\\mapsto D_f(P\\|Q)\\) is jointly convex; consequently, \\(P\\mapsto D_f(P\\|Q)\\) and \\(Q\\mapsto D_f(P\\|Q)\\) are also convex. Proof For non-negativity, apply monotonicity to \\[ D_f(P_X \\| P_Y) = D_f(P_{X, 1} \\| P_{Y, 1}) \\geq D_f(1 \\| 1) = 0 \\] Assume \\(P\\neq Q\\) so there exists measurable \\(A\\) such that \\(P[A]=p \\neq Q[A] = Q\\), then apply the \\(\\chi_A\\) channel and apply DPI; both cases \\(q=1\\) and \\(q\\neq 1\\) contradict strict convexity. Claim (2) follows from monotonicity and recognize \\(P_{Y|x}\\circ P_X\\) as the marginal of \\(P_{Y|X}P_X\\). Joint convexity follows from standard latent variable argument: to prove joint convexity \\[ D(\\lambda P_0 + \\bar \\lambda P_1 \\| Q_0 + \\bar \\lambda Q_1) \\leq \\lambda D(P_0 \\| Q_0) + \\bar \\lambda D(P_1 \\| Q_1) \\] Take \\(\\theta \\sim \\mathrm{Ber}_\\lambda \\to (P, Q)\\), then the RHS is \\(D(P_{P|\\lambda} \\| P_{Q|\\lambda} | P_\\lambda)\\) while the LHS is \\(D(P_{P|\\lambda}\\circ P_\\lambda\\| P_{Q|\\lambda} \\circ P_\\lambda)\\) The following powerful theorem allows us to reduce any general problem to finite alphabets. Theorem 8.4 (finite approximation theorem) Given two probability measures \\(P, Q\\) on \\(\\mathcal X\\) with \\(\\sigma\\)-algebra \\(\\mathcal F\\). Given a finite \\(\\mathcal F\\)-measurable partition \\(\\mathcal E = \\{E_1, \\cdots, E_n\\}\\), define the distribution \\(P_{\\mathcal E}\\) on \\([n]\\) by \\(P_{\\mathcal E}(j) = P[E_j]\\), similarly for \\(Q\\), then \\[ D_f(P\\|Q) = \\sup_{\\mathcal E} D_f(P_{\\mathcal E} \\| Q_{\\mathcal E}) \\] where \\(\\sup\\) is over all finite \\(\\mathcal F\\)-measurable partitions. We omit the technical proof above. Definition 8.3 (f-information) The \\(f\\)-information is defined by \\[ I_f(X; Y) = D_f(P_{XY} | P_XP_Y) \\] Definition 8.4 (f-DPI) For \\(U\\to X\\to Y\\), we have \\(I_f(U; Y) \\leq I_f(U; X)\\). Proof: \\(I_f(U; X) = D_f(P_{UX} \\|P_UP_X) \\geq D_f(P_{UY} \\| P_UP_Y) = I_f(U; Y)\\). TV and Hellinger, hypothesis testing In a binary hypothesis testing problem, one is given an observation \\(X\\), which is known to be \\(X\\sim P\\) or \\(X\\sim Q\\). The goal is to decide \\(\\lambda\\in \\{0, 1\\}\\) based on \\(X\\). In other words, \\[ \\lambda \\to X\\to \\hat \\lambda \\] Our objective is to find a possibly randomized decision function \\(\\phi:\\mathcal X\\to \\{0, 1\\}\\) such that \\[ P[\\phi(X)=1] + Q[\\phi(X) = 0] \\] is minimized. We will see that optimization leads to TV, while asymptotic tensorization leads to \\(H^2\\). Theorem 8.5 (variational characterizations of TV) \\(\\sup\\)-representation: let \\(\\mathcal F = \\{f:\\mathcal X\\to \\mathbb R, \\|f\\|_\\infty \\leq 1\\}\\), then \\[ \\mathrm{TV}(P, Q) = \\sup_E P(E) - Q(E) = \\dfrac 1 2 \\sup_{f\\in \\mathcal F} \\left[\\mathbb E_P f(X) - \\mathbb E_Q f(X)\\right] \\] Supremum is achieved by \\(f=\\chi_E\\), where \\(E=\\{x:p(x)&gt;q(x)\\}\\). \\(\\inf\\)-representation: Provided the diagonal is measurable, \\[ \\mathrm{TV}(P, Q) = \\min_{P_{XY}} \\{P_{XY}[X\\neq Y] \\text{ subject to } P_X=P, P_Y=Q\\} \\] Proof The upper bound by \\(\\mathrm{TV}\\) is intuitive; to demonstrate saturation, let \\(E = \\{x:p(x)&gt;q(x)\\}\\), then \\[\\begin{align} 0 = \\int [p(x) - q(x)]\\, d\\mu &amp;= \\int_E + \\int_{E^c} [p(x) - q(x)]\\, d\\mu \\\\ \\int_E [q(x) - p(x)]\\, d\\mu &amp;= \\int_{E^c} [p(x) - q(x)]\\, d\\mu \\end{align}\\] The sum of these two integrals (note the definition of \\(E\\)) equals \\(2\\mathrm{TV}\\), then \\[ \\mathrm{TV}(P, Q) = \\dfrac 1 2 \\int \\chi_E[(q(x) - p(x)]\\, d\\mu = \\dfrac 1 2 \\mathbb E_P \\chi_E(X) - \\mathbb E_Q \\chi_E(X) \\] For the \\(\\inf\\) representation, given any coupling \\(P_{XY}\\), for \\(f\\in \\mathcal F\\) we have \\[ \\mathbb E_P f(X) - \\mathbb E_Q f(X) = \\mathbb E_{P_{XY}}[f(X) - f(Y)] \\leq 2 P_{XY}[X\\neq Y] \\] This shows that the \\(\\inf\\)-representation is always an upper bound; we obtain saturation when \\(X\\neq Y\\) only happens for possible values of \\(X\\) disjoint from possible values of \\(Y\\), and \\(f\\) is the indicator function on the disjoint support. This is satisfied by the following construction given \\(P, Q\\): Let \\(\\pi = \\int \\pi(x)\\, d\\mu\\) denote the overlap scalar, where \\(\\pi(x) = \\min(p(x), q(x))\\). With probability \\(\\pi\\) take \\(X=Y\\) sampled from the overlap density \\[ r(x) = \\dfrac 1 \\pi \\pi(x) \\] With probability \\(1-\\pi\\) sample \\(X, Y\\) independently from \\[ p_1(x) = \\dfrac{p(x) - \\pi(x)} {1 - \\pi}, \\quad q_1(x)=\\dfrac{q(x) - \\pi(x)}{1 - \\pi} \\] Figure 8.1: Visual representation of joint construction. Note that \\(p_1, q_1\\) have disjoint supports. Now, the marginals are indeed \\(P, Q\\), and this saturates the inequality since \\(P_{XY}[X\\neq Y] = 1-\\pi=\\mathrm{TV}(P, Q)\\) The total variation distance does not tensorize well, but we have the following sandwich bound: \\[ \\dfrac 1 2 H^2 \\leq \\mathrm{TV}\\leq H\\sqrt{1 - \\dfrac{H^2}{4}} \\leq 1 \\tag{8.2} \\] We also have the following asymptotic theorem: Theorem 8.6 (asymptotic equivalence of TV and Hellinger) For any sequence of distributions \\(P_n, Q_n\\) \\[\\begin{align} \\mathrm{TV}(P_n^{\\otimes n}, Q_n^{\\otimes n}) \\to 0 &amp;\\iff H^2(P_n, Q_n) = o(1/n) \\\\ \\mathrm{TV}(P_n^{\\otimes n}, Q_n^{\\otimes n}) \\to 1 &amp;\\iff H^2(P_n, Q_n) = \\omega(1/n) \\end{align}\\] Recall that \\(o(1/n)\\) means asymptotically smaller growth than \\(1/n\\), while \\(\\omega(1/n)\\) is the opposite. Proof We will need the following tensorization result from corollary 8.2: \\[ H^2_n = 2 - 2\\left(1 - \\dfrac 1 2 H^2_1\\right)^n \\approx 2 - 2\\exp\\left[-n\\left(1 - \\dfrac 1 2 H^2_1\\right)\\right] \\] Using the sandwich bound (8.2), \\(\\mathrm{TV}\\to 0\\) implies the exponential going to \\(0\\), then \\(H^2_1\\) has to grow slower than \\(1/n\\). The oppose is true for \\(\\mathrm{TV}\\to 1\\). Joint range We first provide a special case of an inequality. Theorem 8.7 (Pinsker's inequality) For any two distributions, \\(D(P\\|Q) \\geq (2\\log e) \\mathrm{TV}(P, Q)^2\\) Proof By DPI, it suffices to consider Bernoulli distributions via the channel \\(1_E\\) which results in Bernoulli with parameter \\(P(E)\\) or \\(Q(E)\\). Working in natural units, Pinsker’s inequality for Bernoulli distributions yield \\[ \\sqrt{\\dfrac 1 2 D(P\\|Q)} \\geq \\mathrm{TV}(1_E\\circ P, Q_E\\circ Q) = |P(E) - Q(E)| \\] Taking supremum over all \\(E\\) yields the desired inequality per the TV variational characterization theorem 8.5. Definition 8.5 (joint range) Given two \\(f\\)-divergences \\(D_f\\) and \\(D_g\\), their joint range \\(\\mathcal R\\subset [0, \\infty]^2\\) is defined by \\[ \\mathcal R = \\mathrm{Image}\\left[ (P, Q)\\mapsto (D_f(P\\|Q, D_g(P\\|Q)) \\right] \\] The joint range over \\(k\\)-ary distribution is denoted \\(\\mathcal R_k\\). Our next result will characterize the set of \\(f\\)-divergences. Lemma 8.1 (Fenchel-Eggleston-Carathéodory theorem) Let \\(S\\subset \\mathbb R^d\\) and \\(x\\in \\mathrm{co}(S)\\). There exists a set of \\(d+1\\) points \\(S&#39;=\\{x_1, \\cdots x_{d+1}\\}\\in S\\) such that \\(x\\in \\mathrm{co}(S&#39;)\\). If \\(S\\) has at most \\(d\\) connected components, then \\(d\\) points are enough. As a corollary of the following theorem, it suffices to prove joint range for Bernouli, then convexify the range. Theorem 8.8 (Harremoës-Vajda) \\(\\mathcal R = \\mathrm{co}(\\mathcal R_2) = \\mathcal R_4\\) where \\(\\mathrm{co}\\) denotes the convex hull with a natural extension of convex operations to \\([0, \\infty]^2\\). In particular, \\(\\mathrm{co}(\\mathcal R_2) \\subset\\mathcal R_4\\): standard latent variable argument. \\(\\mathcal R_k \\subset \\mathrm{co}(\\mathcal R_2) = \\mathcal R_4\\). \\(\\mathcal R = \\mathcal R_4\\): the approximation theorem already implies \\(\\mathcal R = \\overline{\\bigcup_k \\mathcal R_k}\\); the closure is technical. Proof First consider claim \\(1\\). Construct a convex divergence as follows: for two pairs of distributions \\((P_0, Q_0)\\) and \\((P_1, Q_1)\\) on \\(\\mathcal X\\) and \\(\\lambda \\in [0, 1]\\). Define the typical Bernoulli latent joint \\((X, B)\\) by \\(P_B = Q_B = \\mathrm{Ber}(\\alpha)\\) and \\((P, Q)_{X|B=j} = (P_j, Q_j)\\). Applying the conditional divergence to obtain \\[ D_f(P_{XB} \\| Q_{XB}) = \\bar \\alpha D_f(P_0\\|Q_0) + \\alpha D_f(P_1 \\| Q_1) \\implies \\mathrm{co}(\\mathcal R_2)\\subset \\mathcal R_4 \\] Onto the most nontrivial claim \\(2\\): fixing \\(k\\) and distributions \\(P, Q\\) on \\([k]\\) with distributions \\((p_j), (q_j)\\), w.l.o.g. make \\(q_{j&gt;1}&gt;0\\) and concentrate \\(q_1=0\\) (i.e. concentrate all empty points onto \\(j=1\\). Consider the equivalence class \\(\\mathcal S\\) of all \\((\\tilde p_j, \\tilde q_j)\\) which have the same likelihood ratio on the support of \\(q\\): Let \\(\\phi_{j&gt;1} = p_j / q_j\\) and consider \\[ \\mathcal S = \\left\\{ \\tilde Q = (\\tilde q_j)_{j\\in [k]}: \\tilde q_j\\geq 0, \\sum \\tilde q_j=1, \\tilde q_1=0, \\right\\} \\] Note that \\(\\mathcal S\\) it the intersection of The simplex of all distributions \\(\\tilde q\\). The half-space specified by \\(\\tilde q\\cdot \\phi \\leq 1\\). We can next identify the boundary of \\(\\mathcal S_e\\subset \\mathcal S\\): \\(\\tilde q_{j\\geq 2}=1\\) and \\(\\phi_j\\leq 1\\). \\(\\tilde q_{j_1}+\\tilde q_{j_2}=1\\) and \\(\\tilde q_{j_1}\\phi_{j_1} + \\tilde q_{j_2}\\phi_{j_2}=1\\). Figure 8.2: Gray line corresponds to \\(S\\); blue and green dots correspond to elements of \\(S_e\\) identified by (1) and (2), respectively. We next have \\(\\mathcal S = \\mathrm{co}(\\mathcal S_e)\\); so for \\(Q\\), there exists extreme points \\(\\tilde Q_j\\in \\mathcal S_e\\) (note that \\(\\mathcal S_e\\) is dependent upon \\(Q\\)!) with support on at most \\(2\\) atoms (binary distributions) such that \\(Q = \\alpha_j \\tilde Q_j\\). The map asspciating \\(\\tilde P\\) given \\(\\tilde Q\\) is \\[ \\tilde p_j = \\begin{cases} \\phi_j \\tilde q_j &amp; j\\in \\{2, \\cdots, k\\}, \\\\ 1 - \\sum_{j=2}^k \\phi_j \\tilde q_j &amp; j = 1 \\end{cases} \\] On this particular set which fixes the likelihood ratio, the divergence is an affine map: \\[ \\tilde Q \\mapsto D_f(\\tilde P \\| \\tilde Q) = \\sum_{j\\geq 2}\\tilde q_j f(\\phi_j) + f&#39;(\\infty)\\tilde p_1 \\implies D_f(P\\|Q) = \\sum_{j=1}^m \\alpha_i D_f(\\tilde P_i \\|\\tilde Q_i) \\] We defer detailed examples of joint ranges to the book (7.6). Rényi divergence The Rényi divergences are a monotone transformation of \\(f\\)-divergences; they satisfy DPI and other properties. Definition 8.6 (Rényi divergence) For \\(\\lambda\\in \\mathbb R- \\{0, 1\\}\\), the Rényi divergence of order \\(\\lambda\\) between distributions \\(P, Q\\) is \\[ D_\\lambda(P \\| Q) = \\dfrac 1 {\\lambda - 1} \\log \\mathbb E_Q \\left[ \\left(\\dfrac{dP}{dQ}\\right)^\\lambda \\right] \\] To see its connection with entropy, note that \\[ \\mathbb E_Q \\left(\\dfrac{dP}{dQ}\\right)^\\lambda = \\mathrm{sgn}(\\lambda - 1) D_f(P\\|Q) + 1, \\quad f = \\mathrm{sgn}(\\lambda - 1)(x^\\lambda - 1) \\] with which the Rényi entropy becomes (the \\(\\mathrm{sgn}(\\lambda - 1)\\) is just there to keep \\(f\\) convex): \\[ D_\\lambda(P\\|Q) = \\dfrac 1 {\\lambda - 1} \\log \\left[1 + \\mathrm{sgn}(\\lambda - 1) D_f(P\\|Q) \\right], \\quad f = \\mathrm{sgn}(\\lambda - 1)(x^\\lambda - 1) \\tag{8.3} \\] Proposition 8.6 Under regularity conditions, \\[ \\lim_{\\lambda \\to 1} D_\\lambda (P \\|Q) = D(P \\| Q) \\] Proof Expand \\((d_QP)^\\lambda = \\exp(\\lambda \\ln d_QP)\\) about \\(\\lambda=1\\): \\[ (d_QP)^{\\lambda} \\approx d_QP + \\ln d_QP \\cdot d_QP \\cdot (\\lambda - 1) \\] Taking \\(\\mathbb E_Q\\) yields \\(1 + (\\lambda - 1)\\mathbb E_P[\\ln d_QP]\\); then substituting into \\(\\log x \\approx 1+x\\) yields \\[ D_{\\lambda_\\to 1}(P \\| Q) = \\dfrac 1 {\\lambda - 1} \\log \\left[ 1 + (\\lambda - 1)\\mathbb E_P[\\ln d_QP] \\right] = \\mathbb E_P \\ln d_QP = D(P \\| Q) \\] Proposition 8.7 (special cases of Rényi divergence) Consider \\(\\lambda = 1/2, 2\\): \\[ D_2 = \\log(1 + \\chi^2), \\quad D_{1/2} = -2\\log\\left(1 - \\dfrac{H^2}{2}\\right) \\] Proof The \\(D_2\\) case is apparant in light of equation (8.3). Substitute \\(\\lambda = 1/2\\): \\[ D_{1/2} = \\dfrac 1 {1/2 - 1} \\log[1 - D_{x\\mapsto 1 - \\sqrt x}(P\\|Q)] \\] It remains to show that \\(D_{1 - \\sqrt x} (P\\|Q) = \\dfrac{H^2}{2}\\), applying equation (8.1) \\[ \\mathbb E_Q\\left[1 - \\sqrt{d_QP}\\right] = \\dfrac 1 2 \\mathbb E_Q \\left(1 - \\sqrt{d_QP}\\right)^2 = 1 - \\int \\sqrt{dPdQ} \\] Several other properties: \\(\\lambda \\mapsto D_\\lambda D(P\\|Q)\\) is non-decreasing and \\(\\lambda \\mapsto (1 - \\lambda) D_\\lambda(P\\|Q)\\) is concave. For \\(\\lambda \\in [0, 1]\\) the divergence \\(D_\\lambda\\) is jointly convex. The Rényi entropy for finite alphabet is \\(H_\\lambda(P) = \\log m - D_\\lambda(P \\| U)\\). Definition 8.7 (conditional Rényi entropy) Given \\(P_{X|Y}, Q_{X|Y}\\) and \\(P_Y\\) \\[\\begin{align} D_\\lambda(P_{X|Y} \\| Q_{X|Y} | P_Y) &amp;= D_\\lambda(P_{X|Y} P_Y \\| Q_{X|Y} P_Y) = \\dfrac 1 {\\lambda - 1} \\log \\mathbb E_{Q_{X|Y}P_Y} \\left[ \\dfrac{(P_{X|Y}P_Y)(X, Y)}{(Q_{X|Y}P_Y)(X, Y)} \\right]^\\lambda \\\\ &amp;= \\dfrac 1 {\\lambda - 1} \\log \\mathbb E_{y\\sim P_Y} \\int_{\\mathcal X} P_{X|Y=y}(x)^\\lambda Q_{X|Y=y}(x)^{1-\\lambda} \\end{align}\\] Proposition 8.8 (Rényi chain rule) Given \\(P_{AB}, Q_{AB}\\), define the \\(\\lambda\\)-tilting of \\(P_B\\) towards \\(Q_B\\) by \\[ P_B^{(\\lambda)}(b) = P_B^\\lambda(b) Q_B^{1-\\lambda}(b) \\exp \\left[ -(\\lambda - 1) D_\\lambda(P_B \\| Q_B) \\right] \\] joint Rényi divergence decomposes as \\[ D_\\lambda(P_{AB} \\| Q_{AB}) = D_\\lambda(P_B \\| Q_B) + D_\\lambda( P_{A|B} \\| Q_{A|B} | P_B^{(\\lambda)}) \\] Proof First need to prove that \\(P_B^{(\\lambda)}\\) is indeed correctly normalized: \\[\\begin{align} \\exp \\left[ -(\\lambda - 1) D_\\lambda(P_B \\| Q_B) \\right] &amp;= \\mathbb E_Q \\left[\\left(\\dfrac{dP}{dQ}\\right)^\\lambda\\right] = \\int P_B^\\lambda(b) Q_B^{1-\\lambda}(b)\\, db \\end{align}\\] Next up, computing the RHS explicitly, we have \\[\\begin{align} (\\lambda - 1) [D_\\lambda(P_B \\| Q_B) + D_\\lambda( P_{A|B} \\| Q_{A|B} | P_B^{(\\lambda)})] &amp;= \\log \\int_{\\mathcal B} \\left[ P_B(b)^\\lambda Q_B(b)^{1-\\lambda} \\cdot \\int_{\\mathcal A} P_{A|B=b}(a)^\\lambda Q_{A|B=b}(a)^{1-\\lambda} \\right] \\\\ &amp;= \\log \\mathbb E_{Q_{AB}} \\left(\\dfrac{dP_{AB}}{dQ_{AB}}\\right)^\\lambda = D_\\lambda(P_{AB} \\| Q_{AB}) \\end{align}\\] Corollary 8.1 (Rényi tensorization) Specializing the chain rule to independent joints, \\[ D_\\lambda \\left(\\prod P_{X_j} \\| \\prod Q_{X_j}\\right) = \\sum_j D_\\lambda(P_{X_j} \\| Q_{X_j}) \\] Corollary 8.2 (tensorization of χ² and Hellinger) Applying tensorization and proposition 8.7: \\[\\begin{align} 1 + \\chi^2 \\left(\\prod_j P_j \\| \\prod_j Q_j\\right) &amp;= \\prod_j 1 + \\chi^2(P_j \\| Q_j) \\\\ 1 - \\dfrac 1 2 H^2 \\left(\\prod P_j, \\prod Q_j\\right) &amp;= \\prod_j 1 - \\dfrac 1 2 H^2(P_j, Q_j) \\end{align}\\] Proposition 8.9 (variational characterization via KL) Show that \\(\\forall \\alpha \\in \\mathbb R\\): \\[ \\bar \\alpha D_\\alpha(P \\|Q) = \\inf_R \\left[ \\alpha D(R\\|P) + \\bar \\alpha D(R\\|Q) \\right] \\] Proof The KL case \\(\\alpha=1\\) holds trivially with \\(R=P\\). otherwise expand the LHS to \\[\\begin{align} \\bar \\alpha D_\\alpha(P\\|Q) &amp;= -\\log \\mathbb E_Q \\left(\\dfrac{dP}{DQ}\\right)^\\alpha = \\log \\mathbb E_Q \\left( \\dfrac{dQ}{dP} \\right)^\\alpha \\end{align}\\] Expand the RHS to \\[\\begin{align} \\alpha D(R\\|P) + \\bar \\alpha D(R\\|Q) &amp;= \\mathbb E_R \\log \\left[ \\left(\\dfrac{dR}{dP}\\right)^\\alpha \\left(\\dfrac{dR}{dQ}\\right)^{\\bar \\alpha} \\right] = \\mathbb E_R \\log \\dfrac{dR}{dP^\\alpha dQ^{\\bar \\alpha}} \\\\ &amp;= \\mathbb E_R \\log \\dfrac{dR}{dQ} \\cdot \\left(\\dfrac{dQ}{dP}\\right)^\\alpha = D(R \\| Q) + \\mathbb E_R \\log \\left(\\dfrac{dQ}{dP}\\right)^\\alpha \\end{align}\\] We wish to establish the bound \\[\\begin{align} \\log \\mathbb E_Q \\left( \\dfrac{dQ}{dP} \\right)^\\alpha \\leq \\mathbb E_R \\log \\left[ \\dfrac{dR}{dP^\\alpha dQ^{\\bar \\alpha}} \\right] &amp;= D(R \\| Q) + \\mathbb E_R \\log \\left(\\dfrac{dQ}{dP}\\right)^\\alpha \\\\ D(R\\|Q) &amp;\\geq \\mathbb E_R \\log \\left(\\dfrac{dP}{dQ}\\right)^\\alpha - \\log \\mathbb E_Q \\left(\\dfrac{dP}{dQ}\\right)^\\alpha \\end{align}\\] Comparison between \\(\\log \\mathbb E\\) and \\(\\mathbb E\\log\\) screams Donsker-Varadhan 5.5: \\[ D(R \\|Q) \\geq \\mathbb E_R f - \\log \\mathbb E_Q \\exp f \\] for \\(f = \\alpha \\log(dP/dQ)\\) being the likelihood ratio. Recall that the saturation constraint is \\[ \\log \\left(\\dfrac{dP}{dQ}\\right)^\\alpha = f = \\log \\dfrac{dR}{dQ} + C \\iff \\dfrac{P(x)^\\alpha}{Q(x)^\\alpha} = \\dfrac{R(x)}{Q(x)} \\iff dR \\propto dP^\\alpha dQ^{\\bar \\alpha} \\] the proportionality freedom comes from invariance of \\(f\\mapsto f+C\\) in Donsker-Varadhan and is determined by the normalization constraint. The final result is simply the Renyi-tilt of \\(P\\) towards \\(Q\\) given by \\[ R(x) = P(x)^\\alpha Q(x)^{\\bar \\alpha} \\exp \\left[ -\\bar \\alpha D_\\alpha(P \\| Q) \\right] \\] Variational characterizations Given a convex function \\(f:(0, \\infty)\\to \\mathbb R\\), recall that is convex conjugate \\(f^*:\\mathbb R\\to \\mathbb R\\cup \\{+\\infty\\}\\) is defined by \\[ f^*(y) = \\sup_{x\\in \\mathbb R_+} xy - f(x) \\] For a differentiable convex function, \\(\\mathrm{dom}(f^*) = \\{y: f^*(y)&lt;\\infty\\}\\) is the range of \\(\\nabla f\\). The Legendre transform is convex, involutary, and satisfies \\[ f(x) + f^*(y) \\geq xy \\] Similarly, the convex conjugate of any convex functional \\(\\Psi(P)\\) is, under suitable technical conditions \\[ \\Psi^*(g) = \\sup_P \\int g\\, dP - \\Psi(P), \\quad \\Psi(P) = \\sup_g \\int g\\, dP - \\Psi^*(g) \\] When \\(P\\) is a probability measure, \\(\\int g\\, dP = \\mathbb E_P[g]\\). In this section, we identify \\(f:(0, \\infty)\\to \\mathbb R\\) with its convex extension \\(f:\\mathbb R\\to \\mathbb R\\cup \\{+\\infty\\}\\) (one can just take \\(f = \\infty\\) for all inputs outside the original domain); different choices of \\(f\\)’s extension yield different variational formulas: Theorem 8.9 (supremum characterization of f-divergences) Given probability measures \\(P, Q\\) on \\(\\mathcal X\\); fix an extension of \\(f\\) and corresponding Legendre transform \\(f^*\\); denote \\(\\mathrm{dom}(f^*) = (f^*)^{-1}(\\mathbb R)\\), then \\[ D_f(P\\|Q) = \\sup_{g:\\mathcal X\\to \\mathrm{dom}(f^*)} \\mathbb E_P[g(X)] - \\mathbb E_Q[(f^*\\circ g)(X)] \\] As a result of this variational characterization, we obtain: Convexity: \\((P, Q)\\mapsto D_f(P\\|Q)\\) is convex since \\(D_f\\) is a supremum of affine functions. Weak lower semicontinuity: a simple counter-example to continuity is \\(\\sum_{j=1}^n X_j/\\sqrt n\\to \\mathcal N(0, 1)\\) for \\(X_j=2\\mathrm{Ber}(1/2)-1\\) but each partial sum remains discrete. By the same line of reasining in theorem 5.6: \\[ \\liminf_{n\\to \\infty} D_f(P_n\\|Q_n) \\geq D_f(P\\|Q) \\] Relation to DPI: again, variational characterizations can be used as a version of DPI when using \\(1_E\\) since it allows one to bound the event variation using \\(f\\)-divergence. Example 8.1 (TV) Recall, for \\(\\mathrm{TV}, f(x)=|x-1|/2\\); extending as such to \\(\\mathbb R\\) to obtain the conjugate \\[ f^*(y) = \\sup_x xy - \\dfrac 1 2 |x-1| = \\begin{cases} \\infty &amp; |y|&gt;1/2 \\\\ y &amp; |y| \\leq 1/2 \\end{cases} \\] Applying theorem 8.9 yields \\[ \\mathrm{TV}(P, Q) = \\sup_{g(x)\\in [-1/2, 1/2]} \\mathbb E_P[g] - \\mathbb E_Q[g] \\] This recovers theorem 8.5. Example 8.2 (Hellinger) The conjugate for Hellinger \\(f(x) = (1-\\sqrt x)^2\\) is \\(f^*(y) = \\frac 1 {1-y} - 1\\) with \\(y\\in (-\\infty, 1)\\), yielding \\[ H^2(P, Q) = \\sup_{g(x)\\in (-\\infty, 1)} \\mathbb E_P[g] - \\mathbb E_Q\\left[\\dfrac 1 {1-g} - 1\\right] = \\sup_{g&gt; 0} \\mathbb E_P[1-g] - \\mathbb E_Q \\dfrac 1 g + 1 = 2 - \\inf_{g&gt; 0} \\mathbb E_P g + \\mathbb E_Q \\dfrac 1 g \\] Given \\(f:\\mathcal X\\to [0, 1]\\) and \\(\\tau \\in (0, 1)\\), we have \\(h=1-\\tau f\\) and \\(\\frac 1 h \\leq 1 + \\frac{\\tau}{1-\\tau} f\\), then \\[ \\mathbb E_P[f] = \\dfrac 1 {1-\\tau} \\mathbb E_Q f + \\dfrac 1 \\tau H^2(P, Q), \\quad \\forall f:\\mathcal X\\to [0, 1], \\quad \\tau \\in (0, 1) \\] Example 8.3 (χ²-divergence) Given \\(\\chi^2\\) and \\(f(x)=(x-1)^2\\), the conjugate is \\(f^*(y) = y + \\frac{y^2}{4}\\) and \\[\\begin{align} \\chi^2(P\\|Q) &amp;= \\sup_{g:\\mathcal X\\to \\mathbb R} \\mathbb E_P g - \\mathbb E_Q \\left[ g + \\dfrac{g^2}{4} \\right] \\\\ &amp;= \\sup_{g:\\mathcal X\\to \\mathbb R} \\sup_{\\lambda \\in \\mathbb R} \\left[ \\mathbb E_P g - \\mathbb E_Q g \\right] \\lambda - \\dfrac{\\mathbb E_Q[g^2]}{4} \\lambda^2 \\end{align}\\] Complete the square in the inner problem to obtain \\[\\begin{align} -a \\lambda^2 + b\\lambda &amp;= -a\\left(\\lambda - \\dfrac b {2a}\\right)^2 + \\dfrac{b^2}{4a} \\implies \\chi^2(P\\|Q) = \\sup_{g:\\mathcal X\\to \\mathbb R} \\dfrac{\\left(\\mathbb E_P g - \\mathbb E_Q g\\right)^2}{\\mathbb E_Q[g^2]} \\end{align}\\] The choice of \\(g\\) is invariant under \\(g\\mapsto g+c\\), so we obtain \\[ \\chi^2(P\\|Q) = \\sup_{g:\\mathcal X\\to \\mathbb R} \\dfrac{\\left(\\mathbb E_P g - \\mathbb E_Q g\\right)^2}{\\mathrm{Var}_Q[g^2]} \\tag{8.4} \\] Given two hypotheses \\(P, Q\\), \\(\\chi^2\\) is the maximum ratio between the squared-distance between the mean statistic \\(g\\) and its variance under \\(Q\\). Example 8.4 (Jenson-Shannon divergence) For JS we have \\(f(x) = x\\log \\dfrac{2x}{1+x} + \\log \\dfrac{2}{1+x}\\) with conjugate and variational characterization \\[\\begin{align} f^*(s) &amp;= -\\log(2 - e^s), \\quad s\\in (-\\infty, \\log 2) \\\\ \\mathrm{JS}(P, Q) &amp;= \\sup_{g(x)&lt;\\log 2} \\mathbb E_P g + \\mathbb E_Q \\log(2 - e^g) \\end{align}\\] Reparameterize with \\(h=e^g/2\\) to obtain \\[ \\mathrm{JS}(P, Q) = \\log 2 + \\sup_{h(x)\\in (0, 1)} \\mathbb E_P \\log h + \\mathbb E_Q\\log(1-h) \\] In desity estimation, suppose \\(P\\) is the data distribution wish to approximate with \\(P_{f_\\theta(Z)}\\), then we have \\[ \\inf_\\theta \\sup_\\phi \\mathbb E_{X\\sim P} [\\log h_\\phi(X)] + \\mathbb E_{Z\\sim \\mathcal N} \\log[1 - h_\\phi(f_\\theta(Z))] \\] Here \\(f_\\theta\\) is the generator, and \\(h_\\phi\\) is the critic in GAN. Note that we can never obtain Donsker-Varadhan using theorem 8.9 since DV claims \\[ D(P\\|Q) = \\sup_{g(x)\\in \\mathbb R} \\mathbb E_Pg - \\log \\mathbb E_Q e^g \\] while the theorem result is always linear in \\(Q\\). To do so, we need to consider an extension of \\(D_f(P\\|Q)\\) to non-probability measures. Theorem 8.10 (extended variational characterization) Define \\(f(x)=\\infty\\) for \\(x&lt;0\\) and let \\(S=\\{x:q(x)&gt;0\\}\\) where \\(dQ = q(x) d\\mu\\), then \\[\\begin{align} D_f(P\\|Q) &amp;= f&#39;(\\infty) (1-P[S]) + \\sup_g \\mathbb E_P[g 1_S] - \\Psi^*_{QP}(g) \\\\ \\Psi^*_{QP}(g) = \\inf_{a\\in \\mathbb R} \\mathbb E_Q[(f^*\\circ g) - a] + aP[S] \\end{align}\\] For the special case \\(f&#39;(\\infty) = \\infty\\), which includes KL, we have \\[ D_f(P\\|Q) = \\sup_G E_P[g] - \\Psi^*_Q(g), \\quad \\Psi_Q^*(g) = \\inf_{a\\in \\mathbb R} \\mathbb E_Q[f^*\\circ g - a] + a \\] Empirical distribution and χ² Fisher information, location family We consider a parameterized set of distributions \\(\\{P_\\theta: \\theta\\in \\Theta\\}\\) where \\(\\Theta\\) is an open subset of \\(\\mathbb R^d\\). Further suppose that \\(P_\\theta(dx)=p_\\theta(x)\\mu(dx)\\) for some common dominating measure \\(\\mu\\). Definition 8.8 (Fisher information, score) If for each fixed \\(x\\), the density \\(p_\\theta(x)\\) depends smoothly on \\(\\theta\\), we define the Fisher information matrix w.r.t. \\(\\theta\\) as \\[ \\mathcal J_F(\\theta) = \\mathbb E_\\theta(VV^T), \\quad V = \\nabla_\\theta \\ln p_\\theta(X) = \\dfrac 1 {p_\\theta(X)} \\nabla_\\theta p_\\theta(X) \\] The random variable \\(V\\) is known as the score. Under technical regularity conditions, we have: \\(\\mathbb E_\\theta[V]=0\\): for an intuitive argument \\[ \\mathbb E_\\theta[\\nabla_\\theta \\ln p_\\theta(X)] = \\int \\nabla_\\theta p_\\theta(x) = \\nabla_\\theta \\int p_\\theta = 0 \\] \\(\\mathcal J_F(\\theta) = \\mathrm{Cov}_\\theta(V) = -\\mathbb E_\\theta \\left[\\mathcal H_\\theta \\ln p_\\theta(X)\\right]\\): differentiating (1) \\[\\begin{align} \\partial_{jk}^2 \\ln p_\\theta = \\partial_{j} \\left( \\dfrac 1 {p_\\theta} \\partial_{k} p_\\theta \\right) = -\\dfrac{(\\partial_{j} p_\\theta)(\\partial_{k} p_\\theta)} {p_\\theta^2} + \\left[\\dfrac 1 {p_\\theta} \\partial_{jk}^2 p_\\theta\\right]_{=0} = (VV^T)_{jk} \\tag{8.5} \\end{align}\\] The second term vanishes when we take the expectation. \\(D(P_{\\theta} \\| P_{\\theta + \\xi}) = \\dfrac{\\log e}{2} \\xi^T \\mathcal J_F(\\theta)\\xi + o(\\|\\xi\\|^2)\\): this is obtained by integrating \\[ \\ln p_{\\theta + \\xi}(x) = \\ln p_\\theta(x) + \\xi^T \\nabla_\\theta \\ln p_\\theta(x) + \\dfrac 1 2 \\xi^T [\\mathcal H_\\theta \\ln p_\\theta(x)]\\xi + o(\\|\\xi\\|^2) \\] Under a smooth invertible map \\(\\theta \\to \\tilde \\theta\\), we have \\[ \\mathcal J_F(\\tilde \\theta) = J_{\\tilde \\theta \\to \\theta}^T \\mathcal J_F(\\theta) J_{\\tilde \\theta\\to \\theta} \\] In fact, this allows one to define a Riemannian metric on the parameter space, called the Fisher-Rao metric. Tensorization: under i.i.d. observations, \\(X^n\\sim P_\\theta\\) i.i.d, we have \\(\\{P_\\theta^{\\otimes n}:\\theta \\in \\Theta\\}\\) whose Fisher information \\(\\mathcal J_F^{\\otimes n}(\\theta) = n\\mathcal J_F(\\theta)\\). We next consider the information properties of Fisher information under regularity conditions. Consider a joint distribution \\(P^{XY}_\\theta\\) parameterized by \\(\\theta\\) with marginals \\(P^X_\\theta, P^Y_\\theta\\). Definition 8.9 (conditional Fisher information) The conditional Fisher information \\(\\mathcal J^{Y|X}_\\theta\\) is the semidefinite matrix of size \\(n\\) defined as \\[ \\mathcal J^{Y|X}_F(\\theta) = \\mathbb E_X[\\mathcal J^{Y|X=x}_F(\\theta)] = \\mathbb E_{XY}[V^{Y|X}_\\theta(V^{Y|X}_\\theta)^T], \\quad V^{Y|X}_\\theta(x, y) = \\nabla_\\theta \\ln P^{Y|X}_\\theta(x, y) \\] Proposition 8.10 (Fisher information chain rule) \\(\\mathcal J^{XY}_F(\\theta) = \\mathcal J^X_F(\\theta) + \\mathcal J^Y_F(\\theta)\\) Proof Note that \\(V^{XY}_\\theta, V^{Y|X}_\\theta\\) are functions of \\(x, y\\): \\[\\begin{align} V^{XY}_\\theta(x, y) &amp;= \\nabla_\\theta \\ln P^{XY}_\\theta(x, y) = \\nabla_\\theta \\ln P^{Y|X}_\\theta(x, y) + \\nabla_\\theta \\ln P^X_\\theta(x, y) \\\\ \\mathcal J^{XY}_F(\\theta) = V^{Y|X}_\\theta + V^X_\\theta &amp;= \\mathbb E_{XY} \\left[ (V^{Y|X}_\\theta + V^X_\\theta)(V^{Y|X}_\\theta + V^X_\\theta)^T \\right] \\\\ &amp;= \\mathcal J^X_F + \\mathcal J^{Y|X}_F + 2\\mathbb E_{XY}\\left[ (V^X_\\theta)^T V^{Y|X}_\\theta \\right] \\end{align}\\] The last term vanishes since \\[\\begin{align} \\mathbb E_{Y|X=x}[\\nabla V^{Y|X}_\\theta(x, y)] &amp;= \\int P_{Y|X=x}(y) \\dfrac 1 {P_{Y|X=x}(y)} \\nabla_\\theta P_{Y|X=x}(y)\\, dy = 0 \\\\ \\mathbb E_{XY}\\left[(V^X_\\theta)^T V^{Y|X}_\\theta\\right] &amp;= \\mathbb E_X \\left[ (V^X_\\theta)^T \\mathbb E_{Y|X=x} \\left[ V^{Y|X}_\\theta \\right]_{=0} \\right] = 0 \\end{align}\\] Corollary 8.3 (monotonicity) \\(\\mathcal J^{XY}_F(\\theta) \\geq \\mathcal J^X_F(\\theta)\\), here \\(\\geq\\) is in the sense of positive-semidefinite matrices. Corollary 8.4 (DPI) \\(\\mathcal J^{X}_F(\\theta) \\geq \\mathcal J^Y_F(\\theta)\\) if \\(\\theta\\to X\\to Y\\). Definition 8.10 (location family) For any density \\(p_0\\) on \\(\\mathbb R^d\\), one can define a location family of distributions on \\(\\mathbb R^d\\) by setting \\(P_\\theta(dx) = p_0(x-\\theta)dx\\). In this case the Fisher information is translation-invariant, does not depend on \\(\\theta\\), and its written instead to emphasize its dependence on \\(p_0\\): \\[ J(p_0) = \\mathbb E_{X\\sim p_0} \\left[(\\nabla \\ln p_0)(\\nabla \\ln p_0)^T\\right] = -\\mathbb E_{X\\sim p_0} \\left[ \\mathcal H_X \\ln p_0 \\right] \\] Proposition 8.11 (properties of location family) Given distributions \\(X, Y\\), the following hold: \\(J_F(XY) = J_F(X)+J_F(Y)\\). \\(J_F(a\\theta + X) = a^2J_F(X)\\) Local χ² behavior The first theorem gives the linear coefficient of \\(D_f(\\lambda P + \\bar \\lambda Q \\| Q)\\) as \\(\\lambda\\to 0^+\\). When \\(P\\ll Q\\), the decay is always sublinear with quadratic coefficient given by theorem 8.12. Else the linear coefficient is given by theorem 8.11. When a distribution family satisfies additional regularity conditions 8.11, its local quadratic behavior is governed by the Fisher information matrix using theorem ??. Theorem 8.11 (local linear coefficient) Suppose \\(D_f(P\\|Q)&lt;\\infty\\) and \\(f&#39;(1)\\) exists, then \\[ \\lim_{\\lambda\\to 0} \\dfrac 1 \\lambda D_f(\\lambda P + \\bar \\lambda Q \\|Q) = (1 - P[Q\\neq 0])f&#39;(\\infty) \\] In particular, for \\(P\\ll Q\\), the decay is always sublinear. Proof Per proposition 8.4 we may assume \\(f(1)=f&#39;(1)=0\\) and \\(f\\geq 0\\). Decompose \\(p\\) into absolutely continuous and singular parts \\[ P = \\mu P_1 + \\bar \\mu P_0, \\quad P_0\\perp Q, \\quad P_1\\ll Q \\] Expand out the definition of divergence to obtain \\[ \\dfrac 1 \\lambda D_f(\\lambda P + \\bar \\lambda Q \\| Q) = \\bar \\mu f&#39;(\\infty) + \\int dQ \\dfrac 1 \\lambda f\\left[ 1 + \\lambda \\left( \\mu \\dfrac{dP_1}{dQ} - 1 \\right) \\right] \\] The function \\(g(\\lambda) = f(1 + \\lambda t)\\) is positive and convex for every \\(\\in \\mathbb R\\), then \\(\\dfrac 1 \\lambda g(\\lambda)\\) decreases monotonically to \\(g&#39;(0)=0\\) as \\(\\lambda \\to 0^+\\) (picture a zero-centered convex function). The integrand is \\(Q\\)-integrable at \\(\\lambda=1\\) (which dominates \\(g(t, \\lambda\\leq 1)\\), then applying dominated covergence theorem yields the desired result. Theorem 8.12 (local quadratic coefficient) Let \\(f\\) be twice continuously differentiable on \\((0, \\infty)\\) with \\(\\limsup_{x\\to \\infty} f&#39;&#39;(x)\\) finite. If \\(\\chi^2(P\\|Q)&lt;\\infty\\) (in particular, \\(P\\ll Q\\)), then \\(D_f(\\lambda P + \\bar \\lambda Q \\|Q)\\) is finite for all \\(\\lambda \\in [0, 1]\\) and \\[ \\lim_{\\lambda \\to 0} \\dfrac 1 {\\lambda^2} D_f(\\lambda P + \\bar \\lambda Q \\| Q) = \\dfrac{f&#39;&#39;(1)}{2} \\chi^2(P\\|Q) \\] If \\(\\chi^2(P\\|Q)=\\infty\\) with \\(f&#39;&#39;(1)&gt;0\\), then \\(D_f(\\bar \\lambda Q + \\lambda P \\|Q) = \\omega(\\lambda^2)\\). This includes KL, SKL, \\(H^2\\), JS, LC, and all Renyi divergences of orders \\(\\lambda &lt; 2\\). Proof Taylor expand with integral form, then apply the dominated convergence theorem. Since \\(P\\ll Q\\), we can use the expression \\(D_f(P\\|Q) = \\mathbb E_Q[f(d_QP)]\\). W.l.o.g. again assume \\(f&#39;(1) = f(1)=0\\), then \\(\\chi^2\\) is written as \\[\\begin{align} D_f(\\bar \\lambda Q + \\lambda P \\| Q) &amp;= \\int dQ f\\left(1 - \\lambda + \\lambda \\dfrac{dP}{dQ}\\right) \\, d\\mu \\\\ &amp;= \\int dQ f\\left(1 + \\lambda \\dfrac{dP - dQ}{DQ}\\right) \\, d\\mu \\end{align}\\] Next up, apply Taylor’s theorem with remainder (which is integration by parts) to \\[\\begin{align} f(1+u) &amp;= u^2 \\int_0^1 (1-t) f&#39;&#39;(1+tu)dt \\end{align}\\] Apply to \\(u=\\lambda \\dfrac{P-Q}{Q}\\) to obtain \\[ D_f(\\bar \\lambda Q + \\lambda P \\|Q) = \\int dQ \\int_0^1 dt (1-t) \\lambda^2 \\left(\\dfrac{P-Q}{Q}\\right)^2 f&#39;&#39;\\left(1 + t\\lambda \\dfrac{P-Q}{Q}\\right) \\] Given the condition of the theorem, we can apply the dominated convergence theorem and Fubini to obtain \\[\\begin{align} \\lim_{\\lambda \\to 0} \\dfrac 1 {\\lambda^2} D_f(\\bar \\lambda Q + \\lambda P \\|Q) &amp;= \\int_0^1 dt(1-t) \\int dQ \\left(\\dfrac{P-Q}{Q}\\right)^2 \\lim_{\\lambda\\to 0} f&#39;&#39;\\left(1 + t\\lambda \\dfrac{P-Q}{Q}\\right) \\\\ &amp;= \\int_0^1 (1-t)dt \\int dQ \\left(\\dfrac{P-Q}{Q}\\right)^2 = \\dfrac{f&#39;&#39;(1)}{2}\\chi^2(P\\|Q) \\end{align}\\] Definition 8.11 (regular single-parameter families) We call a single-parameter family \\(\\{P_t, t\\in [0, \\tau)\\}\\) regular at \\(t=0\\) if the following holds: There is a dominating measure: \\(P_t(dx)=p_t(x)\\mu(dx)\\) for some measurable \\((t, x)\\mapsto p_t(x)\\). Density varies smoothly in \\(t\\): there exists measurable \\(\\dot p_s(x)\\) such that for \\(\\mu\\)-a.e. \\(x_0\\) we have \\[ \\int_0^\\tau |\\dot p_s(x_0)|\\, ds &lt; \\infty, \\quad p_ t(x_0) = p_0(x_0) + \\int_0^t \\dot p_s(x_0)\\, ds \\] as well as \\(\\lim_{t\\to 0^+} \\dot p_t(x_0) = \\dot p_0(x_0)\\). \\(p_0(x)=0\\implies \\dot p_t(x)=0\\), and \\[ \\int \\sup_{0\\leq t&lt;\\tau} \\dfrac{\\dot p_t(x)^2}{p_0(x)} \\, d\\mu &lt; \\infty \\] Condition (b) holds for \\(h_t(x) = \\sqrt{p_t(x)}\\), and \\(\\{\\dot h_t\\}_{t\\in [0, \\tau)}\\) is uniformly \\(\\mu\\)-integrable. In particular, conditions (b, c) implies that \\(P_t\\ll P_0\\). Theorem 8.13 (local Fisher behavior of divergence) Given a family \\(P_{t\\in [0, \\tau)}\\) in definition 8.11, we have \\[\\begin{align} \\chi^2(P_t\\|P_0) = J_F(0)t^2 + o(t^2), \\quad D(P_t\\|P_0) = \\dfrac{\\log e}{2}J_F(0)t^2 + o(t^2) \\end{align}\\] Proof Idea: rewrite the difference in \\(\\chi^2\\) as an integral, then apply the dominated convergence theorem. Recall that \\(P_t\\ll P_0\\), then \\[\\begin{align} \\dfrac 1 {t^2}\\chi^2(P_t\\|P_0) &amp;= \\dfrac 1 {t^2} \\int d\\mu \\dfrac{[p_t(x) - p_0(x)]^2}{p_0(x)} \\\\ &amp;= \\dfrac 1 {t^2} \\int \\dfrac{d\\mu}{p_0(x)} \\left( t\\int_0^1 \\dot p_{tu}(x)\\, d\\mu \\right)^2 \\\\ &amp;= \\int d\\mu \\int_0^1 du_1 \\int_0^1 du_2 \\dfrac{\\dot p_{tu_1}(x) \\dot p_{tu_2}(x)}{p_0(x)} \\end{align}\\] By regularity condition (c), take the limit \\(t\\to 0\\) and apply the monotone convergence theorem to yield \\[\\begin{align} \\lim_{t\\to 0} \\dfrac 1 {t^2}\\chi^2(P_t\\|P_0) &amp;= \\int d\\mu \\int_0^1 du_1 \\int_0^1 du_2 \\lim_{t\\to 0} \\dfrac{\\dot p_{tu_1}(x) \\dot p_{tu_2}(x)}{p_0(x)} \\\\ &amp;= \\int d\\mu \\dfrac{\\dot p_0(x)^2}{p_0(x)} = J_F(0) \\end{align}\\] Proposition 8.12 (variational characterization of Fisher information) For a density \\(P\\) on \\(\\mathbb R\\), the location family Fisher information \\[ J(P) = \\sup_h \\dfrac{\\mathbb E_P[h&#39;]^2}{\\mathbb E_P[h]} \\] where the supremum is over test functions which are continuously differentiable and compactly supported such that \\(\\mathbb E_P[h^2] &gt; 0\\). We defer details to textbook 7.13. This can be anticipated from the variational \\(\\chi^2\\) formula (8.4) together with the local \\(\\chi^2\\) expansion 8.12. \\[ \\chi^2(P_t \\| P) = \\sup \\dfrac{\\left(\\mathbb E[h(X+t) - h(X)\\right)^2}{\\mathbb E[h^2]} = \\sup \\dfrac{\\mathbb E[h&#39;]^2}{\\mathbb E[h^2]} t^2 + o(t^2) \\] "],["data-compression.html", "9 Data compression Source coding theorems", " 9 Data compression This section corresponds to Chapters 10-12 of the book. Fixing a source domain \\(\\mathcal X\\), we can define: A compressor \\(f:\\mathcal X\\to \\{0, 1\\}^*\\). A decompressor \\(g:\\{0, 1\\}^*\\to \\mathcal X\\). Highlights include: The average compression length of the optimal single-shot lossless compressor (theorem 9.1). Source coding theorems The single-shot in the definition below refers to the fact that we are compressing and decompressing each symbol, instead of compressing many then decompressing them; this we do not need to impose any constraints on \\(f\\), such as prefix-freeness or unique-decodability. Definition 9.1 (variable-length single-shot lossless compression) A pair \\((f, g)\\) is a variable-length single-shot lossless compressor if: \\(f\\) is of the type \\(f:\\mathcal X\\to \\{0, 1\\}^*\\). The image of \\(f\\) is a codebook, and an element \\(f(x)\\) in the image of \\(f\\) is a codeword. There exists a decompressor such that \\(g\\circ f = 1_{\\mathcal X}\\). Two immediate results of this definition: Lossless compression is only possible for discrete \\(X\\). Without loss of generality, we can sort \\(\\mathcal X\\) so that \\(P(0)\\geq P(1)\\geq \\cdots\\). Definition 9.2 (stochastic dominance) Given two real-valued stochastic variables \\(X, Y\\), we say that \\(X\\preceq Y\\), or \\(Y\\) stochastically dominates \\(X\\), if the CDF of \\(X\\) dominates that of \\(Y\\) everywhere. Definition 9.3 (optimal single-shot lossless compressor) For a down-sorted PMF \\(P(X)\\), the optimal single-shot lossless compressor assigns strings with increasing lengths to symbol \\(j\\in \\mathcal X=\\mathbb N\\). In particular, \\(l(f^*(j)) = \\lfloor \\log_2 j\\rfloor\\), where \\(l\\) is the string-length function. An immediate corollary of the following result is that \\(\\mathbb E[(l\\circ f^*)(X)] \\leq \\mathbb E[(l\\circ f)(X)]\\) for any \\(X\\). Proposition 9.1 For any lossless single-shot compressor \\(f\\) and source distribution \\(X\\), \\(l\\circ f^*\\preceq l\\circ f\\). Proof Let \\(A_k = \\{x:l(f(x)) \\leq k\\}\\) denote the set of inputs which are encoded to length at most \\(k\\). Note that, since encoding is lossless and there are at most \\(2^{k+1}-1\\) binary strings of length \\(k\\), we have \\[ |A_k| \\leq \\sum_{j=0}^k 2^j = 2^{k+1}-1 = |A_k^*| \\] The stochastic dominance follows from the fact that \\(f^*\\) sorts the PMF in descending order: \\[ \\mathrm{Pr}[(l\\circ f)(X)\\leq k] = \\sum_{x\\in A_k} P_X(x) \\leq \\sum_{x\\in A^*_k} P_X(x) = \\mathrm{Pr}[(l\\circ f^*)(X) \\leq k] \\] Working in units of bits, the average compression length of the optimal encoding is \\(H(X)\\). A coding theorem relates a fundamental limit \\(\\mathbb E[(l\\circ f^*)(X)]\\), which is an operational quantity, to an information measure \\(H(X)\\). Lemma 9.1 \\(H(X|L=k)\\leq k\\). This follows from that \\(X|L=k\\) can take at most \\(2^k\\) values, the uniform distribution on which has entropy \\(k\\). Lemma 9.2 For \\(X\\) taking values on \\(\\mathbb N=\\{1, 2, \\cdots\\}\\) and \\(\\mathbb E[X]&lt;\\infty\\), we have \\[ H(X) \\leq h\\left(\\mathbb E[X]^{-1}\\right)\\mathbb E[X] \\] The inequality is saturated by the geometric distribution. Proof Let \\(p = 1 / \\mathbb E[X]\\) and note that \\(\\mathbb E[xp] = 1, \\mathbb E[x\\bar p] = \\mathbb E[x(1-p)] = \\mathbb E[x - 1]\\), then \\[\\begin{align} H(X) - \\mathbb E[X] h(p) &amp;= \\mathbb E\\left[-\\log p(x) - x[-p\\log p - \\bar p\\log \\bar p]\\right] \\\\ &amp;= \\mathbb E\\left[-\\log p(x) + xp \\log p + x\\bar p\\log \\bar p\\right] \\\\ &amp;= \\mathbb E\\left[-\\log p(x) + (x-1) \\log \\bar p + \\log p\\right] \\\\ -D(P_X\\|\\mathrm{Geom}_p) &amp;= -\\mathbb E\\log \\dfrac{p(x)}{p{\\bar p}^{x-1}} \\\\ &amp;= \\mathbb E\\left[-\\log p(x) + (x-1) \\log \\bar p + \\log p\\right] \\\\ -D(P_X\\|\\mathrm{Geom}_p) &amp;= H(X) - \\mathbb E[X] h(p) \\\\ H(X) &amp;= \\mathbb E[X] h(p) + D(P_X\\|\\mathrm{Geom}_p)_{\\geq 0} \\end{align}\\] Theorem 9.1 (average length of optimal encoding) \\(H(X) - \\log_2[e(H(X)+1)] \\leq \\mathbb E[(l\\circ f^*)(X)] \\leq H(X)\\). Proof Let \\(L(X) = (l\\circ f^*)(X)\\), assume \\(X\\) sorted w.l.o.g, then \\(P_X(x) \\leq 1/x\\) and \\[ L(x) = \\lfloor \\log_2(x)\\rfloor \\leq \\log_2 \\dfrac 1 {P_X(x)} \\implies \\mathbb E[L(x)] \\leq \\mathbb E\\left[\\log_2 \\dfrac 1 {P_X(x)}\\right] = H(X) \\] On the other hand, first note that \\(H(X|L=k)\\leq k\\) since \\(X|L=k\\) can take at most \\(2^k\\) values, the uniform distribution on which has entropy \\(k\\). \\[\\begin{align} H(X) &amp;= H(X, L) = H(X|L) + H(L) \\\\ &amp;\\leq \\mathbb E[L] + (\\mathbb E[L] + 1) h\\left(\\dfrac 1 {1+\\mathbb E[L]}\\right) \\\\ &amp;= \\mathbb E[L] + \\log(1+\\mathbb E[L]) + \\mathbb E[L] + \\log \\left(1 + \\dfrac 1 {\\mathbb E[L]}\\right) \\\\ &amp;= \\mathbb E[L] + \\log_2(1+\\mathbb E[L]) + \\log_2 e \\leq \\mathbb E[L] + \\log_2 e(1+H(X)) \\end{align}\\] In the last tep steps, we used \\(x\\log(1+1/x) \\leq \\log e\\) and \\(H(X)\\leq \\mathbb E[L]\\). "],["lecture-notes.html", "10 Lecture notes Oct 2: Fisher information, classical minimax estimation Oct 7: Data compression Oct 9: data compression II", " 10 Lecture notes Oct 2: Fisher information, classical minimax estimation Agenda: \\(\\chi^2\\) variational characterization. \\(1\\)-parameter families; minimax rate. HCR, and Fisher information. Cramer-Rau; van Trees inequality. Key takeaways: \\(R_n^*\\) is the minimax rate of parameterization. LeCam-Hajek theory: \\(R_n^* = \\dfrac{1+o(1)}{n \\min_\\theta I_F(\\theta)}\\). To obtain more well-behaved inequalities, expand a single extremum into a nested one (e.g. scalar multiple) then solve the closed form of the inner optimization. All \\(f\\)-divergences are locally quadratic in parameteric families with Hessian given by Fisher information. Cramer-Rau is an application of the variational characterization of \\(\\chi^2\\). Main content Recalling the variational characterization: \\[ D_f(P\\|Q) = \\sup_g \\mathbb E_p g - \\mathbb E_Q f^*\\circ g \\] where the convex conjugate is given by \\[ f^*(h) = \\sup_{t\\in \\mathbb R} th - f(t), \\quad f(t) = \\sup_{h\\in \\mathbb R} th - f^*(h) \\] Recall that Donsker-varadhan is not linear in \\(\\mathbb E_Q\\), but there is a standard trick to rewrite the expectation. We now apply the variational characterization to \\(\\chi^2\\); Proposition 10.1 (variational characterization of χ²) \\(\\chi^2(P \\|Q) = \\sup_{h:\\mathcal X\\to \\mathbb R} \\mathbb E_p h(X) - \\mathbb E_Q \\left[ h(X) + \\dfrac{h(X)^2}{4} \\right]\\) Expanding this out, the first term is very useful; but the second is not. The first step is to break a single extrema into two: \\[\\begin{align} \\chi^2(P \\|Q) &amp;= \\sup_h \\left[ \\mathbb E_p h - \\mathbb E_Q h \\right] - \\dfrac 1 4 \\mathbb E_Q h^2 \\\\ &amp;= \\sup_g \\sup_{\\lambda\\in \\mathbb R} \\lambda \\left( \\mathbb E_P g - \\mathbb E_Q g \\right) - \\dfrac{\\lambda^2}{4} \\mathbb E_Q g^2 = \\sup_g \\dfrac{(\\mathbb E_p g - \\mathbb E_Q g)^2}{\\mathbb E_q g^2} \\end{align}\\] As a consequence, we obtain \\[ \\left( \\mathbb E_P g - \\mathbb E_Q g \\right)^2 \\leq \\chi^2(P \\|Q) \\mathbb E_Q g^2 \\] In fact, this equation is invariant under \\(g\\mapsto g + c\\), yielding \\[ \\left( \\mathbb E_P g - \\mathbb E_Q g \\right)^2 \\leq \\chi^2(P \\|Q) \\mathrm{Var}_Q g^2 \\] Exercise: \\(\\forall g&gt;0\\), we have \\(\\mathbb E_P g \\leq 2\\mathbb E_Q g + 2_? H^2(P, Q)\\). One-parameter families; minimax rates Statisticians care about sub-manifolds of the probability simplex. For one-parameter families, we typically have \\[ P^\\theta(dx) = p^\\theta(x) \\mu(dx) \\] For discrete, \\(\\mu\\) is counting; for real-valued, \\(\\mu\\) is Lebesgue. Parameter estimation: given \\(X_1, \\cdots, X_n \\sim P^\\theta\\); find function \\(\\hat \\theta(X_1, \\cdots, X_n)\\) such that \\(\\hat \\theta \\approx \\theta\\); here \\(\\approx\\) is defined w.r.t. a risk function \\[ R(\\hat \\theta, \\theta) = \\mathbb E_{X^n} (\\theta - \\hat \\theta(X^n))^2 \\] Note that the first argument is a function, while the second is a parameter coordinate. Example: \\(\\hat \\theta_0 = \\dfrac 1 n \\sum X_j\\) for Bernoulli parameter estimation; we can compute \\[ R(\\hat \\theta, \\theta) = \\dfrac{\\theta \\bar \\theta}{n} \\] Recall that it’s MLE and unbiased. To get mid of the \\(\\theta\\)-dependence, we can consider \\[ R^{\\mathrm{max}}(\\hat \\theta) = \\sup_\\theta R(\\hat \\theta_1, \\theta) \\] Consider a naive estimator \\(\\hat \\theta_1 = 1/2\\) (this has definite bias but not variance) and \\[ \\hat \\theta_\\lambda = \\lambda \\hat \\theta_1 + \\bar \\lambda \\hat \\theta_{\\mathrm{MLE}} \\] One can compute risk = bias^2 +variance^2 and choose an optimal \\(\\lambda_n^*= \\dfrac 1 {1 + \\sqrt n}\\); this allows one to optimize the risk everywhere. Theorem The optimal shrinkage estimator \\(\\hat \\lambda_{\\lambda_n^*}\\) saturates the minimax risk \\[ \\inf_{\\hat \\theta} R^{\\mathrm{max}}(\\hat \\theta) = \\dfrac 1 {4(1 + \\sqrt n)^2} \\] Key idea: obtain MLE; identify points of worst performance (minimum Fisher information), then bias towards it. The optimal minimax risk profile (minimax rate) is \\[ R_n^* = \\inf_{\\hat \\theta} \\sup_\\theta R(\\hat \\theta, \\theta) \\] HCR inequality; Fisher information Proposition 10.2 \\(\\forall \\hat \\theta\\) and \\(\\theta, \\theta_0\\) we have \\[ R_n^* \\geq \\mathrm{Var}_{P^\\theta} \\geq \\dfrac{ \\left(\\mathbb E_{P^\\theta} \\hat \\theta - \\mathbb E_{P^{\\theta_0}} \\hat \\theta\\right)^2 }{ \\chi^2(P^{\\theta \\otimes n}\\| P^{\\theta_0 \\otimes n}) } \\] Proof: The first inequality follows from bias-variance decomposition. For the second, take \\(g(X^n) = \\hat \\theta(X^n)\\) in the variational characterization of \\(\\chi^2\\). To bring Fisher information into the picture, consider \\[ \\chi^2 (P^\\theta \\| P^{\\theta_0}) = \\sum \\dfrac{P^\\theta(x)^2}{P^{\\theta_0}(x)} - 1 \\] Take \\(\\theta = \\theta_0 + \\epsilon\\) for small \\(\\epsilon\\); Taylor-expand to obtain \\[\\begin{align} P^\\theta(x) = P^{\\theta_0}(x) + \\partial_{\\theta }P^\\theta \\big|_{\\theta = \\theta_0} (x)\\epsilon + \\cdots \\end{align}\\] Substitute this into the expression for \\(\\chi^2\\) to obtain \\[ \\chi^2 = \\sum_x \\dfrac{\\left[ P^{\\theta_0}(x) + \\epsilon \\partial_{\\theta }P^\\theta(x) \\right]^2}{P^{\\theta_0}(x)} - 1 + o(\\epsilon^2) \\] Expanding the term, the linear term is \\(0\\) (pull \\(\\partial_{\\theta}\\) out): \\[ \\sum_x \\partial_{\\theta }P^{\\theta}(x) \\big|_{\\theta = \\theta_0} = 0 \\] The null term sums to \\(1\\), cancelling the \\(-1\\), yielding the local definition of \\(\\chi^2\\) \\[ \\chi^2 = \\epsilon^2 \\sum_x \\dfrac{\\left[\\partial_{\\theta }P^{\\theta_0}(x)\\right]^2} {P^{\\theta_0}(x)} + o(\\epsilon^2) \\] Recall that the Fisher information is given by \\[ \\mathcal J_F(\\theta; \\{P^\\theta\\}_{\\theta \\in \\Theta}) = \\int \\dfrac{\\left[\\partial_{\\theta }P^\\theta(x)\\right]^2}{P^\\theta(x)} \\, \\mu(dx) \\] Fisher information is just the second-order derivative of \\(\\chi^2\\). There are two notions of distance: Fisher information tells us, when we nudge \\(\\theta \\mapsto \\theta + \\epsilon\\), how much is the statistical distance between \\(P^\\theta\\) and \\(P^{\\theta + \\epsilon}\\). Moreover, Fisher information is additive under tensorization: KL is additive under tensorization and apply the locality principle. Theorems about Fisher information require a lot more assumptions. Oct 7: Data compression Theorem 10.1 (local expansion of χ) For \\(\\theta_0 = 0\\), when the following conditions hold: \\(\\theta \\in [0, \\tau)\\). There exists \\(\\dot p^\\theta(x)\\) satisfying \\[ P^\\theta(x) = P^0(x) + \\int_0^\\theta \\dot p^t(x)\\, dt \\] The fisher information is defined everywhere in a connected interval open interval about \\(\\theta_0\\): \\[ \\int dx\\, \\sup_{0\\leq t &lt; \\tau} \\dfrac{\\dot p^t(x)^2}{p^0(x)} &lt; \\infty \\] Then \\(\\chi^2(P^{\\theta_0 + \\delta} \\| P^{\\theta_0}) + \\delta^2 \\mathcal J_F(\\theta_0) + o(\\delta^2)\\). The quadratic local behavior of \\(\\chi^2\\) requires not only finite Fisher information at \\(\\theta_0\\); it also requires \\(\\mathcal J_F\\) to be finite almost everywhere on a nontrivial interval. In summary, \\(\\chi^2\\) is nice for the “good” cases where \\(\\mathcal J\\) is finite a.e. However, for some irregular cases it’s best to use \\(H^2(P^{\\theta_0 + \\delta}, P^\\theta)\\), whose local behavior is always determined by the local Fisher information. A location family is a one-parameter family for which the parameter controls the displacement of a fixed density \\(\\nu\\), e.g. \\(\\mathcal N(\\theta, 1)\\); for this family, \\(\\mathcal J(\\theta) = \\mathcal J(0)\\), which we can abbreviate as \\[ \\mathcal J(\\nu) = \\int \\dfrac{(\\nu&#39;)^2}{\\nu} \\, dx \\] Note that Fisher information \\(\\mathcal J_F\\) is originally defined for a one-parameter family; with the notation \\(\\mathcal J(\\nu)\\), we’re assuming the one-parameter family to be the location family of the given density. Theorem 10.2 (van Trees inequality) Under regularity assumptions on \\(P^\\theta\\), for every estimator \\(\\hat \\theta(X)\\) and prior \\(\\pi\\) \\[ \\mathbb E_{\\theta \\sim \\pi} \\mathbb E_{X\\sim P^\\theta} \\left[ [\\theta - \\hat \\theta(X)]^2 \\right] \\geq \\dfrac 1 {\\mathcal J(\\pi) + \\mathbb E_{\\theta \\sim \\pi} \\mathcal J_F(\\theta)} \\] Corollary 10.1 For \\(n\\) i.i.d. observations, for every \\(\\pi\\) \\[ R_n^* \\geq \\dfrac 1 {\\mathcal J(\\pi) + n\\mathbb E_{\\theta \\sim \\pi} \\mathcal J_F(\\theta)} \\] van Trees is important important because it provides a lower bound on minimax risk based on local quantities \\(\\mathcal J_F(\\theta)\\). Two takeaways for information theorists: In Bayesian setting, apply information theory techniques to the joint P_{, X}$. In local neighborhoods, \\(\\chi^2\\) satisfies an additive chain rule (since it’s close to KL). Van Trees replaced \\(\\mathcal J_F\\) with the expectation of \\(\\mathcal J_F\\), and it applies to every (instead of unbiased) estimators. Oct 9: data compression II Overview: Review Distribution of fixed length Arithmetic encoder Stationary, ergodic data sources Lempel-Ziv (adaptive, universal compression) Main takeaways: AEP. Optimal compression is possible using (1) arithmetic encoding and (2) optimal next-token prediction. Information theory is about bounding hard, intractable operational quantities with mathematically analyzable quantities. Review Assume \\(P_X\\) known and ordered \\(P_X(1)\\geq P_X(2)\\geq \\cdots\\). The optimum compressor \\(f^*(x)\\) satisfies \\[ l(f^*(X)) = \\lceil \\log_2 X\\rceil \\] We also proved that the expected optimum compression length satisfies \\[ \\mathbb El(f^*(X)) \\approx H(X) \\] In fact, this is upper-bounded by \\(H(X)\\). But this is not an algorithm! Sorting the distribution is intractible. It’s variable-length but not prefix-free! We’re assuming one-shot compression (comma is for free). Howevver, prefix free code, the optimal expected bound is lower-bounded by \\(H(X)\\). Proof idea: typicality; break region into typical (tractable) and atypical regions with vanishing probability. Asymptotic equipartition property: i.i.d. implies law of large numbers (in log-space). Distribution of compression length is distributed (up to constants) as the entropy density. This is great because entropy density tensorizes. Proposition 10.3 (distribution of compression length) Define random variable \\(L^* = l(f^*(X))\\). Denote the entropy density \\(i_X(a) = -\\log P_X(a)\\); the optimal compressor should compress symbol \\(a\\) to roughly this length: \\[ \\mathrm{Pr}[i(X) \\leq k] \\leq \\mathrm{Pr}[L^* \\leq K] \\leq \\mathrm{Pr}[i(X) \\leq k + \\tau] + 2^{1-\\tau} \\] This holds for every \\(k\\in \\mathbb Z_+, \\tau&gt;0\\). Proof: Left-bound is easy: \\[ L^*(x) = \\lceil \\log_2 X\\rceil \\leq \\log 2 |X| \\leq -\\log P_X(x) = i_X(x) \\] To bound the second term, decompose the probability \\[\\begin{align} \\mathrm{Pr}[L^* \\leq k] &amp;= \\mathrm{Pr}[L^* \\leq k, i(x) \\leq k+\\tau] + \\mathrm{Pr}[L^* \\leq k, i(x) &gt; k+\\tau] \\\\ &amp;\\leq \\mathrm{Pr}[i_X(x) \\leq k+\\tau] + (\\cdots) \\end{align}\\] The second term is bounded by the number of strings which achieves this \\(2^{k+1}\\) times the maximum probability they’re obtaining \\(2^{-k-\\tau}\\), yielding \\(2^{1-\\tau}\\). Corrolary: if, for some sequence of r.v., the normalized entropy rate converges in distribution to \\(U\\), then the normalized optimal compression length (for asymptotically large block length) also converges to \\(U\\). Another corollary: if \\(S_j\\sim P_S\\) i.i.d., then \\[ \\dfrac 1 n i_{S_1^n}(S_1^n) = -\\dfrac 1 n \\log P_{S^n}(S^n) = -\\dfrac 1 n \\sum_{j=1}^n \\log P_S(s_j) \\to H(S) \\] This implies that the expectation of the optimal compression length for i.i.d. source \\(X\\) converges to \\(H(X)\\) in the limit of asymptotically large block lengths. This is a nontrivial result, because the optimal compressor is a very freely-specified object, but we are able to bound its behavior very neatly. In particular, recall that the optimal-compressor maps highest-probability atoms to the empty set, but we see from the asymptotic Gaussian distribution of the compression length that they have almost negligible density. This is a demonstration of the asymptotic equipartition property, which states that for i.i.d. sources, the overwhelming number of sequences have the same probability given by \\(e^{H(X)}\\). Arithmetic encoder Given a general \\(\\{X_j\\}\\)-process and wishing to compress \\(X_1^n\\). First consider i.i.d process. Order the alphabet and recursively partition the interval \\([0, 1]\\) so that each interval has length equal to its probability. The trick is to find the largest dyadic interval (recursive binary partition) that fits inside the interval of the message. For example: \\[ [0, 1]\\to \\emptyset, \\quad [6\\cdot 2^{-3}, 7\\cdot 2^{-3}]\\to 110 \\] In the limit that the codestring goes to infinity, the distribution of binary expansion will be uniform. Fact for the compression length of arithmetic encoder for i.i.d. terms: \\[ \\log \\dfrac 1 {P_{X^n}(x^n)} \\leq l(f_{\\mathrm{ae}}(x^n) \\leq \\log_2 \\dfrac 1 {P_{X^n}(x^n)} + 2 \\] The arithmetic encoder is additionally sequential: it does not need to consume the full string to start outputting compression; the same holds for the decompressor. Implementing the arithmetic encoder for general non-i.i.d. distributions just replace subsequent intervals by the marginals \\(P_{X_n \\| X^{(n-1)}}\\). This means that, if we can sequentially predict the marginal \\(P_{X_n \\|X^{(n-1)}}\\) very well (e.g. next-token prediction LLM), then we can close-to-optimal compress a non-i.i.d. distribution by combining this with the arithmetic encoder. Theorem 10.3 (Shannon-McMillan-Breimen) A.e.p. holds w.r.t. the entropy rate if \\(\\{X_j\\}\\) is a stationary (ensures existence of the entropy rate) ergodic process. Shannon’s proof: every stationary ergodic process can be arbitrarily approximated by a \\(m\\to \\infty\\)-order Markov chain. Lempel-Ziv Central question: how to compress well without \\(P_{X^n}\\)? Lemma 10.1 (Katz's lemma) Given a stationary ergodic process \\((X_{j\\in \\mathbb Z})\\). Define \\(L = \\inf\\{t&gt;0: X_{-t}=X_0\\}\\), then \\[ \\mathbb E[L | X_0=u] = \\mathrm{Pr}[X_0=u]^{-1} \\] Proof: consider the probability that we don’t see \\(u\\) when we look back for \\(k\\) steps, using stationarity: \\[\\begin{align} \\mathrm{Pr}[L&gt;K, X_0=u] &amp;= \\mathrm{Pr}[X_0=u, X_{-1}\\neq U, \\cdots, X_{-k}\\neq u] \\\\ &amp;= \\mathrm{Pr}[X_k=u, X_{k-1}\\neq U, \\cdots, X_0\\neq u] = \\mathrm{Pr}[E_k] \\end{align}\\] todo Another key point is that for stationary ergodic processes, \\(\\mathrm{Pr}[\\bigcup E_k] = 1\\). Using Katz’s lemma, we can do an unbiased estimation of \\(\\mathrm{Pr}[X_0=u]\\) by looking back. Do probability mixtures satisfy all of the local Fisher regularity conditions? "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
