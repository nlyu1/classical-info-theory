<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Lecture notes | 6.7480 Notes</title>
  <meta name="description" content="9 Lecture notes | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Lecture notes | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Lecture notes | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-10-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-compression.html"/>
<link rel="next" href="bibliography.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics and geometry</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#brégmans-theorem"><i class="fa fa-check"></i>Brégman’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="divergence.html"><a href="divergence.html"><i class="fa fa-check"></i><b>3</b> Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#kl-divergence"><i class="fa fa-check"></i>KL-Divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#sufficient-statistic"><i class="fa fa-check"></i>Sufficient statistic</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#entropy-power-inequality"><i class="fa fa-check"></i>Entropy-power Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html"><i class="fa fa-check"></i><b>5</b> Variational Measures of Information</a>
<ul>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#geometric-interpretations-of-mi"><i class="fa fa-check"></i>Geometric interpretations of MI</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#convexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#gaussian-saddle-point"><i class="fa fa-check"></i>Gaussian saddle point</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations"><i class="fa fa-check"></i>Variational characterizations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-compression.html"><a href="data-compression.html"><i class="fa fa-check"></i>Data compression</a></li>
<li class="chapter" data-level="9" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>9</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture-notes" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Lecture notes<a href="lecture-notes.html#lecture-notes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="oct-2-fisher-information-classical-minimax-estimation" class="section level2 unnumbered hasAnchor">
<h2>Oct 2: Fisher information, classical minimax estimation<a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\chi^2\)</span> variational characterization.</li>
<li><span class="math inline">\(1\)</span>-parameter families; minimax rate.</li>
<li>HCR, and Fisher information.</li>
<li>Cramer-Rau; van Trees inequality.</li>
</ol>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R_n^*\)</span> is the minimax rate of parameterization.</li>
<li>LeCam-Hajek theory: <span class="math inline">\(R_n^* = \dfrac{1+o(1)}{n \min_\theta I_F(\theta)}\)</span>.</li>
<li>To obtain more well-behaved inequalities, expand a single extremum
into a nested one (e.g. scalar multiple) then solve the closed form
of the inner optimization.</li>
<li>All <span class="math inline">\(f\)</span>-divergences are locally quadratic in parameteric families
with Hessian given by Fisher information.</li>
<li>Cramer-Rau is an application of the variational characterization
of <span class="math inline">\(\chi^2\)</span>.</li>
</ol>
<div id="main-content" class="section level3 unnumbered hasAnchor">
<h3>Main content<a href="lecture-notes.html#main-content" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recalling the variational characterization:
<span class="math display">\[
    D_f(P\|Q) = \sup_g \mathbb E_p g - \mathbb E_Q f^*\circ g
\]</span>
where the convex conjugate is given by
<span class="math display">\[
    f^*(h) = \sup_{t\in \mathbb R} th - f(t), \quad
    f(t) = \sup_{h\in \mathbb R} th - f^*(h)
\]</span>
Recall that Donsker-varadhan is not linear in <span class="math inline">\(\mathbb E_Q\)</span>,
but there is a standard trick to rewrite the expectation.</p>
<p>We now apply the variational characterization to <span class="math inline">\(\chi^2\)</span>;</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-72" class="proposition"><strong>Proposition 9.1  (variational characterization of χ²) </strong></span><span class="math inline">\(\chi^2(P \|Q)
= \sup_{h:\mathcal X\to \mathbb R} \mathbb E_p h(X) - \mathbb E_Q \left[
    h(X) + \dfrac{h(X)^2}{4}
\right]\)</span></p>
</div>
<p>Expanding this out, the first term is very useful;
but the second is not. The first step is to break a single
extrema into two:
<span class="math display">\[\begin{align}
    \chi^2(P \|Q)
    &amp;= \sup_h \left[
        \mathbb E_p h - \mathbb E_Q h
    \right] - \dfrac 1 4 \mathbb E_Q h^2 \\
    &amp;= \sup_g \sup_{\lambda\in \mathbb R} \lambda \left(
        \mathbb E_P g - \mathbb E_Q g
    \right) - \dfrac{\lambda^2}{4} \mathbb E_Q g^2  
    = \sup_g \dfrac{(\mathbb E_p g - \mathbb E_Q g)^2}{\mathbb E_q g^2}
\end{align}\]</span>
As a consequence, we obtain
<span class="math display">\[
    \left(
        \mathbb E_P g - \mathbb E_Q g
    \right)^2 \leq \chi^2(P \|Q) \mathbb E_Q g^2
\]</span>
In fact, this equation is invariant under <span class="math inline">\(g\mapsto g + c\)</span>, yielding
<span class="math display">\[
    \left(
        \mathbb E_P g - \mathbb E_Q g
    \right)^2 \leq \chi^2(P \|Q) \mathrm{Var}_Q g^2
\]</span></p>
<p><span style="color:green">
Exercise: <span class="math inline">\(\forall g&gt;0\)</span>, we have
<span class="math inline">\(\mathbb E_P g \leq 2\mathbb E_Q g + 2_? H^2(P, Q)\)</span>.
</span></p>
</div>
<div id="one-parameter-families-minimax-rates" class="section level3 unnumbered hasAnchor">
<h3>One-parameter families; minimax rates<a href="lecture-notes.html#one-parameter-families-minimax-rates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statisticians care about sub-manifolds
of the probability simplex.</p>
<p>For one-parameter families, we typically have
<span class="math display">\[
    P^\theta(dx) = p^\theta(x) \mu(dx)
\]</span>
For discrete, <span class="math inline">\(\mu\)</span> is counting;
for real-valued, <span class="math inline">\(\mu\)</span> is Lebesgue.</p>
<p><strong>Parameter estimation</strong>: given <span class="math inline">\(X_1, \cdots, X_n \sim P^\theta\)</span>;
find function <span class="math inline">\(\hat \theta(X_1, \cdots, X_n)\)</span> such that
<span class="math inline">\(\hat \theta \approx \theta\)</span>; here <span class="math inline">\(\approx\)</span> is defined w.r.t.
a risk function
<span class="math display">\[
    R(\hat \theta, \theta) = \mathbb E_{X^n} (\theta - \hat \theta(X^n))^2
\]</span>
Note that the first argument is a function, while the second is a
parameter coordinate.</p>
<p>Example: <span class="math inline">\(\hat \theta_0 = \dfrac 1 n \sum X_j\)</span> for Bernoulli
parameter estimation; we can compute
<span class="math display">\[
    R(\hat \theta, \theta) = \dfrac{\theta \bar \theta}{n}
\]</span>
Recall that it’s MLE and unbiased. To get mid of the
<span class="math inline">\(\theta\)</span>-dependence, we can consider
<span class="math display">\[
    R^{\mathrm{max}}(\hat \theta)
    = \sup_\theta R(\hat \theta_1, \theta)
\]</span></p>
<p>Consider a naive estimator <span class="math inline">\(\hat \theta_1 = 1/2\)</span>
(this has definite bias but not variance) and
<span class="math display">\[
    \hat \theta_\lambda = \lambda \hat \theta_1
    + \bar \lambda \hat \theta_{\mathrm{MLE}}
\]</span>
One can compute risk = bias^2 +variance^2
and choose an optimal <span class="math inline">\(\lambda_n^*= \dfrac 1 {1 + \sqrt n}\)</span>;
this allows one to optimize the risk everywhere.</p>
<p><strong>Theorem</strong> The optimal shrinkage estimator
<span class="math inline">\(\hat \lambda_{\lambda_n^*}\)</span> saturates the minimax risk
<span class="math display">\[
    \inf_{\hat \theta} R^{\mathrm{max}}(\hat \theta)
    = \dfrac 1 {4(1 + \sqrt n)^2}
\]</span>
<em>Key idea:</em> obtain MLE; identify points of worst
performance (minimum Fisher information), then bias
towards it.</p>
<p>The optimal minimax risk profile (minimax rate) is
<span class="math display">\[
    R_n^* = \inf_{\hat \theta} \sup_\theta R(\hat \theta, \theta)
\]</span></p>
</div>
<div id="hcr-inequality-fisher-information" class="section level3 unnumbered hasAnchor">
<h3>HCR inequality; Fisher information<a href="lecture-notes.html#hcr-inequality-fisher-information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proposition">
<p><span id="prp:unlabeled-div-73" class="proposition"><strong>Proposition 9.2  </strong></span><span class="math inline">\(\forall \hat \theta\)</span> and <span class="math inline">\(\theta, \theta_0\)</span> we have
<span class="math display">\[
    R_n^* \geq \mathrm{Var}_{P^\theta} \geq \dfrac{
        \left(\mathbb E_{P^\theta} \hat \theta
        - \mathbb E_{P^{\theta_0}} \hat \theta\right)^2
    }{
    \chi^2(P^{\theta \otimes n}\| P^{\theta_0 \otimes n})
    }
\]</span></p>
</div>
<p><em>Proof:</em> The first inequality follows from
bias-variance decomposition. For the second, take
<span class="math inline">\(g(X^n) = \hat \theta(X^n)\)</span> in the variational characterization
of <span class="math inline">\(\chi^2\)</span>.</p>
<p>To bring Fisher information into the picture, consider
<span class="math display">\[
    \chi^2 (P^\theta \| P^{\theta_0})
    = \sum \dfrac{P^\theta(x)^2}{P^{\theta_0}(x)} - 1
\]</span>
Take <span class="math inline">\(\theta = \theta_0 + \epsilon\)</span> for small <span class="math inline">\(\epsilon\)</span>;
Taylor-expand to obtain
<span class="math display">\[\begin{align}
    P^\theta(x) = P^{\theta_0}(x)
    + \partial_{\theta }P^\theta \big|_{\theta = \theta_0} (x)\epsilon +
    \cdots
\end{align}\]</span>
Substitute this into the expression for <span class="math inline">\(\chi^2\)</span> to obtain
<span class="math display">\[
    \chi^2 = \sum_x \dfrac{\left[
        P^{\theta_0}(x) + \epsilon \partial_{\theta }P^\theta(x)
    \right]^2}{P^{\theta_0}(x)} - 1 + o(\epsilon^2)
\]</span>
Expanding the term, the linear term is <span class="math inline">\(0\)</span> (pull <span class="math inline">\(\partial_{\theta}\)</span> out):
<span class="math display">\[
    \sum_x \partial_{\theta }P^{\theta}(x) \big|_{\theta = \theta_0}
    = 0
\]</span>
The null term sums to <span class="math inline">\(1\)</span>, cancelling the <span class="math inline">\(-1\)</span>, yielding the
local definition of <span class="math inline">\(\chi^2\)</span>
<span class="math display">\[
    \chi^2 = \epsilon^2 \sum_x
    \dfrac{\left[\partial_{\theta }P^{\theta_0}(x)\right]^2}
    {P^{\theta_0}(x)} + o(\epsilon^2)
\]</span></p>
<p>Recall that the <strong>Fisher information</strong> is given by
<span class="math display">\[
    \mathcal J_F(\theta; \{P^\theta\}_{\theta \in \Theta})
    = \int \dfrac{\left[\partial_{\theta }P^\theta(x)\right]^2}{P^\theta(x)} \, \mu(dx)
\]</span>
Fisher information is just the second-order derivative
of <span class="math inline">\(\chi^2\)</span>.</p>
<p>There are two notions of distance:
Fisher information tells us, when we nudge
<span class="math inline">\(\theta \mapsto \theta + \epsilon\)</span>,
how much is the statistical distance between <span class="math inline">\(P^\theta\)</span>
and <span class="math inline">\(P^{\theta + \epsilon}\)</span>.</p>
<p>Moreover, Fisher information is additive under
tensorization: KL is additive under tensorization
and apply the locality principle.</p>
<p>Theorems about Fisher information require a lot
more assumptions.</p>
</div>
</div>
<div id="oct-7-data-compression" class="section level2 unnumbered hasAnchor">
<h2>Oct 7: Data compression<a href="lecture-notes.html#oct-7-data-compression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-74" class="theorem"><strong>Theorem 9.1  (local expansion of χ) </strong></span>For <span class="math inline">\(\theta_0 = 0\)</span>, when the following conditions hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\theta \in [0, \tau)\)</span>.</li>
<li>There exists <span class="math inline">\(\dot p^\theta(x)\)</span> satisfying
<span class="math display">\[
P^\theta(x) = P^0(x) + \int_0^\theta \dot p^t(x)\, dt
\]</span></li>
<li>The fisher information is defined <em>everywhere</em>
in a connected interval open interval about <span class="math inline">\(\theta_0\)</span>:
<span class="math display">\[
\int dx\, \sup_{0\leq t &lt; \tau}
\dfrac{\dot p^t(x)^2}{p^0(x)} &lt; \infty
\]</span></li>
</ol>
<p>Then <span class="math inline">\(\chi^2(P^{\theta_0 + \delta} \| P^{\theta_0})
    + \delta^2 \mathcal J_F(\theta_0) + o(\delta^2)\)</span>.</p>
</div>
<p>The quadratic local behavior of <span class="math inline">\(\chi^2\)</span> requires
not only finite Fisher information at <span class="math inline">\(\theta_0\)</span>;
it also requires <span class="math inline">\(\mathcal J_F\)</span> to be finite almost everywhere
on a nontrivial interval.</p>
<p><span style="color:green">
In summary, <span class="math inline">\(\chi^2\)</span> is nice for the “good” cases
where <span class="math inline">\(\mathcal J\)</span> is finite a.e. However, for some
irregular cases it’s best to use
<span class="math inline">\(H^2(P^{\theta_0 + \delta}, P^\theta)\)</span>, whose local
behavior is always determined by the local Fisher information.
</span></p>
<p>A location family is a one-parameter family for which
the parameter controls the displacement of a fixed density <span class="math inline">\(\nu\)</span>,
e.g. <span class="math inline">\(\mathcal N(\theta, 1)\)</span>; for this family,
<span class="math inline">\(\mathcal J(\theta) = \mathcal J(0)\)</span>, which we can abbreviate as
<span class="math display">\[
    \mathcal J(\nu) = \int \dfrac{(\nu&#39;)^2}{\nu} \, dx
\]</span>
Note that Fisher information <span class="math inline">\(\mathcal J_F\)</span> is originally defined for
a one-parameter family; with the notation <span class="math inline">\(\mathcal J(\nu)\)</span>,
we’re assuming the one-parameter family to be the location family
of the given density.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-75" class="theorem"><strong>Theorem 9.2  (van Trees inequality) </strong></span>Under regularity assumptions on <span class="math inline">\(P^\theta\)</span>,
for every estimator <span class="math inline">\(\hat \theta(X)\)</span> and prior
<span class="math inline">\(\pi\)</span>
<span class="math display">\[
    \mathbb E_{\theta \sim \pi} \mathbb E_{X\sim P^\theta} \left[
        [\theta - \hat \theta(X)]^2
    \right] \geq \dfrac 1 {\mathcal J(\pi) +
    \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\]</span></p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-76" class="corollary"><strong>Corollary 9.1  </strong></span>For <span class="math inline">\(n\)</span> i.i.d. observations, for every <span class="math inline">\(\pi\)</span>
<span class="math display">\[
    R_n^* \geq \dfrac 1 {\mathcal J(\pi) +
    n\mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\]</span></p>
</div>
<p>van Trees is important important because it provides
a lower bound on minimax risk based on
local quantities <span class="math inline">\(\mathcal J_F(\theta)\)</span>.</p>
<p>Two takeaways for information theorists:</p>
<ol style="list-style-type: decimal">
<li>In Bayesian setting, apply information theory techniques
to the joint P_{, X}$.</li>
<li>In local neighborhoods, <span class="math inline">\(\chi^2\)</span> satisfies an additive
chain rule (since it’s close to KL).</li>
</ol>
<p>Van Trees replaced <span class="math inline">\(\mathcal J_F\)</span> with the expectation of
<span class="math inline">\(\mathcal J_F\)</span>, and it applies to every (instead of unbiased) estimators.</p>
<p>Back to compression:</p>
<div class="definition">
<p><span id="def:unlabeled-div-77" class="definition"><strong>Definition 9.1  (lossless compression) </strong></span>Given a discrete alphabet <span class="math inline">\(\mathcal X\)</span> a compressor
has signature <span class="math inline">\(f:\mathcal X\to \{0, 1\}^*\)</span>.
It is lossless w.r.t. <span class="math inline">\(P_X\)</span> if <span class="math inline">\(\exists g:\{0, 1\}^*\to \mathcal X\)</span>
such that <span class="math inline">\(g\circ f = 1\)</span> almost everywhere w.r.t. <span class="math inline">\(P_X\)</span>.</p>
</div>
<p>The optimal lossless compressor is apparantly given by
<span class="math inline">\(1\to \emptyset, 2\to 0, 3\to 1, 4\to 00, \cdots\)</span>.
where <span class="math inline">\(j\)</span> is the <span class="math inline">\(j\)</span>-th most frequent occurence.</p>
<p>Exercise: compute
<span class="math display">\[
    \sup_{Q \text{ on } \N}
    \{H(Q): \sum_{l=0}^\infty l Q(l) \leq \mu \}
\]</span>
Solution: <span class="math inline">\((1 + \mu)h(1/(1+\mu))\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-compression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliography.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
