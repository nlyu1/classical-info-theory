<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14 Lecture notes | 6.7480 Notes</title>
  <meta name="description" content="14 Lecture notes | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="14 Lecture notes | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14 Lecture notes | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-10-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="universal-compression.html"/>
<link rel="next" href="bibliography.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#todo-list"><i class="fa fa-check"></i>Todo list</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#entropy-power-inequality"><i class="fa fa-check"></i>Entropy-power Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#source-coding-theorems"><i class="fa fa-check"></i>Source coding theorems</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="fixed-length-compression.html"><a href="fixed-length-compression.html"><i class="fa fa-check"></i><b>11</b> Fixed-length compression</a>
<ul>
<li class="chapter" data-level="" data-path="fixed-length-compression.html"><a href="fixed-length-compression.html#source-coding-theorems-1"><i class="fa fa-check"></i>Source coding theorems</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="entropy-of-ergodic-processes.html"><a href="entropy-of-ergodic-processes.html"><i class="fa fa-check"></i><b>12</b> Entropy of Ergodic Processes</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-of-ergodic-processes.html"><a href="entropy-of-ergodic-processes.html#stochasticProcessPrelim"><i class="fa fa-check"></i>Preliminaries</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="universal-compression.html"><a href="universal-compression.html"><i class="fa fa-check"></i><b>13</b> Universal compression</a>
<ul>
<li class="chapter" data-level="" data-path="universal-compression.html"><a href="universal-compression.html#arithmetic-encoding"><i class="fa fa-check"></i>Arithmetic encoding</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>14</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture-notes" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">14</span> Lecture notes<a href="lecture-notes.html#lecture-notes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="oct-2-fisher-information-classical-minimax-estimation" class="section level2 unnumbered hasAnchor">
<h2>Oct 2: Fisher information, classical minimax estimation<a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\chi^2\)</span> variational characterization.</li>
<li><span class="math inline">\(1\)</span>-parameter families; minimax rate.</li>
<li>HCR, and Fisher information.</li>
<li>Cramer-Rau; van Trees inequality.</li>
</ol>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R_n^*\)</span> is the minimax rate of parameterization.</li>
<li>LeCam-Hajek theory: <span class="math inline">\(R_n^* = \dfrac{1+o(1)}{n \min_\theta I_F(\theta)}\)</span>.</li>
<li>To obtain more well-behaved inequalities, expand a single extremum
into a nested one (e.g. scalar multiple) then solve the closed form
of the inner optimization.</li>
<li>All <span class="math inline">\(f\)</span>-divergences are locally quadratic in parameteric families
with Hessian given by Fisher information.</li>
<li>Cramer-Rau is an application of the variational characterization
of <span class="math inline">\(\chi^2\)</span>.</li>
</ol>
<div id="main-content" class="section level3 unnumbered hasAnchor">
<h3>Main content<a href="lecture-notes.html#main-content" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recalling the variational characterization:
<span class="math display">\[
    D_f(P\|Q) = \sup_g \mathbb E_p g - \mathbb E_Q f^*\circ g
\]</span>
where the convex conjugate is given by
<span class="math display">\[
    f^*(h) = \sup_{t\in \mathbb R} th - f(t), \quad
    f(t) = \sup_{h\in \mathbb R} th - f^*(h)
\]</span>
Recall that Donsker-varadhan is not linear in <span class="math inline">\(\mathbb E_Q\)</span>,
but there is a standard trick to rewrite the expectation.</p>
<p>We now apply the variational characterization to <span class="math inline">\(\chi^2\)</span>;</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-94" class="proposition"><strong>Proposition 14.1  (variational characterization of χ²) </strong></span><span class="math inline">\(\chi^2(P \|Q) = \sup_{h:\mathcal X\to \mathbb R} \mathbb E_p h(X) - \mathbb E_Q \left[  h(X) + \dfrac{h(X)^2}{4} \right]\)</span></p>
</div>
<p>Expanding this out, the first term is very useful;
but the second is not. The first step is to break a single
extrema into two:
<span class="math display">\[\begin{align}
    \chi^2(P \|Q)
    &amp;= \sup_h \left[
        \mathbb E_p h - \mathbb E_Q h
    \right] - \dfrac 1 4 \mathbb E_Q h^2 \\
    &amp;= \sup_g \sup_{\lambda\in \mathbb R} \lambda \left(
        \mathbb E_P g - \mathbb E_Q g
    \right) - \dfrac{\lambda^2}{4} \mathbb E_Q g^2  
    = \sup_g \dfrac{(\mathbb E_p g - \mathbb E_Q g)^2}{\mathbb E_q g^2}
\end{align}\]</span>
As a consequence, we obtain
<span class="math display">\[
    \left(
        \mathbb E_P g - \mathbb E_Q g
    \right)^2 \leq \chi^2(P \|Q) \mathbb E_Q g^2
\]</span>
In fact, this equation is invariant under <span class="math inline">\(g\mapsto g + c\)</span>, yielding
<span class="math display">\[
    \left(
        \mathbb E_P g - \mathbb E_Q g
    \right)^2 \leq \chi^2(P \|Q) \mathrm{Var}_Q g^2
\]</span></p>
<p><span style="color:green">
Exercise: <span class="math inline">\(\forall g&gt;0\)</span>, we have
<span class="math inline">\(\mathbb E_P g \leq 2\mathbb E_Q g + 2_? H^2(P, Q)\)</span>.
</span></p>
</div>
<div id="one-parameter-families-minimax-rates" class="section level3 unnumbered hasAnchor">
<h3>One-parameter families; minimax rates<a href="lecture-notes.html#one-parameter-families-minimax-rates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statisticians care about sub-manifolds
of the probability simplex.</p>
<p>For one-parameter families, we typically have
<span class="math display">\[
    P^\theta(dx) = p^\theta(x) \mu(dx)
\]</span>
For discrete, <span class="math inline">\(\mu\)</span> is counting;
for real-valued, <span class="math inline">\(\mu\)</span> is Lebesgue.</p>
<p><strong>Parameter estimation</strong>: given <span class="math inline">\(X_1, \cdots, X_n \sim P^\theta\)</span>;
find function <span class="math inline">\(\hat \theta(X_1, \cdots, X_n)\)</span> such that
<span class="math inline">\(\hat \theta \approx \theta\)</span>; here <span class="math inline">\(\approx\)</span> is defined w.r.t.
a risk function
<span class="math display">\[
    R(\hat \theta, \theta) = \mathbb E_{X^n} (\theta - \hat \theta(X^n))^2
\]</span>
Note that the first argument is a function, while the second is a
parameter coordinate.</p>
<p>Example: <span class="math inline">\(\hat \theta_0 = \dfrac 1 n \sum X_j\)</span> for Bernoulli
parameter estimation; we can compute
<span class="math display">\[
    R(\hat \theta, \theta) = \dfrac{\theta \bar \theta}{n}
\]</span>
Recall that it’s MLE and unbiased. To get mid of the
<span class="math inline">\(\theta\)</span>-dependence, we can consider
<span class="math display">\[
    R^{\mathrm{max}}(\hat \theta)
    = \sup_\theta R(\hat \theta_1, \theta)
\]</span></p>
<p>Consider a naive estimator <span class="math inline">\(\hat \theta_1 = 1/2\)</span>
(this has definite bias but not variance) and
<span class="math display">\[
    \hat \theta_\lambda = \lambda \hat \theta_1
    + \bar \lambda \hat \theta_{\mathrm{MLE}}
\]</span>
One can compute risk = bias^2 +variance^2
and choose an optimal <span class="math inline">\(\lambda_n^*= \dfrac 1 {1 + \sqrt n}\)</span>;
this allows one to optimize the risk everywhere.</p>
<p><strong>Theorem</strong> The optimal shrinkage estimator
<span class="math inline">\(\hat \lambda_{\lambda_n^*}\)</span> saturates the minimax risk
<span class="math display">\[
    \inf_{\hat \theta} R^{\mathrm{max}}(\hat \theta)
    = \dfrac 1 {4(1 + \sqrt n)^2}
\]</span>
<em>Key idea:</em> obtain MLE; identify points of worst
performance (minimum Fisher information), then bias
towards it.</p>
<p>The optimal minimax risk profile (minimax rate) is
<span class="math display">\[
    R_n^* = \inf_{\hat \theta} \sup_\theta R(\hat \theta, \theta)
\]</span></p>
</div>
<div id="hcr-inequality-fisher-information" class="section level3 unnumbered hasAnchor">
<h3>HCR inequality; Fisher information<a href="lecture-notes.html#hcr-inequality-fisher-information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proposition">
<p><span id="prp:unlabeled-div-95" class="proposition"><strong>Proposition 14.2  </strong></span><span class="math inline">\(\forall \hat \theta\)</span> and <span class="math inline">\(\theta, \theta_0\)</span> we have
<span class="math display">\[
    R_n^* \geq \mathrm{Var}_{P^\theta} \geq \dfrac{
        \left(\mathbb E_{P^\theta} \hat \theta
        - \mathbb E_{P^{\theta_0}} \hat \theta\right)^2
    }{
    \chi^2(P^{\theta \otimes n}\| P^{\theta_0 \otimes n})
    }
\]</span></p>
</div>
<p><em>Proof:</em> The first inequality follows from
bias-variance decomposition. For the second, take
<span class="math inline">\(g(X^n) = \hat \theta(X^n)\)</span> in the variational characterization
of <span class="math inline">\(\chi^2\)</span>.</p>
<p>To bring Fisher information into the picture, consider
<span class="math display">\[
    \chi^2 (P^\theta \| P^{\theta_0})
    = \sum \dfrac{P^\theta(x)^2}{P^{\theta_0}(x)} - 1
\]</span>
Take <span class="math inline">\(\theta = \theta_0 + \epsilon\)</span> for small <span class="math inline">\(\epsilon\)</span>;
Taylor-expand to obtain
<span class="math display">\[\begin{align}
    P^\theta(x) = P^{\theta_0}(x)
    + \partial_{\theta }P^\theta \big|_{\theta = \theta_0} (x)\epsilon +
    \cdots
\end{align}\]</span>
Substitute this into the expression for <span class="math inline">\(\chi^2\)</span> to obtain
<span class="math display">\[
    \chi^2 = \sum_x \dfrac{\left[
        P^{\theta_0}(x) + \epsilon \partial_{\theta }P^\theta(x)
    \right]^2}{P^{\theta_0}(x)} - 1 + o(\epsilon^2)
\]</span>
Expanding the term, the linear term is <span class="math inline">\(0\)</span> (pull <span class="math inline">\(\partial_{\theta}\)</span> out):
<span class="math display">\[
    \sum_x \partial_{\theta }P^{\theta}(x) \big|_{\theta = \theta_0}
    = 0
\]</span>
The null term sums to <span class="math inline">\(1\)</span>, cancelling the <span class="math inline">\(-1\)</span>, yielding the
local definition of <span class="math inline">\(\chi^2\)</span>
<span class="math display">\[
    \chi^2 = \epsilon^2 \sum_x
    \dfrac{\left[\partial_{\theta }P^{\theta_0}(x)\right]^2}
    {P^{\theta_0}(x)} + o(\epsilon^2)
\]</span></p>
<p>Recall that the <strong>Fisher information</strong> is given by
<span class="math display">\[
    \mathcal J_F(\theta; \{P^\theta\}_{\theta \in \Theta})
    = \int \dfrac{\left[\partial_{\theta }P^\theta(x)\right]^2}{P^\theta(x)} \, \mu(dx)
\]</span>
Fisher information is just the second-order derivative
of <span class="math inline">\(\chi^2\)</span>.</p>
<p>There are two notions of distance:
Fisher information tells us, when we nudge
<span class="math inline">\(\theta \mapsto \theta + \epsilon\)</span>,
how much is the statistical distance between <span class="math inline">\(P^\theta\)</span>
and <span class="math inline">\(P^{\theta + \epsilon}\)</span>.</p>
<p>Moreover, Fisher information is additive under
tensorization: KL is additive under tensorization
and apply the locality principle.</p>
<p>Theorems about Fisher information require a lot
more assumptions.</p>
</div>
</div>
<div id="oct-7-data-compression" class="section level2 unnumbered hasAnchor">
<h2>Oct 7: Data compression<a href="lecture-notes.html#oct-7-data-compression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-96" class="theorem"><strong>Theorem 14.1  (local expansion of χ) </strong></span>For <span class="math inline">\(\theta_0 = 0\)</span>, when the following conditions hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\theta \in [0, \tau)\)</span>.</li>
<li>There exists <span class="math inline">\(\dot p^\theta(x)\)</span> satisfying
<span class="math display">\[
P^\theta(x) = P^0(x) + \int_0^\theta \dot p^t(x)\, dt
\]</span></li>
<li>The fisher information is defined <em>everywhere</em>
in a connected interval open interval about <span class="math inline">\(\theta_0\)</span>:
<span class="math display">\[
\int dx\, \sup_{0\leq t &lt; \tau}
\dfrac{\dot p^t(x)^2}{p^0(x)} &lt; \infty
\]</span></li>
</ol>
<p>Then <span class="math inline">\(\chi^2(P^{\theta_0 + \delta} \| P^{\theta_0})  + \delta^2 \mathcal J_F(\theta_0) + o(\delta^2)\)</span>.</p>
</div>
<p>The quadratic local behavior of <span class="math inline">\(\chi^2\)</span> requires
not only finite Fisher information at <span class="math inline">\(\theta_0\)</span>;
it also requires <span class="math inline">\(\mathcal J_F\)</span> to be finite almost everywhere
on a nontrivial interval.</p>
<p><span style="color:green">
In summary, <span class="math inline">\(\chi^2\)</span> is nice for the “good” cases
where <span class="math inline">\(\mathcal J\)</span> is finite a.e. However, for some
irregular cases it’s best to use
<span class="math inline">\(H^2(P^{\theta_0 + \delta}, P^\theta)\)</span>, whose local
behavior is always determined by the local Fisher information.
</span></p>
<p>A location family is a one-parameter family for which
the parameter controls the displacement of a fixed density <span class="math inline">\(\nu\)</span>,
e.g. <span class="math inline">\(\mathcal N(\theta, 1)\)</span>; for this family,
<span class="math inline">\(\mathcal J(\theta) = \mathcal J(0)\)</span>, which we can abbreviate as
<span class="math display">\[
    \mathcal J(\nu) = \int \dfrac{(\nu&#39;)^2}{\nu} \, dx
\]</span>
Note that Fisher information <span class="math inline">\(\mathcal J_F\)</span> is originally defined for
a one-parameter family; with the notation <span class="math inline">\(\mathcal J(\nu)\)</span>,
we’re assuming the one-parameter family to be the location family
of the given density.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-97" class="theorem"><strong>Theorem 14.2  (van Trees inequality) </strong></span>Under regularity assumptions on <span class="math inline">\(P^\theta\)</span>,
for every estimator <span class="math inline">\(\hat \theta(X)\)</span> and prior
<span class="math inline">\(\pi\)</span>
<span class="math display">\[
    \mathbb E_{\theta \sim \pi} \mathbb E_{X\sim P^\theta} \left[
        [\theta - \hat \theta(X)]^2
    \right] \geq \dfrac 1 {\mathcal J(\pi) +
    \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\]</span></p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-98" class="corollary"><strong>Corollary 14.1  </strong></span>For <span class="math inline">\(n\)</span> i.i.d. observations, for every <span class="math inline">\(\pi\)</span>
<span class="math display">\[
    R_n^* \geq \dfrac 1 {\mathcal J(\pi) +
    n\mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\]</span></p>
</div>
<p>van Trees is important important because it provides
a lower bound on minimax risk based on
local quantities <span class="math inline">\(\mathcal J_F(\theta)\)</span>.</p>
<p>Two takeaways for information theorists:</p>
<ol style="list-style-type: decimal">
<li>In Bayesian setting, apply information theory techniques
to the joint P_{, X}$.</li>
<li>In local neighborhoods, <span class="math inline">\(\chi^2\)</span> satisfies an additive
chain rule (since it’s close to KL).</li>
</ol>
<p>Van Trees replaced <span class="math inline">\(\mathcal J_F\)</span> with the expectation of
<span class="math inline">\(\mathcal J_F\)</span>, and it applies to every (instead of unbiased) estimators.</p>
</div>
<div id="oct-9-data-compression-ii" class="section level2 unnumbered hasAnchor">
<h2>Oct 9: data compression II<a href="lecture-notes.html#oct-9-data-compression-ii" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Overview:</p>
<ol style="list-style-type: decimal">
<li>Review</li>
<li>Distribution of fixed length</li>
<li>Arithmetic encoder</li>
<li>Stationary, ergodic data sources</li>
<li>Lempel-Ziv (adaptive, universal compression)</li>
</ol>
<p>Main takeaways:</p>
<ol style="list-style-type: decimal">
<li>AEP.</li>
<li>Optimal compression is possible using (1) arithmetic encoding
and (2) optimal next-token prediction.</li>
</ol>
<p>Information theory is about bounding
hard, intractable operational quantities with mathematically
analyzable quantities.</p>
<div id="review" class="section level3 unnumbered hasAnchor">
<h3>Review<a href="lecture-notes.html#review" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume <span class="math inline">\(P_X\)</span> known and ordered <span class="math inline">\(P_X(1)\geq P_X(2)\geq \cdots\)</span>.
The optimum compressor <span class="math inline">\(f^*(x)\)</span> satisfies
<span class="math display">\[
    l(f^*(X)) = \lceil \log_2 X\rceil
\]</span>
We also proved that the expected
optimum compression length satisfies
<span class="math display">\[
    \mathbb El(f^*(X)) \approx H(X)
\]</span>
In fact, this is upper-bounded by <span class="math inline">\(H(X)\)</span>.
<em>But this is not an algorithm!</em></p>
<ol style="list-style-type: decimal">
<li>Sorting the distribution is intractible.</li>
<li>It’s variable-length but not prefix-free!
We’re assuming one-shot compression
(comma is for free). Howevver, prefix free code,
the optimal expected bound is lower-bounded by <span class="math inline">\(H(X)\)</span>.</li>
<li>Proof idea: typicality; break region into typical (tractable)
and atypical regions with vanishing probability.</li>
<li>Asymptotic equipartition property: i.i.d. implies
law of large numbers (in log-space).</li>
</ol>
<p>Distribution of compression length is distributed (up to constants)
as the entropy density. This is great because entropy density tensorizes.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-99" class="proposition"><strong>Proposition 14.3  (distribution of compression length) </strong></span>Define random variable <span class="math inline">\(L^* = l(f^*(X))\)</span>.
Denote the entropy density <span class="math inline">\(i_X(a) = -\log P_X(a)\)</span>;
the optimal compressor should compress symbol <span class="math inline">\(a\)</span> to roughly
this length:
<span class="math display">\[
    \mathrm{Pr}[i(X) \leq k] \leq \mathrm{Pr}[L^* \leq K] \leq \mathrm{Pr}[i(X) \leq k + \tau]
    + 2^{1-\tau}
\]</span>
This holds for every <span class="math inline">\(k\in \mathbb Z_+, \tau&gt;0\)</span>.</p>
</div>
<p><em>Proof:</em> Left-bound is easy:
<span class="math display">\[
    L^*(x) = \lceil \log_2 X\rceil \leq \log 2 |X|
    \leq -\log P_X(x) = i_X(x)
\]</span>
To bound the second term, decompose the probability
<span class="math display">\[\begin{align}
    \mathrm{Pr}[L^* \leq k]
    &amp;= \mathrm{Pr}[L^* \leq k, i(x) \leq k+\tau] + \mathrm{Pr}[L^* \leq k, i(x) &gt; k+\tau] \\
    &amp;\leq \mathrm{Pr}[i_X(x) \leq k+\tau] + (\cdots)
\end{align}\]</span>
The second term is bounded by the number of strings
which achieves this <span class="math inline">\(2^{k+1}\)</span> times the maximum probability
they’re obtaining <span class="math inline">\(2^{-k-\tau}\)</span>, yielding <span class="math inline">\(2^{1-\tau}\)</span>.</p>
<p>Corrolary: if, for some sequence of r.v., the normalized entropy
rate converges in distribution to <span class="math inline">\(U\)</span>, then the normalized optimal
compression length (for asymptotically large block length)
also converges to <span class="math inline">\(U\)</span>.</p>
<p>Another corollary: if <span class="math inline">\(S_j\sim P_S\)</span> i.i.d., then
<span class="math display">\[
    \dfrac 1 n i_{S_1^n}(S_1^n) = -\dfrac 1 n \log P_{S^n}(S^n)
    = -\dfrac 1 n \sum_{j=1}^n \log P_S(s_j) \to H(S)
\]</span>
This implies that the expectation of the optimal compression length
for i.i.d. source <span class="math inline">\(X\)</span> converges to <span class="math inline">\(H(X)\)</span> in the limit of asymptotically
large block lengths.</p>
<p><u> This is a nontrivial result, because the optimal compressor
is a very freely-specified object, but we are able to bound its
behavior very neatly. </u></p>
<p>In particular, recall that the optimal-compressor maps
highest-probability atoms to the empty set, but we see from the
asymptotic Gaussian distribution of the compression length that
they have almost negligible density. This is a demonstration of the
<strong>asymptotic equipartition property</strong>, which states that
for i.i.d. sources, the overwhelming number of sequences have
the same probability given by <span class="math inline">\(e^{H(X)}\)</span>.</p>
</div>
<div id="arithmetic-encoder" class="section level3 unnumbered hasAnchor">
<h3>Arithmetic encoder<a href="lecture-notes.html#arithmetic-encoder" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a general <span class="math inline">\(\{X_j\}\)</span>-process and wishing to compress <span class="math inline">\(X_1^n\)</span>.</p>
<p>First consider i.i.d process. Order the alphabet and
recursively partition the interval <span class="math inline">\([0, 1]\)</span> so that
each interval has length equal to its probability.
The trick is to find the largest dyadic interval (recursive
binary partition) that fits inside the interval of the message.
For example:
<span class="math display">\[
    [0, 1]\to \emptyset, \quad [6\cdot 2^{-3}, 7\cdot 2^{-3}]\to 110
\]</span>
In the limit that the codestring goes to infinity,
the distribution of binary expansion will be uniform.</p>
<p>Fact for the compression length of arithmetic encoder for i.i.d. terms:
<span class="math display">\[
    \log \dfrac 1 {P_{X^n}(x^n)} \leq l(f_{\mathrm{ae}}(x^n)
    \leq \log_2 \dfrac 1 {P_{X^n}(x^n)} + 2
\]</span>
The arithmetic encoder is additionally sequential: it does not
need to consume the full string to start outputting compression;
the same holds for the decompressor.</p>
<p>Implementing the arithmetic encoder for general non-i.i.d.
distributions just replace subsequent intervals by the marginals
<span class="math inline">\(P_{X_n \| X^{(n-1)}}\)</span>.</p>
<p><u>
This means that, if we can sequentially predict the marginal
<span class="math inline">\(P_{X_n \|X^{(n-1)}}\)</span> very well (e.g. next-token prediction LLM),
then we can close-to-optimal compress a non-i.i.d. distribution
by combining this with the arithmetic encoder.
</u></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-100" class="theorem"><strong>Theorem 14.3  (Shannon-McMillan-Breimen) </strong></span>A.e.p. holds w.r.t. the entropy rate
if <span class="math inline">\(\{X_j\}\)</span> is a stationary (ensures
existence of the entropy rate) ergodic process.</p>
</div>
<p>Shannon’s proof:
every stationary ergodic process can be arbitrarily
approximated by a <span class="math inline">\(m\to \infty\)</span>-order Markov chain.</p>
</div>
<div id="lempel-ziv" class="section level3 unnumbered hasAnchor">
<h3>Lempel-Ziv<a href="lecture-notes.html#lempel-ziv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Central question: how to compress well without <span class="math inline">\(P_{X^n}\)</span>?</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-101" class="lemma"><strong>Lemma 14.1  (Katz's lemma) </strong></span>Given a stationary ergodic process <span class="math inline">\((X_{j\in \mathbb Z})\)</span>.
Define <span class="math inline">\(L = \inf\{t&gt;0: X_{-t}=X_0\}\)</span>, then
<span class="math display">\[
    \mathbb E[L | X_0=u] = \mathrm{Pr}[X_0=u]^{-1}
\]</span></p>
</div>
<p><em>Proof:</em> consider the probability that we don’t see <span class="math inline">\(u\)</span>
when we look back for <span class="math inline">\(k\)</span> steps, using stationarity:
<span class="math display">\[\begin{align}
    \mathrm{Pr}[L&gt;K, X_0=u]
    &amp;= \mathrm{Pr}[X_0=u, X_{-1}\neq U, \cdots, X_{-k}\neq u]  \\
    &amp;= \mathrm{Pr}[X_k=u, X_{k-1}\neq U, \cdots, X_0\neq u]
    = \mathrm{Pr}[E_k]
\end{align}\]</span>
<em>todo</em> Another key point is that for stationary ergodic
processes, <span class="math inline">\(\mathrm{Pr}[\bigcup E_k] = 1\)</span>.</p>
<p>Using Katz’s lemma, we can do an unbiased estimation of <span class="math inline">\(\mathrm{Pr}[X_0=u]\)</span>
by looking back.</p>
<p><span style="color:green">
Do probability mixtures satisfy all of the local Fisher
regularity conditions?
</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="universal-compression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliography.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
