<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12 Lecture notes | 6.7480 Notes</title>
  <meta name="description" content="12 Lecture notes | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="12 Lecture notes | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12 Lecture notes | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-12-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hypothesis-testing-large-deviations.html"/>
<link rel="next" href="bibliography.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recurring-themes"><i class="fa fa-check"></i>Recurring themes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#insightful-proof-techniques"><i class="fa fa-check"></i>Insightful proof techniques</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics.html"><a href="entropy-method-in-combinatorics.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html"><i class="fa fa-check"></i><b>3</b> Kullback-Leibler Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="kullback-leibler-divergence.html"><a href="kullback-leibler-divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-characterizations.html"><a href="variational-characterizations.html"><i class="fa fa-check"></i><b>5</b> Variational Characterizations</a>
<ul>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#geometric-interpretation-of-mi"><i class="fa fa-check"></i>Geometric interpretation of MI</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-characterizations.html"><a href="variational-characterizations.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a>
<ul>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#mi-gaussian-saddle-point"><i class="fa fa-check"></i>MI, Gaussian saddle point</a></li>
<li class="chapter" data-level="" data-path="tensorization.html"><a href="tensorization.html#information-rates"><i class="fa fa-check"></i>Information rates</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition-1"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations-1"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html"><i class="fa fa-check"></i><b>9</b> Statistical Decision Applications</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimax-and-bayes-risks"><i class="fa fa-check"></i>Minimax and Bayes risks</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#minimaxDual"><i class="fa fa-check"></i>A duality perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#sample-complexity-tensor-products"><i class="fa fa-check"></i>Sample complexity, tensor products</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#hcr-lower-bound"><i class="fa fa-check"></i>HCR lower bound</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#bayesian-perspective"><i class="fa fa-check"></i>Bayesian perspective</a></li>
<li class="chapter" data-level="" data-path="statistical-decision-applications.html"><a href="statistical-decision-applications.html#miscellany-mle"><i class="fa fa-check"></i>Miscellany: MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lossless-compression.html"><a href="lossless-compression.html"><i class="fa fa-check"></i><b>10</b> Lossless compression</a>
<ul>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#variable-length-source-coding"><i class="fa fa-check"></i>Variable-length source coding</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#uniquely-decodable-codes"><i class="fa fa-check"></i>Uniquely decodable codes</a></li>
<li class="chapter" data-level="" data-path="lossless-compression.html"><a href="lossless-compression.html#fixed-length-source-coding"><i class="fa fa-check"></i>Fixed-length source coding</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing, Large Deviations</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#npFormulation"><i class="fa fa-check"></i>Neyman-Pearson</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-large-deviations.html"><a href="hypothesis-testing-large-deviations.html#large-deviations-theory"><i class="fa fa-check"></i>Large deviations theory</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>12</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-28-binary-hypothesis-testing"><i class="fa fa-check"></i>Oct 28: Binary hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#more-on-universal-compression"><i class="fa fa-check"></i>More on universal compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#binary-hypothesis-testing"><i class="fa fa-check"></i>Binary hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-4.-channel-coding"><i class="fa fa-check"></i>Nov 4. Channel coding</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#bht-large-deviations-theory"><i class="fa fa-check"></i>BHT, large deviations theory</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#i-projections"><i class="fa fa-check"></i>I-projections</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#channel-coding"><i class="fa fa-check"></i>Channel coding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-6-channel-coding-ii"><i class="fa fa-check"></i>Nov 6, Channel coding II</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-18-channel-coding-iii"><i class="fa fa-check"></i>Nov 18, Channel Coding III</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#decoding-with-constraint"><i class="fa fa-check"></i>Decoding with constraint</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-20.-quantization"><i class="fa fa-check"></i>Nov 20. Quantization</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#water-filling"><i class="fa fa-check"></i>Water-filling</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#metric-entropy"><i class="fa fa-check"></i>Metric-entropy</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#nov-25-rate-distortion-theorem"><i class="fa fa-check"></i>Nov 25: Rate-distortion theorem</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#scalar-quantization"><i class="fa fa-check"></i>Scalar quantization</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#vector-quantization"><i class="fa fa-check"></i>Vector quantization</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#dec-4-density-estimation"><i class="fa fa-check"></i>Dec 4: Density estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review-of-metric-entropy"><i class="fa fa-check"></i>Review of metric entropy</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#dec-9.-strong-dpi"><i class="fa fa-check"></i>Dec 9. Strong DPI</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#minimax-stats"><i class="fa fa-check"></i>Minimax stats</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#combinatorial-statistics"><i class="fa fa-check"></i>Combinatorial statistics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#dec-11-sdpi-distributed-estimation"><i class="fa fa-check"></i>Dec 11: SDPI, Distributed Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#spiked-wigner"><i class="fa fa-check"></i>Spiked Wigner</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#correlation-estimation"><i class="fa fa-check"></i>Correlation estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture-notes" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">12</span> Lecture notes<a href="lecture-notes.html#lecture-notes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="oct-2-fisher-information-classical-minimax-estimation" class="section level2 unnumbered hasAnchor">
<h2>Oct 2: Fisher information, classical minimax estimation<a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\chi^2\)</span> variational characterization.</li>
<li><span class="math inline">\(1\)</span>-parameter families; minimax rate.</li>
<li>HCR, and Fisher information.</li>
<li>Cramer-Rau; van Trees inequality.</li>
</ol>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R_n^*\)</span> is the minimax rate of parameterization.</li>
<li>LeCam-Hajek theory: <span class="math inline">\(R_n^* = \dfrac{1+o(1)}{n \min_\theta I_F(\theta)}\)</span>.</li>
<li>To obtain more well-behaved inequalities, expand a single extremum
into a nested one (e.g. scalar multiple) then solve the closed form
of the inner optimization.</li>
<li>All <span class="math inline">\(f\)</span>-divergences are locally quadratic in parameteric families
with Hessian given by Fisher information.</li>
<li>Cramer-Rau is an application of the variational characterization
of <span class="math inline">\(\chi^2\)</span>.</li>
</ol>
<div id="main-content" class="section level3 unnumbered hasAnchor">
<h3>Main content<a href="lecture-notes.html#main-content" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recalling the variational characterization:
<span class="math display">\[
    D_f(P\|Q) = \sup_g \mathbb E_p g - \mathbb E_Q f^*\circ g
\]</span>
where the convex conjugate is given by
<span class="math display">\[
    f^*(h) = \sup_{t\in \mathbb R} th - f(t), \quad
    f(t) = \sup_{h\in \mathbb R} th - f^*(h)
\]</span>
Recall that Donsker-varadhan is not linear in <span class="math inline">\(\mathbb E_Q\)</span>,
but there is a standard trick to rewrite the expectation.</p>
<p>We now apply the variational characterization to <span class="math inline">\(\chi^2\)</span>;</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-103" class="proposition"><strong>Proposition 12.1  (variational characterization of χ²) </strong></span><span class="math inline">\(\chi^2(P \|Q)
= \sup_{h:\mathcal X\to \mathbb R} \mathbb E_p h(X) - \mathbb E_Q \left[
    h(X) + \dfrac{h(X)^2}{4}
\right]\)</span></p>
</div>
<p>Expanding this out, the first term is very useful;
but the second is not. The first step is to break a single
extrema into two:
<span class="math display">\[\begin{align}
    \chi^2(P \|Q)
    &amp;= \sup_h \left[
        \mathbb E_p h - \mathbb E_Q h
    \right] - \dfrac 1 4 \mathbb E_Q h^2 \\
    &amp;= \sup_g \sup_{\lambda\in \mathbb R} \lambda \left(
        \mathbb E_P g - \mathbb E_Q g
    \right) - \dfrac{\lambda^2}{4} \mathbb E_Q g^2  
    = \sup_g \dfrac{(\mathbb E_p g - \mathbb E_Q g)^2}{\mathbb E_q g^2}
\end{align}\]</span>
As a consequence, we obtain
<span class="math display">\[
    \left(
        \mathbb E_P g - \mathbb E_Q g
    \right)^2 \leq \chi^2(P \|Q) \mathbb E_Q g^2
\]</span>
In fact, this equation is invariant under <span class="math inline">\(g\mapsto g + c\)</span>, yielding
<span class="math display">\[
    \left(
        \mathbb E_P g - \mathbb E_Q g
    \right)^2 \leq \chi^2(P \|Q) \mathrm{Var}_Q g^2
\]</span></p>
<p><span style="color:green">
Exercise: <span class="math inline">\(\forall g&gt;0\)</span>, we have
<span class="math inline">\(\mathbb E_P g \leq 2\mathbb E_Q g + 2_? H^2(P, Q)\)</span>.
</span></p>
</div>
<div id="one-parameter-families-minimax-rates" class="section level3 unnumbered hasAnchor">
<h3>One-parameter families; minimax rates<a href="lecture-notes.html#one-parameter-families-minimax-rates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statisticians care about sub-manifolds
of the probability simplex.</p>
<p>For one-parameter families, we typically have
<span class="math display">\[
    P^\theta(dx) = p^\theta(x) \mu(dx)
\]</span>
For discrete, <span class="math inline">\(\mu\)</span> is counting;
for real-valued, <span class="math inline">\(\mu\)</span> is Lebesgue.</p>
<p><strong>Parameter estimation</strong>: given <span class="math inline">\(X_1, \cdots, X_n \sim P^\theta\)</span>;
find function <span class="math inline">\(\hat \theta(X_1, \cdots, X_n)\)</span> such that
<span class="math inline">\(\hat \theta \approx \theta\)</span>; here <span class="math inline">\(\approx\)</span> is defined w.r.t.
a risk function
<span class="math display">\[
    R(\hat \theta, \theta) = \mathbb E_{X^n} (\theta - \hat \theta(X^n))^2
\]</span>
Note that the first argument is a function, while the second is a
parameter coordinate.</p>
<p>Example: <span class="math inline">\(\hat \theta_0 = \dfrac 1 n \sum X_j\)</span> for Bernoulli
parameter estimation; we can compute
<span class="math display">\[
    R(\hat \theta, \theta) = \dfrac{\theta \bar \theta}{n}
\]</span>
Recall that it’s MLE and unbiased. To get mid of the
<span class="math inline">\(\theta\)</span>-dependence, we can consider
<span class="math display">\[
    R^{\mathrm{max}}(\hat \theta)
    = \sup_\theta R(\hat \theta_1, \theta)
\]</span></p>
<p>Consider a naive estimator <span class="math inline">\(\hat \theta_1 = 1/2\)</span>
(this has definite bias but not variance) and
<span class="math display">\[
    \hat \theta_\lambda = \lambda \hat \theta_1
    + \bar \lambda \hat \theta_{\mathrm{MLE}}
\]</span>
One can compute risk = bias^2 +variance^2
and choose an optimal <span class="math inline">\(\lambda_n^*= \dfrac 1 {1 + \sqrt n}\)</span>;
this allows one to optimize the risk everywhere.</p>
<p><strong>Theorem</strong> The optimal shrinkage estimator
<span class="math inline">\(\hat \lambda_{\lambda_n^*}\)</span> saturates the minimax risk
<span class="math display">\[
    \inf_{\hat \theta} R^{\mathrm{max}}(\hat \theta)
    = \dfrac 1 {4(1 + \sqrt n)^2}
\]</span>
<em>Key idea:</em> obtain MLE; identify points of worst
performance (minimum Fisher information), then bias
towards it.</p>
<p>The optimal minimax risk profile (minimax rate) is
<span class="math display">\[
    R_n^* = \inf_{\hat \theta} \sup_\theta R(\hat \theta, \theta)
\]</span></p>
</div>
<div id="hcr-inequality-fisher-information" class="section level3 unnumbered hasAnchor">
<h3>HCR inequality; Fisher information<a href="lecture-notes.html#hcr-inequality-fisher-information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proposition">
<p><span id="prp:unlabeled-div-104" class="proposition"><strong>Proposition 12.2  </strong></span><span class="math inline">\(\forall \hat \theta\)</span> and <span class="math inline">\(\theta, \theta_0\)</span> we have
<span class="math display">\[
    R_n^* \geq \mathrm{Var}_{P^\theta} \geq \dfrac{
        \left(\mathbb E_{P^\theta} \hat \theta
        - \mathbb E_{P^{\theta_0}} \hat \theta\right)^2
    }{
    \chi^2(P^{\theta \otimes n}\| P^{\theta_0 \otimes n})
    }
\]</span></p>
</div>
<p><em>Proof:</em> The first inequality follows from
bias-variance decomposition. For the second, take
<span class="math inline">\(g(X^n) = \hat \theta(X^n)\)</span> in the variational characterization
of <span class="math inline">\(\chi^2\)</span>.</p>
<p>To bring Fisher information into the picture, consider
<span class="math display">\[
    \chi^2 (P^\theta \| P^{\theta_0})
    = \sum \dfrac{P^\theta(x)^2}{P^{\theta_0}(x)} - 1
\]</span>
Take <span class="math inline">\(\theta = \theta_0 + \epsilon\)</span> for small <span class="math inline">\(\epsilon\)</span>;
Taylor-expand to obtain
<span class="math display">\[\begin{align}
    P^\theta(x) = P^{\theta_0}(x)
    + \partial_{\theta }P^\theta \big|_{\theta = \theta_0} (x)\epsilon +
    \cdots
\end{align}\]</span>
Substitute this into the expression for <span class="math inline">\(\chi^2\)</span> to obtain
<span class="math display">\[
    \chi^2 = \sum_x \dfrac{\left[
        P^{\theta_0}(x) + \epsilon \partial_{\theta }P^\theta(x)
    \right]^2}{P^{\theta_0}(x)} - 1 + o(\epsilon^2)
\]</span>
Expanding the term, the linear term is <span class="math inline">\(0\)</span> (pull <span class="math inline">\(\partial_{\theta}\)</span> out):
<span class="math display">\[
    \sum_x \partial_{\theta }P^{\theta}(x) \big|_{\theta = \theta_0}
    = 0
\]</span>
The null term sums to <span class="math inline">\(1\)</span>, cancelling the <span class="math inline">\(-1\)</span>, yielding the
local definition of <span class="math inline">\(\chi^2\)</span>
<span class="math display">\[
    \chi^2 = \epsilon^2 \sum_x
    \dfrac{\left[\partial_{\theta }P^{\theta_0}(x)\right]^2}
    {P^{\theta_0}(x)} + o(\epsilon^2)
\]</span></p>
<p>Recall that the <strong>Fisher information</strong> is given by
<span class="math display">\[
    \mathcal J_F(\theta; \{P^\theta\}_{\theta \in \Theta})
    = \int \dfrac{\left[\partial_{\theta }P^\theta(x)\right]^2}{P^\theta(x)} \, \mu(dx)
\]</span>
Fisher information is just the second-order derivative
of <span class="math inline">\(\chi^2\)</span>.</p>
<p>There are two notions of distance:
Fisher information tells us, when we nudge
<span class="math inline">\(\theta \mapsto \theta + \epsilon\)</span>,
how much is the statistical distance between <span class="math inline">\(P^\theta\)</span>
and <span class="math inline">\(P^{\theta + \epsilon}\)</span>.</p>
<p>Moreover, Fisher information is additive under
tensorization: KL is additive under tensorization
and apply the locality principle.</p>
<p>Theorems about Fisher information require a lot
more assumptions.</p>
</div>
</div>
<div id="oct-7-data-compression" class="section level2 unnumbered hasAnchor">
<h2>Oct 7: Data compression<a href="lecture-notes.html#oct-7-data-compression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-105" class="theorem"><strong>Theorem 12.1  (local expansion of χ) </strong></span>For <span class="math inline">\(\theta_0 = 0\)</span>, when the following conditions hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\theta \in [0, \tau)\)</span>.</li>
<li>There exists <span class="math inline">\(\dot p^\theta(x)\)</span> satisfying
<span class="math display">\[
P^\theta(x) = P^0(x) + \int_0^\theta \dot p^t(x)\, dt
\]</span></li>
<li>The fisher information is defined <em>everywhere</em>
in a connected interval open interval about <span class="math inline">\(\theta_0\)</span>:
<span class="math display">\[
\int dx\, \sup_{0\leq t &lt; \tau}
\dfrac{\dot p^t(x)^2}{p^0(x)} &lt; \infty
\]</span></li>
</ol>
<p>Then <span class="math inline">\(\chi^2(P^{\theta_0 + \delta} \| P^{\theta_0})
    + \delta^2 \mathcal J_F(\theta_0) + o(\delta^2)\)</span>.</p>
</div>
<p>The quadratic local behavior of <span class="math inline">\(\chi^2\)</span> requires
not only finite Fisher information at <span class="math inline">\(\theta_0\)</span>;
it also requires <span class="math inline">\(\mathcal J_F\)</span> to be finite almost everywhere
on a nontrivial interval.</p>
<p><span style="color:green">
In summary, <span class="math inline">\(\chi^2\)</span> is nice for the “good” cases
where <span class="math inline">\(\mathcal J\)</span> is finite a.e. However, for some
irregular cases it’s best to use
<span class="math inline">\(H^2(P^{\theta_0 + \delta}, P^\theta)\)</span>, whose local
behavior is always determined by the local Fisher information.
</span></p>
<p>A location family is a one-parameter family for which
the parameter controls the displacement of a fixed density <span class="math inline">\(\nu\)</span>,
e.g. <span class="math inline">\(\mathcal N(\theta, 1)\)</span>; for this family,
<span class="math inline">\(\mathcal J(\theta) = \mathcal J(0)\)</span>, which we can abbreviate as
<span class="math display">\[
    \mathcal J(\nu) = \int \dfrac{(\nu&#39;)^2}{\nu} \, dx
\]</span>
Note that Fisher information <span class="math inline">\(\mathcal J_F\)</span> is originally defined for
a one-parameter family; with the notation <span class="math inline">\(\mathcal J(\nu)\)</span>,
we’re assuming the one-parameter family to be the location family
of the given density.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-106" class="theorem"><strong>Theorem 12.2  (van Trees inequality) </strong></span>Under regularity assumptions on <span class="math inline">\(P^\theta\)</span>,
for every estimator <span class="math inline">\(\hat \theta(X)\)</span> and prior
<span class="math inline">\(\pi\)</span>
<span class="math display">\[
    \mathbb E_{\theta \sim \pi} \mathbb E_{X\sim P^\theta} \left[
        [\theta - \hat \theta(X)]^2
    \right] \geq \dfrac 1 {\mathcal J(\pi) +
    \mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\]</span></p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-107" class="corollary"><strong>Corollary 12.1  </strong></span>For <span class="math inline">\(n\)</span> i.i.d. observations, for every <span class="math inline">\(\pi\)</span>
<span class="math display">\[
    R_n^* \geq \dfrac 1 {\mathcal J(\pi) +
    n\mathbb E_{\theta \sim \pi} \mathcal J_F(\theta)}
\]</span></p>
</div>
<p>van Trees is important important because it provides
a lower bound on minimax risk based on
local quantities <span class="math inline">\(\mathcal J_F(\theta)\)</span>.</p>
<p>Two takeaways for information theorists:</p>
<ol style="list-style-type: decimal">
<li>In Bayesian setting, apply information theory techniques
to the joint P_{, X}$.</li>
<li>In local neighborhoods, <span class="math inline">\(\chi^2\)</span> satisfies an additive
chain rule (since it’s close to KL).</li>
</ol>
<p>Van Trees replaced <span class="math inline">\(\mathcal J_F\)</span> with the expectation of
<span class="math inline">\(\mathcal J_F\)</span>, and it applies to every (instead of unbiased) estimators.</p>
</div>
<div id="oct-9-data-compression-ii" class="section level2 unnumbered hasAnchor">
<h2>Oct 9: data compression II<a href="lecture-notes.html#oct-9-data-compression-ii" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Overview:</p>
<ol style="list-style-type: decimal">
<li>Review</li>
<li>Distribution of fixed length</li>
<li>Arithmetic encoder</li>
<li>Stationary, ergodic data sources</li>
<li>Lempel-Ziv (adaptive, universal compression)</li>
</ol>
<p>Main takeaways:</p>
<ol style="list-style-type: decimal">
<li>AEP.</li>
<li>Optimal compression is possible using (1) arithmetic encoding
and (2) optimal next-token prediction.</li>
</ol>
<p>Information theory is about bounding
hard, intractable operational quantities with mathematically
analyzable quantities.</p>
<div id="review" class="section level3 unnumbered hasAnchor">
<h3>Review<a href="lecture-notes.html#review" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume <span class="math inline">\(P_X\)</span> known and ordered <span class="math inline">\(P_X(1)\geq P_X(2)\geq \cdots\)</span>.
The optimum compressor <span class="math inline">\(f^*(x)\)</span> satisfies
<span class="math display">\[
    l(f^*(X)) = \lceil \log_2 X\rceil
\]</span>
We also proved that the expected
optimum compression length satisfies
<span class="math display">\[
    \mathbb El(f^*(X)) \approx H(X)
\]</span>
In fact, this is upper-bounded by <span class="math inline">\(H(X)\)</span>.
<em>But this is not an algorithm!</em></p>
<ol style="list-style-type: decimal">
<li>Sorting the distribution is intractible.</li>
<li>It’s variable-length but not prefix-free!
We’re assuming one-shot compression
(comma is for free). Howevver, prefix free code,
the optimal expected bound is lower-bounded by <span class="math inline">\(H(X)\)</span>.</li>
<li>Proof idea: typicality; break region into typical (tractable)
and atypical regions with vanishing probability.</li>
<li>Asymptotic equipartition property: i.i.d. implies
law of large numbers (in log-space).</li>
</ol>
<p>Distribution of compression length is distributed (up to constants)
as the entropy density. This is great because entropy density tensorizes.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-108" class="proposition"><strong>Proposition 12.3  (distribution of compression length) </strong></span>Define random variable <span class="math inline">\(L^* = l(f^*(X))\)</span>.
Denote the entropy density <span class="math inline">\(i_X(a) = -\log P_X(a)\)</span>;
the optimal compressor should compress symbol <span class="math inline">\(a\)</span> to roughly
this length:
<span class="math display">\[
    \mathrm{Pr}[i(X) \leq k] \leq \mathrm{Pr}[L^* \leq K] \leq \mathrm{Pr}[i(X) \leq k + \tau]
    + 2^{1-\tau}
\]</span>
This holds for every <span class="math inline">\(k\in \mathbb Z_+, \tau&gt;0\)</span>.</p>
</div>
<p><em>Proof:</em> Left-bound is easy:
<span class="math display">\[
    L^*(x) = \lceil \log_2 X\rceil \leq \log 2 |X|
    \leq -\log P_X(x) = i_X(x)
\]</span>
To bound the second term, decompose the probability
<span class="math display">\[\begin{align}
    \mathrm{Pr}[L^* \leq k]
    &amp;= \mathrm{Pr}[L^* \leq k, i(x) \leq k+\tau] + \mathrm{Pr}[L^* \leq k, i(x) &gt; k+\tau] \\
    &amp;\leq \mathrm{Pr}[i_X(x) \leq k+\tau] + (\cdots)
\end{align}\]</span>
The second term is bounded by the number of strings
which achieves this <span class="math inline">\(2^{k+1}\)</span> times the maximum probability
they’re obtaining <span class="math inline">\(2^{-k-\tau}\)</span>, yielding <span class="math inline">\(2^{1-\tau}\)</span>.</p>
<p>Corrolary: if, for some sequence of r.v., the normalized entropy
rate converges in distribution to <span class="math inline">\(U\)</span>, then the normalized optimal
compression length (for asymptotically large block length)
also converges to <span class="math inline">\(U\)</span>.</p>
<p>Another corollary: if <span class="math inline">\(S_j\sim P_S\)</span> i.i.d., then
<span class="math display">\[
    \dfrac 1 n i_{S_1^n}(S_1^n) = -\dfrac 1 n \log P_{S^n}(S^n)
    = -\dfrac 1 n \sum_{j=1}^n \log P_S(s_j) \to H(S)
\]</span>
This implies that the expectation of the optimal compression length
for i.i.d. source <span class="math inline">\(X\)</span> converges to <span class="math inline">\(H(X)\)</span> in the limit of asymptotically
large block lengths.</p>
<p><u> This is a nontrivial result, because the optimal compressor
is a very freely-specified object, but we are able to bound its
behavior very neatly. </u></p>
<p>In particular, recall that the optimal-compressor maps
highest-probability atoms to the empty set, but we see from the
asymptotic Gaussian distribution of the compression length that
they have almost negligible density. This is a demonstration of the
<strong>asymptotic equipartition property</strong>, which states that
for i.i.d. sources, the overwhelming number of sequences have
the same probability given by <span class="math inline">\(e^{H(X)}\)</span>.</p>
</div>
<div id="arithmetic-encoder" class="section level3 unnumbered hasAnchor">
<h3>Arithmetic encoder<a href="lecture-notes.html#arithmetic-encoder" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a general <span class="math inline">\(\{X_j\}\)</span>-process and wishing to compress <span class="math inline">\(X_1^n\)</span>.</p>
<p>First consider i.i.d process. Order the alphabet and
recursively partition the interval <span class="math inline">\([0, 1]\)</span> so that
each interval has length equal to its probability.
The trick is to find the largest dyadic interval (recursive
binary partition) that fits inside the interval of the message.
For example:
<span class="math display">\[
    [0, 1]\to \emptyset, \quad [6\cdot 2^{-3}, 7\cdot 2^{-3}]\to 110
\]</span>
In the limit that the codestring goes to infinity,
the distribution of binary expansion will be uniform.</p>
<p>Fact for the compression length of arithmetic encoder for i.i.d. terms:
<span class="math display">\[
    \log \dfrac 1 {P_{X^n}(x^n)} \leq l(f_{\mathrm{ae}}(x^n)
    \leq \log_2 \dfrac 1 {P_{X^n}(x^n)} + 2
\]</span>
The arithmetic encoder is additionally sequential: it does not
need to consume the full string to start outputting compression;
the same holds for the decompressor.</p>
<p>Implementing the arithmetic encoder for general non-i.i.d.
distributions just replace subsequent intervals by the marginals
<span class="math inline">\(P_{X_n \| X^{(n-1)}}\)</span>.</p>
<p><u>
This means that, if we can sequentially predict the marginal
<span class="math inline">\(P_{X_n \|X^{(n-1)}}\)</span> very well (e.g. next-token prediction LLM),
then we can close-to-optimal compress a non-i.i.d. distribution
by combining this with the arithmetic encoder.
</u></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-109" class="theorem"><strong>Theorem 12.3  (Shannon-McMillan-Breimen) </strong></span>A.e.p. holds w.r.t. the entropy rate
if <span class="math inline">\(\{X_j\}\)</span> is a stationary (ensures
existence of the entropy rate) ergodic process.</p>
</div>
<p>Shannon’s proof:
every stationary ergodic process can be arbitrarily
approximated by a <span class="math inline">\(m\to \infty\)</span>-order Markov chain.</p>
</div>
<div id="lempel-ziv" class="section level3 unnumbered hasAnchor">
<h3>Lempel-Ziv<a href="lecture-notes.html#lempel-ziv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Central question: how to compress well without <span class="math inline">\(P_{X^n}\)</span>?</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-110" class="lemma"><strong>Lemma 12.1  (Katz's lemma) </strong></span>Given a stationary ergodic process <span class="math inline">\((X_{j\in \mathbb Z})\)</span>.
Define <span class="math inline">\(L = \inf\{t&gt;0: X_{-t}=X_0\}\)</span>, then
<span class="math display">\[
    \mathbb E[L | X_0=u] = \mathrm{Pr}[X_0=u]^{-1}
\]</span></p>
</div>
<p><em>Proof:</em> consider the probability that we don’t see <span class="math inline">\(u\)</span>
when we look back for <span class="math inline">\(k\)</span> steps, using stationarity:
<span class="math display">\[\begin{align}
    \mathrm{Pr}[L&gt;K, X_0=u]
    &amp;= \mathrm{Pr}[X_0=u, X_{-1}\neq U, \cdots, X_{-k}\neq u]  \\
    &amp;= \mathrm{Pr}[X_k=u, X_{k-1}\neq U, \cdots, X_0\neq u]
    = \mathrm{Pr}[E_k]
\end{align}\]</span>
<em>todo</em> Another key point is that for stationary ergodic
processes, <span class="math inline">\(\mathrm{Pr}[\bigcup E_k] = 1\)</span>.</p>
<p>Using Katz’s lemma, we can do an unbiased estimation of <span class="math inline">\(\mathrm{Pr}[X_0=u]\)</span>
by looking back.</p>
<p><span style="color:green">
Do probability mixtures satisfy all of the local Fisher
regularity conditions?
</span></p>
</div>
</div>
<div id="oct-28-binary-hypothesis-testing" class="section level2 unnumbered hasAnchor">
<h2>Oct 28: Binary hypothesis testing<a href="lecture-notes.html#oct-28-binary-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Finish universal compression.</li>
<li>Define binary hypothesis testing; define <span class="math inline">\(R(P, Q)\)</span>.</li>
<li>Stein’s lemma.</li>
</ol>
<div id="more-on-universal-compression" class="section level3 unnumbered hasAnchor">
<h3>More on universal compression<a href="lecture-notes.html#more-on-universal-compression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that universal compression wants to compress
<span class="math inline">\(P_{X^n}\)</span> to entropy, where <span class="math inline">\(P_{X^n}\)</span> is unknown
and drawn from a class <span class="math inline">\(\Pi\)</span>.</p>
<p>The <strong>fundamental limit</strong> of compression is
<em>the redundancy of class <span class="math inline">\(\Pi\)</span></em>, defined by
<span class="math display">\[
    R_n^*(\Pi) = \inf_{Q_{X^n}}
    \sup_{P_{X^n}\in \Pi} \mathbb E_{X^n\sim P_{X^n}}
    \log \dfrac 1 {Q_{X^n}(X^n)} - H(X^n)
\]</span>
Note the order of <span class="math inline">\(\inf\)</span> and <span class="math inline">\(\sup\)</span>! Else
the “redundancy” would be trivial.</p>
<p>It turns out that for the uniform mixture of
all Bernoullis approximately, for all <span class="math inline">\(\theta\)</span>.
<span class="math display">\[
    Q_{X^n} = \int_0^1 d\theta\, \mathrm{Ber}(\theta)^{\otimes n}
\]</span>
satisfies <span class="math inline">\(D(\mathrm{Ber}(\theta)^{\otimes n} \| Q_{X^n}) \leq \log n\)</span>
To see this, for non-extremal <span class="math inline">\(\theta, \theta&#39;\)</span> we have
<span class="math display">\[
    d(\theta \| \theta&#39;) \approx \dfrac 1 2 (\theta - \theta&#39;)^2
\]</span>
The distribution <span class="math inline">\(Q_{X^n}\)</span> is the <em>universal probability assignment.</em>
(or maximally ignorant</p>
<p><strong>Universal data compression (probability assignment) <span class="math inline">\(\approx\)</span>
choosing the best prior.</strong></p>
<p>The most important theorem: redundancy is equal to the following capacity:
<span class="math display">\[
    R_n^*(\Pi) = \sup_{\pi \in \mathcal P(\Pi)} I(\theta; X^n)
\]</span></p>
<p><u>Jeffrey’s (objective) prior</u>: given smoothness conditions for a family <span class="math inline">\(\Theta\)</span>,
we have
<span class="math display">\[\begin{align}
    R_n^*(\Theta)
    &amp;= \dfrac d 2 \log \dfrac{n}{2\pi e} + \log \int_{\Theta}
    \sqrt{\det \mathcal J_F(\theta)}\, d\theta + o(1) \\
    \pi_n^* &amp;\to \pi_\infty^* \text{ weakly }
    = \dfrac 1 Z \sqrt{\det \mathcal J_F(\theta)}\, d\theta
\end{align}\]</span>
The objective prior is invariant to
reparameterizations. Importantly, for Bernoullis the Jeffreys prior is
<span class="math display">\[\begin{align}
    \pi^*(\theta) = \dfrac 1 {\theta(1 - \theta)}\quad k=2, \text{ else }
    \mathrm{Direchlet}(1/2, \dots, 1/2).
\end{align}\]</span>
The marginal estimator <span class="math inline">\(Q_{X_t|X_1^{t-1}}\)</span> turns out to correspond
to the add-half estimator (KT code).</p>
<p><em>Connection between universal compression and sequential prediction</em>:
Recalling the setup of sequential prediction, for each step the predictor
is penalized by the negative log likelihood (with minimizer given by the true
distribution). From learning theory, the natural metric for this problem
(of a complexity of a class <span class="math inline">\(\Pi\)</span>) is
<span class="math display">\[
    \sup_{P_X\in \Pi} \mathbb E\sum_{t=1}^n
    \log \dfrac 1 {\tilde P_t(X_t; X_1^{t-1})}
    - \log \dfrac 1 {P_{X_t|X_1^{t-1}}}
\]</span>
If some estimator achieves <span class="math inline">\(o(n)\)</span> for the preceding quantity,
then the class is learnable since the estimator is asymptotically as
good as the oracle. It turns out that
<span class="math display">\[
    \inf_{\tilde P} \text{regret} = R_n^*(\Pi).
\]</span>
Relation to <em>in-context learning</em>: training phase allows for representation
for different generating procedures (e.g. Jeffreys prior),
and context chooses the posterior which specializes to the suitable
data-generating process.</p>
<p><strong>Every regret-minimizing estimator is an in-context learner.</strong>
This is related to a theorem for density estimation (related to the
Young-Barron bound):
given <span class="math inline">\(X_1, \cdots X_n\sim P\in \Pi\)</span> i.i.d, we wish to find <span class="math inline">\(\tilde P\)</span>
which will estimate <span class="math inline">\(P\)</span> very well.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-111" class="theorem"><strong>Theorem 12.4  </strong></span>For every i.i.d. class <span class="math inline">\(\Pi\)</span> as above, there exists estimator such that
<span class="math display">\[
    \mathbb E\mathrm{TV}^2 \leq \mathbb ED(P\|\tilde P_{\text{this is random}})
    \leq \dfrac{R_{n+1}^*(\Pi)}{n+1}
\]</span></p>
</div>
<div style="color:blue">
<p>This is a very insightful theorem since the right-hand side quantity
can be upper-bounded by the redundancy theorem.
Universal data compression (information)
reduces to the problem of universal probability assignment,
which is simultaneously the core of density estimation (statistics)
and sequential prediction (learning).</p>
<p><strong>Minimum description length principle</strong>: the more likely
(class) of data-generating process is the one under whose universal
compressor compresses the data down to a smaller length; this
is equivalent to point-point hypothesis testing after collapsing
the classes down to their universal prior.</p>
</div>
</div>
<div id="binary-hypothesis-testing" class="section level3 unnumbered hasAnchor">
<h3>Binary hypothesis testing<a href="lecture-notes.html#binary-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are interested in simple v. simple hypothesis testing i.e. 
distinguishing beween <span class="math inline">\(\theta_1\neq \theta_2\in \Theta\)</span> (Neyman-Pearson);
equivalently, distinguishing between <span class="math inline">\(P_{X^n}\)</span> and <span class="math inline">\(Q_{X^n}\)</span>.
Our goal is to design a binary estimator <span class="math inline">\(Z=Z(X^n)\in \{0, 1\}\)</span>, with
<span class="math inline">\(0\)</span> denoting <span class="math inline">\(X^n\sim P\)</span> and <span class="math inline">\(1\)</span> denoting <span class="math inline">\(X^n\sim Q\)</span>
(the first serious treatment originated with radars detecting incoming planes).
Two associated quantities:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\alpha = P[Z=0]\)</span>: probability of success under <span class="math inline">\(P\)</span>.</li>
<li><span class="math inline">\(\beta = Q[Z=0]\)</span>: probability of failure under <span class="math inline">\(Q\)</span>.</li>
</ol>
<div class="theorem">
<p><span id="thm:unlabeled-div-112" class="theorem"><strong>Theorem 12.5  (Stein's lemma) </strong></span>Among all tests with <span class="math inline">\(\alpha \geq 1 - \epsilon\)</span>, the smallest possible
<span class="math inline">\(\beta = e^{-nD(P\|Q) + o_\epsilon(n)}\)</span> assuming <span class="math inline">\(X_j^n\sim P\)</span> or <span class="math inline">\(Q\)</span>.</p>
</div>
<p>Several comments:</p>
<ol style="list-style-type: decimal">
<li>This theorem is not symmetric w.r.t. <span class="math inline">\(P\leftrightarrow Q\)</span>.</li>
<li>The dependence upon <span class="math inline">\(\epsilon\)</span> is captured in <span class="math inline">\(o_\epsilon(n)\)</span>.</li>
</ol>
<p>To reason about binary hypothesis testing, define
<span class="math display">\[
    \mathcal R(P, Q) = \{(P[Z=0], Q[Z=0])\}
\]</span></p>
<div class="proposition">
<p><span id="prp:unlabeled-div-113" class="proposition"><strong>Proposition 12.4  </strong></span><span class="math inline">\(\mathcal R\subset [0, 1]^2\)</span> is closed, convex, symmetric
w.r.t. <span class="math inline">\((1/2, 1/2)\)</span>, and
<span class="math display">\[
    \mathcal R = \overline{\mathrm{co}\,
        \mathcal R_{\mathrm{det}}(P, Q)
    }
\]</span>
where <span class="math inline">\(\mathcal R_{\mathrm{det}}\)</span> denotes the set of deterministic
joint ranges.</p>
</div>
<p>As a corollary, the problem can be simplified by noting that
<span class="math display">\[
    \mathcal R_{\mathrm{det}} = \{(P[E], Q[E]), \quad E\in \mathcal X\}
\]</span>
For the Bernoulli case, <span class="math inline">\(E\)</span> takes values in <span class="math inline">\(\{\emptyset, 0, 1, \mathcal X\}\)</span>.
The first and last cases correspond to <span class="math inline">\((0, 0)\)</span> and <span class="math inline">\((1, 1)\)</span>.</p>
<p>Perspective on the proof of the NP-lemma:
to understand <span class="math inline">\(\mathcal R_{\mathrm{det}}\)</span>, it suffices to understand
its extremal points, which corresponds to the problem
<span class="math display">\[
    \sup_Z P[Z=0]_{=\alpha} - e^\gamma Q[Z=0]_{=\beta}
\]</span>
The parameterization <span class="math inline">\(e^\gamma\)</span> simply assures positivity.
We “scan” over the supporting hyperplanes with a certain offset.
The upshot is that BHT boils down to computing <span class="math inline">\(P[\log P/Q &gt; \gamma]\)</span>
and <span class="math inline">\(Q[\log P/Q &lt; \gamma]\)</span> for a range of <span class="math inline">\(\gamma\)</span>’s.</p>
<p>The “fatter” this joint range is, the more distinguishable the
two distributions are; identical distributions correspond to the
line <span class="math inline">\(\alpha=\beta\)</span> with zero measure. This motivates the
following results:</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-114" class="proposition"><strong>Proposition 12.5  </strong></span><span class="math inline">\(d(\alpha \| \beta) \leq D(P\|Q)\)</span> for <span class="math inline">\(\alpha, \beta \in \mathcal R\)</span>.
This results from applying DPI to the estimator <span class="math inline">\(Z\)</span>.</p>
</div>
</div>
</div>
<div id="nov-4.-channel-coding" class="section level2 unnumbered hasAnchor">
<h2>Nov 4. Channel coding<a href="lecture-notes.html#nov-4.-channel-coding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Finish Sanov’s theorem, large deviation theory, and I-projection.</li>
<li>Define error-correction codes; example using BSC.</li>
<li>Weak converse bound.</li>
</ol>
<div id="bht-large-deviations-theory" class="section level3 unnumbered hasAnchor">
<h3>BHT, large deviations theory<a href="lecture-notes.html#bht-large-deviations-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To properly understand hypothesis testing, we need to
understand the large-deviations question:
to compute <span class="math inline">\(E_0, E_1\)</span>, we need to compute <span class="math inline">\(E\)</span> in
<span class="math display">\[
    \mathrm{Pr}[\bar T &lt; \gamma] \sim e^{-nE(\gamma)}
\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-115" class="theorem"><strong>Theorem 12.6  (Sanov principle) </strong></span>under regularity conditions on <span class="math inline">\(\mathcal E\)</span>, we have
<span class="math inline">\(\mathrm{Pr}[\hat P_n\in \mathcal E] = e^{-nE + o(n)}\)</span>, where
<span class="math display">\[
    E = \inf_{R\in \mathcal E} D(R\|P)
\]</span></p>
</div>
<p><span style="color:green">
Extremely important proof below!
</span></p>
<div class="lemma">
<p><span id="lem:unlabeled-div-116" class="lemma"><strong>Lemma 12.2  </strong></span>If <span class="math inline">\(\mathcal X\)</span> is finite and <span class="math inline">\(\mathcal E\)</span> is <em>convex</em> with
nonempty interior, then Sanov holds.</p>
</div>
<p>First consider an upper bound: <span class="math inline">\(\mathrm{Pr}[\hat P_n\in \mathcal E] \leq e^{-nE}\)</span>.
By <span class="math inline">\(P_{X^n} = P^{\otimes n}\)</span>, define <span class="math inline">\(\tilde P_{X^n} = P_{X^n | E_n}\)</span>.
Note that the KL divergence
<span class="math display">\[
    D(\tilde P_{X^n} \| P_{X^n})
    = \mathbb E_{P_{X^n}} \log \dfrac{P_{X^n|E_n}(x)}{P_{X^n}(x)}
    = \log \dfrac 1 {P_{X^n}(E_n)}
\]</span>
This is the extension of Theorem 11.9. This implies a lot of bounds!
Chernoff, Berstein, Benett, Okamota, etc. In particular,
<span class="math display">\[
    \forall k, \quad  \mathrm{Pr}[\mathrm{Bin}(n, p)\geq k] \leq \exp \left[-n d(d/n\|p)\right]
\]</span></p>
<p><span style="color:green">
Idea to proving lower bounds: prove for the easy case,
then lift to the general case using information measures.
</span></p>
<p>To prove the lower bound, if <span class="math inline">\(P\in \mathrm{int}(E)\)</span>,
then using the law of large numbers yield <span class="math inline">\(\mathrm{Pr}[\hat P_n\in \mathcal E]\to 1\)</span>.
Otherwise, take <span class="math inline">\(R\in \mathrm{int}(\mathcal E)\)</span>, then
<span class="math inline">\(R^{\otimes n}[\hat P_n\in \mathcal E]\to 1\)</span>. Apply DPI on the indicator <span class="math inline">\(1_{E_n}\)</span>.</p>
</div>
<div id="i-projections" class="section level3 unnumbered hasAnchor">
<h3>I-projections<a href="lecture-notes.html#i-projections" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Take <span class="math inline">\(X\in \mathbb R^d\)</span> samples from <span class="math inline">\(X\sim P\)</span>; define the information projection by
<span class="math display">\[
    I(\gamma) = \inf D(R\|P)
\]</span>
where infimum is over <span class="math inline">\(\mathbb E_P[X]=\gamma\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-117" class="proposition"><strong>Proposition 12.6  (tilting is optimal if exists) </strong></span>Suppose there exists <span class="math inline">\(\lambda \in \mathbb R^d\)</span> such that
<span class="math inline">\(\mathbb E_{X\sim P_\lambda}[X]=\gamma\)</span>, then
for every <span class="math inline">\(R\)</span> satisfying <span class="math inline">\(\mathbb E_R[X]=\gamma\)</span>, we have
<span class="math display">\[
    D(R\|P) = D(R\|P_\lambda) + D(P_\lambda \| P) \geq D(P_\lambda \| P).
\]</span></p>
</div>
<p><em>Proof:</em> <span class="math inline">\(D(R\|P) = \mathbb E_R \log \dfrac{dR}{dP} \dfrac{dR}{dP_\lambda}\)</span> and regroup
by cross-terms. Recognize <span class="math inline">\(\mathbb E_R \log dP_\lambda/dP = \mathbb E_R[\lambda^T X - \psi(\lambda)]\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-118" class="theorem"><strong>Theorem 12.7  (I-projection on a hyperplane) </strong></span></p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\exists R: \mathbb E_R[X]=\gamma\)</span> and <span class="math inline">\(D(R\|P)&lt;\infty\)</span>,
then <span class="math inline">\(\gamma\in \mathrm{csupp}(P)\)</span>.</li>
<li><span class="math inline">\(\exists P_\lambda\)</span> with <span class="math inline">\(\mathbb E_R[X]=\gamma \iff \gamma\in \mathrm{int}(\mathrm{csupp}\, P)\)</span>.</li>
</ol>
</div>
<p>Here the convex support of <span class="math inline">\(P\)</span> is the intersection of all sets <span class="math inline">\(S\)</span>
such that <span class="math inline">\(P[S]=1\)</span> and <span class="math inline">\(S\)</span> is closed and convex; to visualize this, imagine
the mixture of two disjointly supported distributions.</p>
<p>In other words, information projection is solvable iff <span class="math inline">\(\gamma\)</span> is in
csupp, and it is solvable by tilting if it <span class="math inline">\(\gamma\)</span> is in the interior.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-119" class="theorem"><strong>Theorem 12.8  (tradeoff in error exponents) </strong></span><span class="math inline">\(E_1(E_0) = \inf_{D(R\|P)&lt;E_0} D(R\|Q)\)</span>.</p>
</div>
<p>Relation to Wald’s SPRT principle by slightly
modifying the problem formulation: commit to an expected
instead of exact number of samples. For composite v.s.
composite tests, Hellinger computation becomes important.</p>
<p>Hypothesis testing is still a thriving topic!
Instead of specifying the exact generating distributions
<span class="math inline">\(P, Q\)</span>, only samples are provided (<strong>likelihood-free inference</strong>).</p>
</div>
<div id="channel-coding" class="section level3 unnumbered hasAnchor">
<h3>Channel coding<a href="lecture-notes.html#channel-coding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We started with information measures, next found
in compression that <em>compression</em> is described by entropy;
consequently, the optical error decay in <em>hypothesis testing</em>
is provided by divergence. The problem of <em>channel coding</em>
is solved by mutual information.</p>
<p>Given a channel <span class="math inline">\(P_{Y|X}:\mathcal X\to \mathcal Y\)</span>, we wish to
construct <span class="math inline">\(f:[M]\to \mathcal X\)</span> and <span class="math inline">\(g:\mathcal Y\to [M]\cup \{e\}\)</span>
such that the maximal error rate (across all <span class="math inline">\(j\)</span>) <span class="math inline">\(\leq \epsilon\)</span>.
For generality, allow randomized <span class="math inline">\(f, g\)</span>.
<span class="math display">\[
    W\xrightarrow f X\xrightarrow{P_{Y|X}} Y\xrightarrow g \hat W
\]</span>
This is a maximal probability of error code, in comparison to
average probability of error codes.</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(f\)</span> is given, the design of <span class="math inline">\(g\)</span> is essentially <span class="math inline">\(M\)</span>-ary H. T.
For <span class="math inline">\(M&gt;2\)</span> there is no uniquely optimal criteria.
However, if we postulate <span class="math inline">\(j\sim [M]\)</span> uniformly, then
the optimal solution is given by maximum-likelihood decoder.</li>
</ol>
<p>Note that the maximum-likelihood decoder tesselates the
decoder-input space according to 1-nearest neighbor.</p>
<p><em>In information theory, almost all of the work is designing a great distribution.</em></p>
</div>
</div>
<div id="nov-6-channel-coding-ii" class="section level2 unnumbered hasAnchor">
<h2>Nov 6, Channel coding II<a href="lecture-notes.html#nov-6-channel-coding-ii" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(M^*(\epsilon)\)</span>.</li>
<li>Define a channel (again); fundamental limit of
finite-blocklength coding, and capacity.</li>
<li>Weak converse.</li>
</ol>
<p>Recall our definition of a channel last time:</p>
<ol style="list-style-type: decimal">
<li>Channel <span class="math inline">\(P_{Y|X}: \mathcal X\to \mathcal Y\)</span>.</li>
<li>Encoder <span class="math inline">\([M]\to \mathcal X\)</span>, decoder <span class="math inline">\(\mathcal Y\to [M]\cup \{e\}\)</span>.</li>
</ol>
<p>Graphically, this can be represented as
<span class="math display">\[
    W\xrightarrow f X
    \xrightarrow {P_{Y|X}}
    Y\xrightarrow g \hat W
\]</span></p>
<p>We need to put some assumptions on <span class="math inline">\(W\)</span>,
so let’s take <span class="math inline">\(W\sim \mathrm{Unif}[M]\)</span>.
Overwhelmingly, the encoder
<span class="math inline">\(P_{X|W}\)</span> will be deterministic, and
most of the times <span class="math inline">\(P_{\hat W|Y}\)</span> is deterministic;
one does sometime need to randomize when minimizing the
minimax risk, however.</p>
<div class="definition">
<p><span id="def:unlabeled-div-120" class="definition"><strong>Definition 12.1  (error criteria) </strong></span>Consider two types of the probability of error reminiscent
of the case for risk in statistical estimation:</p>
<ol style="list-style-type: decimal">
<li>Average probability of error: <span class="math inline">\(P_{e, \mathrm{avg}} = \mathrm{Pr}[\hat W\neq W]\)</span>.</li>
<li>Maximum probability of error:
<span class="math inline">\(P_{e, \mathrm{max}} = \max_{j\in [M]} \mathrm{Pr}[\hat W\neq j|W=j]\)</span>.</li>
</ol>
</div>
<p>We say that <span class="math inline">\((f, g)\)</span> is an <span class="math inline">\((M, \epsilon)\)</span>-code if
if <span class="math inline">\(P_{e, \mathrm{avg}}\leq \epsilon\)</span>. It is an
<span class="math inline">\((M, \epsilon)_{\mathrm{max}}\)</span> code if <span class="math inline">\(P_{e, \mathrm{max}}\leq \epsilon\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-121" class="definition"><strong>Definition 12.2  (single-shot fundamental limit) </strong></span>Fixing a channel, <span class="math inline">\(M^*(\epsilon)\)</span> is the maximum alphabet size
such that there exists a <span class="math inline">\((M, \epsilon)\)</span>-code.
The max-error limit <span class="math inline">\(M^*_{\mathrm{max}}\)</span> is defined similarly.</p>
</div>
<p>Here <span class="math inline">\(M^*(\epsilon)\)</span> is related the number of messages one can
reliably separate. In general, this is a <em>computationally
intractable quantity</em>, but we will be able to bound it quite tightly.</p>
<p>Recall from last time that, given <span class="math inline">\(f\)</span>, the optimal <span class="math inline">\(g\)</span>
which minimizes <span class="math inline">\(P_{e, \mathrm{avg}}\)</span> is the MLE. This follows
from the MLE coinciding with MAP under the uniform prior.</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(f, g\)</span> are deterministic, then <span class="math inline">\(f(j)=c_j\in \mathcal X\)</span>
is a <strong>codeword</strong>. An encoder can be equivalently be viewed
as a collection of codewords.</li>
<li>The pre-images <span class="math inline">\(g^{-1}(j)=D_j\subset \mathcal Y\)</span> are
the <strong>decoding regions</strong>.</li>
</ol>
<p>When <span class="math inline">\(\mathcal X=\mathcal Y\)</span>, one should think of codewords as
little singleton “points” in space, while the decoding
regions are partitions of the space which attempts to best
separate the codewords in such a way as to be robust to
noise perturbations.</p>
<p>Putting two codewords in space is equivalent to binary
hypothesis testing; if we require <span class="math inline">\(\epsilon\to 0\)</span>,
then <span class="math inline">\(M^*(\epsilon)\to 0\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-122" class="theorem"><strong>Theorem 12.9  (Fano's bound) </strong></span><span class="math inline">\(\log M^*_{\mathrm{max}}(\epsilon) \leq \log M^*(\epsilon)
\leq \dfrac{\sup_X I(X; Y) + \log 2}{1-\epsilon}\)</span>.</p>
</div>
<p><em>Proof:</em> Again, to prove impossibility bounds
begin with the easy case.
Recall <span class="math inline">\(W\to X\to Y\to \hat W\)</span>, consider
<span class="math inline">\(Q_{Y|X}=Q_Y\)</span> which does not look at its input, then
<span class="math display">\[
    Q[W=\hat W] = \dfrac 1 M \sum_j Q[\hat W = j] = \dfrac 1 M
\]</span>
Under the <span class="math inline">\(Q\)</span>-channel, then <span class="math inline">\(1-\epsilon_Q = \dfrac 1 M\)</span>.
Apply DPI to lift the statement about <span class="math inline">\(Q\)</span>-channel to
<span class="math inline">\(P\)</span>-channel, process <span class="math inline">\((W, X, T, \hat W)\)</span> with <span class="math inline">\(1_{W=\hat W}\)</span>
to obtain <span class="math inline">\(\mathrm{Ber}(1/M)\)</span> and <span class="math inline">\(\mathrm{Ber}(1-\epsilon)\)</span>. Then
<span class="math display">\[
    D(P_{WXY\hat W} \| Q_{WXY\hat W})
    \geq d(1-\epsilon \| 1/M)
\]</span>
Noting the Markov structure, apply the chain rule to obtain
<span class="math display">\[\begin{align}
    D(P_{WXY\hat W} \| Q_{WXY\hat W})
    &amp;= D(P_W\|Q_W)_{=0} + D(P_{X|W} \| Q_{X|W} \| P_W)_{=0}
    + D(P_{Y|X} \| Q_Y | P_X) + D(P_{\hat W|Y} \| Q_{\hat W|Y} | P_Y)_{=0}
\end{align}\]</span>
The zeroed terms are due to us only replacing the <span class="math inline">\(X\to Y\)</span> part
of the Markov chain, then <span class="math inline">\(\forall Q\)</span>, we obtain
<span class="math display">\[
    (1-\epsilon)\log M \leq D(P_{Y|X} \| Q_Y | P_X) + \log 2
\]</span>
Take <span class="math inline">\(\inf\)</span> over <span class="math inline">\(Q_Y\)</span> and apply the golden formula to
obtain mutual information. Note that <span class="math inline">\(P_X=f\circ U_{[M]}\)</span>
is usually hard to compute, so take <span class="math inline">\(\sup_X\)</span> to obtain the
desired inequality.</p>
<p><span style="color:green">
In information theory, a second definition of a
<strong>channel</strong> is a triplet consisting of
(1) (single-letter) alphabet <span class="math inline">\(\mathcal A\)</span>,
(2) output alphabet <span class="math inline">\(\mathcal B\)</span>, with (3)
a family of Markov kernels <span class="math inline">\(P_{B^n|A^n}\)</span> for each <span class="math inline">\(n\geq 1\)</span>.
In this sense a channel is interpreted as
<strong>a sequence of Markov kernels</strong>, and <span class="math inline">\(n\)</span>
is the <strong>block length</strong>.
</span></p>
<p>Two examples of additive channels:
1. Gaussian channel: <span class="math inline">\(\mathcal A=\mathcal B=\mathbb R\)</span>, and
<span class="math inline">\(P_{B^n|A^n} = \mathcal N(0, \sigma^2 I_n)\)</span>.
2. Binary symmetric channel <span class="math inline">\(\mathrm{BSC}_\delta\)</span>:
<span class="math inline">\(\mathcal A=\mathcal B=\{0, 1\}\)</span>, and
<span class="math inline">\(P_{B^n | A^n}(b^n|a^n) = \delta^d(1-\delta)^{n-d}\)</span>,
where <span class="math inline">\(d=d_M(a^n, b^n)\)</span> is the Hamming distance between
the two strings.
- Equivalently, <span class="math inline">\(B^n = A^n + Z^n\mod 2, \quad
    Z_j\sim \mathrm{Ber}(\delta)\)</span> i.i.d.</p>
<p>More generally, a discrete stationary memoryless channel (DMC)
is just a tensorized channel which acts i.i.d on the input components.</p>
<div class="definition">
<p><span id="def:unlabeled-div-123" class="definition"><strong>Definition 12.3  (fundamental limit of channel coding, capacity) </strong></span>For any channel, an <span class="math inline">\((M, \epsilon)\)</span> code
for <span class="math inline">\(P_{B^n|A^n}\)</span> is called a <span class="math inline">\((M, n, \epsilon)\)</span> code.
The capacity <span class="math inline">\(C\)</span> of the channel is defined as
<span class="math display">\[\begin{align}
    C_\epsilon &amp;= \liminf_{n\to \infty} \dfrac 1 n \log M^*(n, \epsilon), \quad
    C = \lim_{\epsilon \to 0^+} C_\epsilon.
\end{align}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-124" class="proposition"><strong>Proposition 12.7  (almost no difference between max v.s. average) </strong></span><span class="math inline">\(\forall \tau \in (0, 1)\)</span> we have
<span class="math display">\[
    (1-\tau)M^*(n, \tau \epsilon) \leq M^*_{\mathrm{max}}(n, \epsilon)
    \leq M^*(n, \epsilon)
\]</span></p>
</div>
<p><em>Proof:</em> The second inequality is trivial. For the first,
take an <span class="math inline">\((n, M, \tilde \epsilon)\)</span> code and define
<span class="math inline">\(\lambda_j = \mathrm{Pr}[\hat W\neq j|W=j]\)</span>, then
<span class="math display">\[
    \dfrac 1 M \sum_j \lambda_j \leq \tilde \epsilon \implies
    \text{ number of $j$ s.t. } \lambda&gt;\tilde \epsilon/\tau
    \text{ is less than } \tau M
\]</span>
For every average-prob error code, we can apply the same idea
behind Markov’s inequality to select a max-probability sub-code.</p>
<div class="corollary">
<p><span id="cor:unlabeled-div-125" class="corollary"><strong>Corollary 12.2  </strong></span><span class="math inline">\(C_\epsilon \geq C_\epsilon^{\mathrm{max}}
\geq \lim_{\delta \to 0^-} C_{\epsilon - \delta} \implies C^{\mathrm{max}} = C\)</span>.</p>
</div>
<p>One perspective on <span class="math inline">\(C\)</span> is that it is the maximal rate of
asymptotically reliable communication. In other words,
it is the supremum over all rates <span class="math inline">\(R\)</span> such that there exists a
sequence of <span class="math inline">\(\delta_n, \epsilon_n\to 0\)</span>,
<span class="math inline">\((n, e^{n(R-\delta_n)}, \epsilon_n)\)</span>-codes.</p>
<p><span style="color:green">
Proof: exercise??!
</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-126" class="definition"><strong>Definition 12.4  (information capacity) </strong></span>The information capacity <span class="math inline">\(C^{(i)}\)</span> of a channel is
<span class="math display">\[
    C^{(i)} = \liminf_{n\to \infty} \dfrac 1 n
    \sup_{P_{A^n}} I(A^n; B^n)
\]</span>
Compared to the actual capacity
(integer programming problem), this is easy!</p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-127" class="proposition"><strong>Proposition 12.8  (capacity of BSC) </strong></span>For <span class="math inline">\(\mathrm{BSC}_\delta\)</span>, we obtain <span class="math inline">\(C^{(i)} = \log_2 - h(\delta)\)</span>.</p>
</div>
<p><em>Proof:</em> upper-bound by tensorization of mutual information.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-128" class="theorem"><strong>Theorem 12.10  (relation between information and actual capacity) </strong></span>Fano’s bound implies<br />
<span class="math inline">\(C_\epsilon \leq \dfrac{C^{(i)}}{1-\epsilon} \implies C\leq C^{(i)}\)</span>.</p>
</div>
<p>For the BSC channel, the optimal decoder <span class="math inline">\(g_{\mathrm{MLE}}\)</span>
just looks for the
codeword which is the fewest number of flips away from the input.
Approaching this from a different perspective, the average
number of flips for transmitting <span class="math inline">\(c_1\in \{0, 1\}^n\)</span> is <span class="math inline">\(n\delta\)</span>;
concentration becomes sharp as <span class="math inline">\(n\to \infty\)</span>. This motivates
the construction of the <strong>bounded distance encoder</strong>
<span class="math display">\[
    g_{\mathrm{BD}}(y) = \begin{cases}
        1 &amp; \text{if $c_j$ is a unique codeword in $B_r(y)$} \\
        e &amp; \text{otherwise}
    \end{cases}
\]</span>
Shannon had the idea to construct random codewords
<span class="math inline">\(c_1, \dots, c_M\)</span> uniformly from the Hamming cube. Then
<span class="math display">\[\begin{align}
    \mathbb E\epsilon_2(c_1, \dots, c_M)
    &amp;\leq \sum_{j=2}^M \mathrm{Pr}[d_M(c_j, Y) \leq r | X=c_1]
    = (M-1) \mathrm{Pr}[d_M(c_1, Y) \leq r] \\
    &amp;= \dfrac{M-1}{2^n} | B_r(0)|
\end{align}\]</span>
Note crucially that <span class="math inline">\(C_j \perp\!\!\!\perp Y\)</span>. The second
error rate <span class="math inline">\(\epsilon_2\)</span> vanishes if <span class="math inline">\(M\ll 2^{nC^{(i)}}\)</span>.
This implies, for BSC, <span class="math inline">\(C = C^{(i)} = \log 2 - h(\delta)\)</span>.</p>
</div>
<div id="nov-18-channel-coding-iii" class="section level2 unnumbered hasAnchor">
<h2>Nov 18, Channel Coding III<a href="lecture-notes.html#nov-18-channel-coding-iii" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Finish DT bound.</li>
<li>DT bound with input constraints.</li>
<li>General channels with input constraints.</li>
<li>Capacity of Gaussian channels.</li>
</ol>
<p>TODO for notes:</p>
<ol style="list-style-type: decimal">
<li>Equivalence between maximum and average error.</li>
<li>DT bound.</li>
</ol>
<p>Other questions:</p>
<ol style="list-style-type: decimal">
<li>More intuition for the strong v.s. weak converse.</li>
<li>Explain <span class="math inline">\(\liminf\)</span>, also why is <span class="math inline">\(C_0\)</span> a combinatorial problem,
different from <span class="math inline">\(C_{\to 0}\)</span>.</li>
<li>Wasserstein distance.</li>
</ol>
<p>Pset questions:</p>
<ol style="list-style-type: decimal">
<li>Pointwise maximization.</li>
<li>Existence of the extension.</li>
<li>Idea for applying the meta-converse.</li>
</ol>
<p>Recall the information density:
given a joint distribution <span class="math inline">\(P_{XY}\)</span>, the information
density is the joint.</p>
<p>For the BSC, the channel only gives half the required
quantitites for computing information density (i.e. the channel).
We still need the input distribution. Take <span class="math inline">\(P_{X^n}\)</span> to uniform
on the Hamming cube, then
<span class="math display">\[
    i(X^n; ^n) = \log \dfrac{P_{Y^n|X^n}}{P_{Y^n}}
    = \log \dfrac{\delta^{d_M} \bar \delta^{-n-d_M}}{2^{-n}}
    = n\log (2\bar \delta) - d_M \log \dfrac{\bar \delta}{\delta}
\]</span>
This makes explicit the connection that ML decoder is equivalent
to minimizing the Hamming distance <span class="math inline">\(d_M\)</span>.</p>
<ul>
<li>For memoryless channels, the information density tensorizes.</li>
<li>This allows us to apply LLN (or large-deviations theory).</li>
</ul>
<p>We can generalize the notion from stationary memoryless channels,
since information stability (tensorization and concentration about average)
is sufficient to prove capacity.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-129" class="theorem"><strong>Theorem 12.11  (random coding DT bound) </strong></span><span class="math inline">\(\forall P_{Y|X}\)</span> and <span class="math inline">\(P_X, M\geq 1\)</span>, there exists <span class="math inline">\((M, \epsilon)\)</span>-code
such that
<span class="math display">\[
    \epsilon \leq \mathbb E\exp -\max\left[0, i(X; Y) - \log \dfrac{M-1}{2}\right]
\]</span></p>
</div>
<p><em>Proof:</em> Treating the codebook as a random variable, a decoding error of message <span class="math inline">\(j\)</span>
can happen if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(i(C_j; Y) \leq \gamma\)</span>: the info density of the true codeword is not enough.
In BSC, this is probability that Hamming distance between true message and
received message is larger than the threshold.</li>
<li>There exists another codeword which exceeds <span class="math inline">\(\gamma\)</span> prior to <span class="math inline">\(j\)</span>. <strong>Since we
are taking the average over random codebooks, this terms becomes tractable.</strong>
In BSC, this corresponds to some other codewords existing in the thresholded ball.</li>
</ol>
<p>Proceeding to bound the first term,
<span class="math display">\[
    P_e = \sum_{x, y} P_{XY}(x, y) 1 \left\{
        P_{XY} \leq e^\gamma P_{\bar X Y}
    \right\} + \dfrac{M-1}{2} P_{\bar XY}(x, y) 1\left\{
        P_{XY} &gt; e^\gamma P_{\bar XY}
    \right\}
\]</span>
To minimize this, the optimal selection is <span class="math inline">\(\gamma = \log \dfrac{M-1}{2}\)</span>;
this construction of events selects the minimum of the two contributions.</p>
<p>Perspective on information theory: when proving results, complexity do not
matter. When results are significant, someone will find a computationally feasible way : )</p>
<div id="decoding-with-constraint" class="section level3 unnumbered hasAnchor">
<h3>Decoding with constraint<a href="lecture-notes.html#decoding-with-constraint" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The capacity of the AWGN without any constraint is <span class="math inline">\(\infty\)</span>,
since we can space inputs out significantly large enough so that
the added noise cause negligible overlap.</p>
<p>The constraint <span class="math inline">\(\|C\|^2 \leq nP\)</span> yields finite capacity <span class="math inline">\(C=\dfrac 1 2 \log(1+P)\)</span>.</p>
<p>We wish to find a codebook <span class="math inline">\(c_1, \dots, c_M\)</span> such that <span class="math inline">\(P_e(C) \leq \epsilon\)</span>,
and that <span class="math inline">\(\|C_j\|^2 \leq nP\)</span>. In short, we have to place our codeword
in a sphere of radius <span class="math inline">\(\sqrt{nP}\)</span>. Each codeword will be roughly
spent to a sphere of radius <span class="math inline">\(\|\mathcal N(0, I_n)\| \approx \sqrt n\)</span>.
The output will land with high probability within the sphere with radius
<span class="math display">\[
    \mathbb E\|C + \mathcal N(0, I_n)\|^2 \approx n(1+p)
\]</span>
How many spheres <span class="math inline">\(\sqrt n B^{n-1}\)</span> can we pack inside <span class="math inline">\(\sqrt{n(1+p)}B\)</span>?
An upper bound is <span class="math inline">\((1+p)^{n/2}\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-130" class="definition"><strong>Definition 12.5  (channel with constraints) </strong></span>Given a channel and a cost <span class="math inline">\(c:\mathcal X\to \mathbb R_+\)</span>,
an encoder-decoder <span class="math inline">\((f, g)\)</span> is an <span class="math inline">\((n, M, \epsilon, P)\)</span> code if
<span class="math display">\[
    \dfrac 1 n \sum_j \mathrm{Pr}[g(y)\neq j|X=f(j)] \leq \epsilon, \quad
    \forall j, c_n(f(j)) \leq nP
\]</span>
<span style="color:green">
Note that we are amortizing w.r.t. blocklength and randomness of the encoder,
but the constraint should be uniformly obeyed by all elements of the alphabet.
</span></p>
</div>
<p>For AWGN, <span class="math inline">\(c(x)=x^2\)</span>. Further, for memoryless channels, the constrained
capacity is
<span class="math display">\[
    C^{(I)}(P) = \sup_{\mathbb Ec(X_1)\leq P} I(X_1; Y_1)
\]</span></p>
<div class="proposition">
<p><span id="prp:unlabeled-div-131" class="proposition"><strong>Proposition 12.9  (properties of capacity-cost function) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P\mapsto C^{(I)}(P)\)</span> is concave, increasing, and continuous on interior.</li>
<li><span class="math inline">\(C(P)\leq C^{(I)}(P)\)</span>.</li>
</ol>
</div>
<p><em>Proof:</em> Concavity follows by averaging the capacity-achieving distributions.
The second part follows from applying the weak converse.
Take a <span class="math inline">\((n, M, \epsilon, P)\)</span>-code, .</p>
</div>
</div>
<div id="nov-20.-quantization" class="section level2 unnumbered hasAnchor">
<h2>Nov 20. Quantization<a href="lecture-notes.html#nov-20.-quantization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Finish capacity of Gaussian cahannel with cost</li>
<li>Quantization: scalar case</li>
<li>Definition of vector case</li>
</ol>
<p>Channel coding with constraint:</p>
<p><span class="math display">\[
    C^{(I)}(P) = \liminf_{n\to \infty} \dfrac 1 n \sup_{
        \mathbb E\left[\frac 1 n \sum c(X_t)\right]
    } I(X^n; Y^n)
\]</span>
By (1) the concavity of the capacity-cost function, and
(2) tensorization of MI, we can tensorize both the extremized
quantity as well as the condition.</p>
<p>Due to the concavity of the capacity-cost function,
maximum communication per unit cost is achieved by having vanishing rate
since <span class="math inline">\(C^{(I)}(P) / P\)</span> is maximized <span class="math inline">\(P\to 0\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-132" class="theorem"><strong>Theorem 12.12  (Fano's inequality under constraint) </strong></span><span class="math inline">\(C(P) \leq C^{(I)}(P)\)</span>.</p>
</div>
<p>The information stability criterion
<span class="math display">\[
    \dfrac 1 n i(X^n; Y^n)\xrightarrow{\mathbb P} C^{(I)}(P)
\]</span>
is equivalent to demanding the equivalent analogue of
law of large numbers holding for non-stationary channels.</p>
<p>For AWGN, we have <span class="math inline">\(C^{(I)}(P) = \dfrac 1 2 \log(1+P)\)</span>,
and recalling our previous discussion,
<span class="math display">\[
    \sup_P \dfrac{\frac 1 2 \log(1+P)}{P} = \dfrac{\log e}{2}
    = -1.6\mathrm{dB} \dfrac{\mathrm{bit}}{\mathrm{Joule}}
\]</span>
is the a fundamental limit in deep-space communication.</p>
<div id="water-filling" class="section level3 unnumbered hasAnchor">
<h3>Water-filling<a href="lecture-notes.html#water-filling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a non-stationary memoryless Gaussian channel
<span class="math display">\[
    Y_t = X_t + Z_t, \quad Z_t \sim \mathcal N(0, \sigma_t^2)
\]</span>
Using tensorization, we obtain
<span class="math display">\[
    I(X^n; Y^n) \leq \sum_t I(X_t; X_t + Z_t)
\]</span>
with inequality saturated by independent Gaussians.
Fixing the power allocation <span class="math inline">\(P_t\)</span> for each letter, we obtain
<span class="math display">\[
    I(X^n; Y^n) \leq \dfrac 1 2 \sum_{t=1}^n \log \left(
        1 + \dfrac{P_t}{\sigma_t^2}
    \right) \text{ subject to } \sum P_t \leq n P
\]</span>
This is a convex problem. Using Lagrange multipliers,
we obtain
</p>
<p>Consider a non-white Gaussian noise channel
<span class="math inline">\(Y_t = X_t + Z_t\)</span>, where <span class="math inline">\(Z_1, \dots, X_n, \dots\)</span> is a
zero-mean stationary Gaussian process, which means:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb EZ_t^2 = \sigma^2, \forall t\)</span>.</li>
<li><span class="math inline">\(\mathbb E[Z_t Z_{t+k}] = \psi(k)\)</span> is a function of <span class="math inline">\(k\)</span> only.
The auto-correlation can be represented as a Fourier transform
<span class="math display">\[
\psi(k) = \dfrac 1 {2\pi} \int_{-\pi}^\pi \cos(k\omega) f_z(\omega)\, d\omega
\]</span>
In this case <span class="math inline">\(f_z\)</span> is the power spectral density (PSD) of noise.</li>
</ol>
<p>To compute the capacity, note that we cannot apply tensorization
anymore since the channel is not memoryless. The main idea is to
diagonalize the covariance matrix by rotating the input. Effectively,
we whiten the channel noise by jointly transforming the input and channel.</p>
<p>It turns out that the eigenvalues of the covariance matrix
<span class="math inline">\((\Sigma_n)_{kl} = \psi_n(k, l)\)</span> has very nice structure, since
the covariance matrix is a Toeplitz matrix.</p>
<p>In short, we obtain
<span class="math display">\[
    C(P) = \dfrac 1 2 \int{-\pi}^\pi \log_+ \dfrac{T}{f_z(w)}\, dw, \quad
    \int |T - f_Z(\omega)|_+\, d\omega = P
\]</span></p>
</div>
<div id="metric-entropy" class="section level3 unnumbered hasAnchor">
<h3>Metric-entropy<a href="lecture-notes.html#metric-entropy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a Hamming game:</p>
<ol style="list-style-type: decimal">
<li>Player A writs down 100 bits distributed i.i.d.</li>
<li>Player B looks at the 100 bits, then writes down 50 bits.
The original 100 bits are erased.</li>
<li>Player C looks at the written 50 bits, then tries to reconstruct the 100 bits.</li>
</ol>
<p>Approaches to solving this problem:</p>
<ol style="list-style-type: decimal">
<li>Record 50 bits, this yields 50% error.</li>
<li>Compress the 100 bits.</li>
<li>The optimal answer is 11% error.</li>
</ol>
<p>The key idea is that, due to asymptotic equipartition property, we can
stare at typical sequences. The main idea is to try covering the
Hamming cube <span class="math inline">\(\{0, 1\}^{100}\)</span> using <span class="math inline">\(2^{50}\)</span> Hamming balls with
radius as small as possible. The decoding then looks at the indexed sphere
and outputs its center.</p>
</div>
</div>
<div id="nov-25-rate-distortion-theorem" class="section level2 unnumbered hasAnchor">
<h2>Nov 25: Rate-distortion theorem<a href="lecture-notes.html#nov-25-rate-distortion-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Scalar quantization.</li>
<li>Define <span class="math inline">\(R(D)\)</span>.</li>
<li>Random coding bound.</li>
<li>(Next time): asymptotics.</li>
</ol>
<div id="scalar-quantization" class="section level3 unnumbered hasAnchor">
<h3>Scalar quantization<a href="lecture-notes.html#scalar-quantization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two change of perspectives in transmitting continuous signals:</p>
<ol style="list-style-type: decimal">
<li>PCM (A. Reeves): Binning of continuous numbers.</li>
<li>Sampling (Nyquist): without loss of generality
we can consider discrete signals, i.e. discretizing the
time axis.</li>
</ol>
<p>Quantization is the replacement of <span class="math inline">\(\mathbb R\)</span>-valued things with discrete things.
Given a signal <span class="math inline">\(X_1^n = [X_1, \dots, X_n]\in \mathbb R^n\)</span>, map to
<span class="math inline">\(f_S(X_1^n)=\hat X_1^n \in [M]^n\subset \mathbb R^n\)</span>. We wish to minimize
<span class="math display">\[
    \mathbb E|X - \hat X|^2
\]</span></p>
<ol style="list-style-type: decimal">
<li>First idea: split into quantiles, this maximizes the entropy of the codeword.
However, this turns out not to be the best idea: why?</li>
<li>Second idea: uniform quantization, just split the compact domain
into equi-measure components.</li>
</ol>
<p>For uniform quantization, the limit <span class="math inline">\(M\to \infty\)</span> makes the conditional
distribution of <span class="math inline">\(X\in \Delta_j\)</span> roughly uniform. Let <span class="math inline">\(|\Delta_j| = \dfrac{2A}{M},
M=2^R\)</span>, we obtain
<span class="math display">\[\begin{align}
    \mathbb E|X - \hat X|^2
    &amp;= \sum_j \mathrm{Pr}[X\in \Delta_j] \mathbb E_{|X\in \Delta_j} |X - \hat X|^2 \\
    &amp;= \sum_{j=1}^M \mathrm{Pr}[X\in \Delta_j] \dfrac{A^2} {3M^2}
    = \dfrac{A^2}{3M^2} = \dfrac{A^2}{3} 2^{-2R}
\end{align}\]</span>
The improvement of 1 bit in quantization leads to 4-fold drop (6dB) in rmse.</p>
<p>Panter-Dite ’51 asked the question: what is the best possible
way to arrange <span class="math inline">\(c_j\)</span>’s? Again, the discrete combinatorial problem is hard:
even for <span class="math inline">\(M=4\)</span> and Gaussian the solution is unknown.</p>
<ul>
<li>Fixing a partition <span class="math inline">\(c_j\)</span>, the decision rule is easy: just pick the nearest point!</li>
<li>Fixing <span class="math inline">\(\Delta_j\)</span>’s, the optimal <span class="math inline">\(c_j\)</span> are the centroids i.e. conditional means
<span class="math inline">\(c_j\)</span>
<ul>
<li>Note that, fixing <span class="math inline">\(\Delta_j\)</span>’s and computed <span class="math inline">\(c_j\)</span>’s, the quantization
intervals <span class="math inline">\(\Delta_j\)</span>’s are not optimal anymore. This leads to Lloyd-Max:
start with <span class="math inline">\(c_j\)</span>’s, compute <span class="math inline">\(\Delta_j\)</span>’s, then compute <span class="math inline">\(c_j\)</span>’s, and
repeat by computing <span class="math inline">\(\Delta_j\)</span>’s again such that they’re equidistant
from the <span class="math inline">\(c_j\)</span>’s.</li>
<li>Lloyd-Max yields the <strong>CVT</strong> (Centroidal Voronoi Tesselation), and
the optimal quantization is always a CVT.
Lloyd-Max yields a (not-necessarily optima) CVT.</li>
</ul></li>
<li>One key idea is to consider the point density <span class="math inline">\(\lambda\geq 0\)</span> with
<span class="math inline">\(\int \lambda = 1\)</span>.</li>
</ul>
<p>In the continuum limit of the entropy-maximizing method and fixing <span class="math inline">\(\lambda(x)\)</span>,
let’s parameterize <span class="math inline">\(c_j\)</span> in terms of a free parameter <span class="math inline">\(\lambda_j\)</span> by the following equation:
<span class="math display">\[\begin{align}
    c_j
    &amp;= \int_{-\infty}^{c_j} = \dfrac j m \\
    |\Delta_j| \cdot \lambda(c_j)
    &amp;\approx \dfrac 1 M \implies |\Delta_j| = \dfrac 1 {M \lambda(c_j)} \\
    \mathbb E|X - \hat X|^2
    &amp;=\sum \mathrm{Pr}[X\in \Delta_j] \mathbb E_{|X\in \Delta_j} |X - c_j|^2 \\
    &amp;\approx \sum \mathrm{Pr}[\lambda \in \Delta_j] \dfrac{|\Delta_j|^2}{12}
    = \sum \mathrm{Pr}[X \in \Delta_j] \dfrac 1 {12M^2} \dfrac 1 {\lambda^2(c_j)} \\
    &amp;\approx \dfrac 1 {12M^2} \int p_X(x) \dfrac 1 {\lambda^2(x)}\, dx
\end{align}\]</span>
This variational problem can be solved to yield <span class="math inline">\(\lambda^*(x) = c \cdot p(x)^{1/3}\)</span>.
<strong>This generalizes to non-quadratic losses</strong>!
In other words, to extremize the rmse, we should maximize the entropy
(i.e. have uniform distribution) of <span class="math inline">\(f_X^{1/3}\)</span>. It turns out that
<span class="math display">\[
    D_{\mathrm{scalar}}(R) = \dfrac{\pi \sqrt 3}{2} \sigma^2 2^{-2R}, \quad R\gg 1
\]</span>
<strong>Quantization for quantum systems?</strong></p>
</div>
<div id="vector-quantization" class="section level3 unnumbered hasAnchor">
<h3>Vector quantization<a href="lecture-notes.html#vector-quantization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By <strong>vector quantization</strong>, we mean i.i.d <span class="math inline">\(X_j\)</span>. For i.i.d Gaussians,
we obtain <span class="math inline">\(1.65\)</span> better in RMSE, or 0.72 bit / sample
better (i.e. <span class="math inline">\(R\mapsto R+0.72\)</span> kills the constant factor).
<span class="math display">\[
    D(R) = \sigma^2 2^{-2R}, \quad \forall R&gt;0
\]</span>
Given <span class="math inline">\(X^n\sim P_X\)</span> i.i.d on <span class="math inline">\(\mathcal A\)</span> with reconstruction alphabet <span class="math inline">\(\hat A\)</span>,
the distortion metric <span class="math inline">\(d(x, \hat x):\mathcal A\times \hat {\mathcal A}\to \bar{\mathbb R}\)</span>.
I.i.d. assumption is not as important, but the separability of distortion metric is:
<span class="math display">\[
    d(x^n, \hat x^n) = \dfrac 1 n \sum_{t=1}^n d(x_t, \hat x_t)
\]</span>
Define an <span class="math inline">\((n, M, D)\)</span>-code (or vector quantizer) to be the pair <span class="math inline">\((f, g)\)</span> with
<span class="math display">\[
    X^n\xrightarrow f W\in [M] \xrightarrow g \hat X^n\in \hat{\mathcal A}
\]</span>
such that <span class="math inline">\(\mathbb E[d(X^n, \hat X^n)] \leq D\)</span>. Every scalar quantizer can be tensorized
into a vector quantizer. Similarly, define the fundamental limit
<span class="math display">\[
    M^*(n, D) = \inf_{\exists(N, M, D)-\mathrm{code}} M
\]</span>
Note that this is subadditive, since
<span class="math display">\[
    \log M^*(n_1+n_2, D)\leq \log M^*(n_1, D) + \log M^*(n_2, D)
\]</span>
The rate distortion function is defined as the normalized
limit of this function
<span class="math display">\[
    R(D) = \lim_{n\to \infty} \dfrac 1 n \log M^*(n, D)
\]</span>
For the non-separable metric <span class="math inline">\(d_n(X^n, \hat X^n) = 1_{X^n=\hat X^n}\)</span>
we obtain lossless compression. The information version of the distortion rate is
<span class="math display">\[\begin{align}
    R^{(I)}(D)
    &amp;= \lim_{n\to \infty} \dfrac 1 n
    \inf_{P_{\hat X^n|X^n} | \mathbb Ed_n(X^n, \hat X^n) \leq D}
    I(X^n; \hat X^n)
\end{align}\]</span>
For i.i.d. source and separable distortion, applying tensorization yields
<span class="math display">\[
    R^{(I)}(D) = \inf_{\mathbb Ed(X_1, \hat X_1)\leq D} I(X_1; \hat X_1)
\]</span>
Moreover, the map <span class="math inline">\(D\mapsto R^{(I)}(D)\)</span> is convex (follows by mixing the
rate-achieving channels and applying convexity of MI).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-133" class="theorem"><strong>Theorem 12.13  </strong></span>For every <span class="math inline">\((n, M, D)\)</span>-code, <span class="math inline">\(\log M \geq n R^{(I)}(D)\)</span>.</p>
</div>
<p>By definition, <span class="math inline">\(I(X^n; \hat X^n) \geq R^{(I)}(D)\)</span>;
on the other hand, the MI is upper-bounded by the
maximal entropy of the bottleneck <span class="math inline">\(\log M\)</span>.</p>
<p>The converse bound (analogous to shannon’s noisy coding theorem)
is obtained by a similar random coding technique.</p>
</div>
</div>
<div id="dec-4-density-estimation" class="section level2 unnumbered hasAnchor">
<h2>Dec 4: Density estimation<a href="lecture-notes.html#dec-4-density-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Review of metric entropy.</li>
<li><span class="math inline">\(\log N \propto 1/\epsilon\)</span>.</li>
<li>Density estimation.</li>
<li>Yang-Barron method.</li>
<li>Lower bound.</li>
</ol>
<p>Rule of thumb: the minimax rate of a class
<span class="math display">\[
    R_n^* \approx \inf_\epsilon \epsilon^2 + \dfrac 1 n \log N(\epsilon)
\]</span></p>
<div id="review-of-metric-entropy" class="section level3 unnumbered hasAnchor">
<h3>Review of metric entropy<a href="lecture-notes.html#review-of-metric-entropy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given <span class="math inline">\(\Theta\subset V\)</span> a metric space, define the metric entropy
<span class="math inline">\(N(\epsilon; \Theta)\)</span> to be the minimum number of <span class="math inline">\(\epsilon\)</span>-balls
which cover <span class="math inline">\(\Theta\)</span>. This characterizes how “compact” the space <span class="math inline">\(\Theta\)</span> is.</p>
<p>Similarly, the packing number <span class="math inline">\(M(\epsilon; \Theta)\)</span> is the maximum number
of pointwise <span class="math inline">\(\epsilon\)</span>-far points contained in <span class="math inline">\(\Theta\)</span>. We have shown that
<span class="math display">\[
    M(\epsilon) \geq N(\epsilon) \geq M(2\epsilon)
\]</span>
We also considered the class <span class="math inline">\(\mathcal F_\beta^d(C)\)</span> as the class of
pdf’s such that <span class="math inline">\(f^{\lfloor \beta\rfloor}\)</span> is <span class="math inline">\((\beta-\lfloor \beta\rfloor)\)</span>-Holder.
An <span class="math inline">\(\alpha\)</span>-Holder function satisfies <span class="math inline">\(|g(x) - g(y)| \leq C^\alpha |x-y|^\alpha\)</span>.</p>
<p>Foundational theorem: for <span class="math inline">\(L_{p\in [1, \infty]}\)</span>-distances,
<span class="math inline">\(\log N(\epsilon, \mathcal F_\beta^d) = \Omega\, (1/\epsilon)^{d/\beta}\)</span>.</p>
<p>One immediate implication is the proof (of non-existence) of
Hilbert’s 13-th problem for <span class="math inline">\(1\)</span>-Lipschitzz problems.</p>
<p>Consider samples <span class="math inline">\(X_1, \dots, X_n\)</span> drawn i.i.d from
<span class="math inline">\(f\in \mathcal F_\beta^d\)</span> on <span class="math inline">\([0, 1]^d\)</span>, or the collection of
<span class="math inline">\(\lceil \beta\rceil\)</span>-differentiable densities which are <span class="math inline">\((\beta - \lfloor \beta\rfloor)\)</span>-Holder.</p>
<p>Define the minimax rate to be
<span class="math display">\[
    R_{n, \mathrm{dist}}^*(\mathcal F_\beta^d)
    = \inf_{\hat f_n:X^n\to \mathcal F_\beta^d} \sup_{f\in \mathcal F_\beta^d}
    \mathbb E\, \mathrm{dist}(\hat f_n, f)
\]</span>
Note that <span class="math inline">\(L_{p\neq 1}\)</span> are nonstatistical as in a homogeneous rescaling
of the whole space will change the distance.
The foundational result in nonparametric statistics is as follows,
<span class="math display">\[
    R^*_{n, L_p} = \Omega\, n^{-\beta/(2\beta + d}
    = \left(\dfrac{\log n}{n}\right)^{\beta/(2\beta+d)} \text{ for }p=\infty
\]</span>
Today we will prove for <span class="math inline">\(p=2\)</span>. Note that for <span class="math inline">\(p\to \infty\)</span>,
the rate goes as <span class="math inline">\(\sqrt{1/n}\)</span>, which is a parametric rate!</p>
<p>Proceeding to the proof, define the contracted class
<span class="math display">\[
    F_\beta&#39; = \dfrac 1 2 U + \dfrac 1 2 \mathcal F_\beta
\]</span></p>
<p>We first work with a contracted class with common support.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-134" class="lemma"><strong>Lemma 12.3  </strong></span><span class="math inline">\(R_{n, L_p}^*(\mathcal F_\beta&#39;)\)</span> is the same as that
of <span class="math inline">\(\mathcal F_\beta\)</span> up to a factor of <span class="math inline">\(2\)</span>.</p>
</div>
<p><em>Proof:</em> One direction is trivial:
<span class="math inline">\(\mathcal F_\beta&#39;\subset \mathcal F_\beta \implies R_n^*(\mathcal F_\beta&#39;) \leq
R_n^*(\mathcal F_\beta)\)</span>. Next, given data from <span class="math inline">\(\mathcal F_\beta\)</span>,
use <span class="math inline">\(R_{n, L_p}^*(\mathcal F_\beta&#39;)\)</span> to select an estimator for
the mixture with <span class="math inline">\(U\)</span> (which belongs in <span class="math inline">\(\mathcal F_\beta&#39;\)</span>) and apply
Markov’s inequality.</p>
<p><span style="color:green">
Usually, the bottleneck in proving entropic characterizations is proving
<span class="math inline">\(H^2 \approx D\)</span>. Yang-Barron is one of the first applications
of information theory to obtain a statistical upper-bound.
</span></p>
<div class="lemma">
<p><span id="lem:unlabeled-div-135" class="lemma"><strong>Lemma 12.4  </strong></span><span class="math inline">\(\forall f, g\in \mathcal F_\beta&#39;\)</span>, up to global constants
the <span class="math inline">\(H^2, KL, \chi^2, L_2^2\)</span> are equivalent.</p>
</div>
<p><em>Proof:</em></p>
<ol style="list-style-type: decimal">
<li>First note that <span class="math inline">\(f, g\geq 1/2\)</span>, and there exists a upper bound <span class="math inline">\(C_{\beta, d}\)</span>
such that <span class="math inline">\(f, g\leq C_{\beta, d}\)</span>.</li>
<li>This immediately yields <span class="math inline">\(\chi^2 \sim L_2\)</span> using the universal constant
<span class="math display">\[
\chi^2(f\|g) = \int \dfrac{(f-g)^2}{f} \approx \int (f-g)^2
\]</span>
The same applies to Hellinger with <span class="math inline">\((\sqrt x-1)^2 \geq c(x-1)^2\)</span>, so
<span class="math inline">\(H^2 \geq c\cdot L_2^2\)</span>.</li>
</ol>
<p>Proceeding to prove the Yang-Barron method, we first use the
online-to-batch conversion: given <span class="math inline">\(X_j\)</span> sampled i.i.d from <span class="math inline">\(P_{\theta\in \Theta}\)</span>
and an estimator <span class="math inline">\(\hat \theta_n(X_1, \dots, X_n)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>The batch minimax risk <span class="math inline">\(R_n^*(\Theta) = \inf_{\hat \theta_n}
\sup_{\theta\in \Theta} \mathbb E\, l(\theta, \hat \theta_n)\)</span>.</li>
<li>The online (cumulative) risk
<span class="math display">\[
C_n^*(\Theta) = \inf_{\hat \theta_1, \dots, \theta_n}
\sup_{\theta \in \Theta} \sum_{t=1}^n \mathbb E\, l(\theta, \hat \theta_t)
\]</span></li>
</ol>
<div class="proposition">
<p><span id="prp:unlabeled-div-136" class="proposition"><strong>Proposition 12.10  </strong></span><span class="math inline">\((n+1)R_n^*(\Theta) \leq C_{n+1}(\Theta)\)</span>. Moreover,
if <span class="math inline">\(C_n \approx n^{\alpha&gt;0}\)</span>, then <span class="math inline">\(R_n^*\approx n^{\alpha - 1}\)</span>.</p>
</div>
<p>The main constructive step in Yang-Barron is to take <span class="math inline">\(\theta\leftrightarrow f\)</span>,
and <span class="math inline">\(\hat \theta_t \leftrightarrow \hat f_t(\cdot; X_1, \dots, X_{t-1})\)</span>.
Take <span class="math inline">\(l(\theta, \hat \theta_t) = D(f\|\hat f_t)\)</span>.</p>
<ul>
<li>The batch loss is exactly the <span class="math inline">\(L_2\)</span> risk for KL.</li>
<li>What about the cumulative loss?
<span class="math display">\[\begin{align}
C_n(\mathcal F)
&amp;= \inf_{\hat f_0, \dots, \hat f_{n-1}} \sup_f \sum_{t=0}^{n-1} D(f\|\hat f_t) \\
&amp;= \inf \sup D(f^{\otimes n} \| \hat f_0\times \dots \times \hat f_{n-1})
\end{align}\]</span>
But the choice of <span class="math inline">\(\hat f_0, \dots, \hat f_{n-1}\)</span>.
The cumulative minimax risk in KL for density estimation is
<span class="math display">\[
C_n(\mathcal F) = \inf_{Q_{X^n}} \sup_{f\in \mathcal F} D(f^{\otimes n} \| Q_{X^n})
= \sup_{\pi \in \mathcal P(\mathcal F)} I(f; X^n)
\]</span>
Note that we can upper-bound the capacity using the metric entropy.</li>
</ul>
<p>The final Yang-Barron estimator is an online exponential-</p>
<p>A recap of the proof sketch:</p>
<ol style="list-style-type: decimal">
<li>Reduce <span class="math inline">\(\mathcal F_\beta\to \mathcal F_\beta&#39;\)</span>
to a manageable class with common support.</li>
<li>Demonstrate that on <span class="math inline">\(\mathcal F_\beta&#39;\)</span>, KL and <span class="math inline">\(L^2\)</span>
are equivalent up to universal constants.</li>
<li>Apply the online-to-batch reduction and recognize the
cumulative risk in terms of metric entropy.</li>
</ol>
<p>Proceeding to prove the lower-bund, given a family <span class="math inline">\(\Theta\)</span>,
let <span class="math inline">\(M_H(\epsilon, \Theta)\)</span> be the packing number unfer Hellinger.
The capacity
<span class="math display">\[
    C_n(\Theta) \geq \min \left[\dfrac{\log e}{2} n \epsilon^2,
    \log M_H(\epsilon)\right] - \log 2
\]</span>
To prove the lower bound, take <span class="math inline">\(\theta_1, \dots, \theta_M\)</span>
such that <span class="math inline">\(H^2(P_{\theta_j}, P_{\theta_k}) \geq \epsilon^2\)</span>.
Recall that there exists hypothesis test for
<span class="math inline">\(P_{\theta_j}^{\otimes n}\)</span> against <span class="math inline">\(P_{\theta_k}^{\otimes n}\)</span> such that
(using TV-<span class="math inline">\(H^2\)</span> comparison and tensorization of <span class="math inline">\(H^2\)</span>):
<span class="math display">\[
    P_e \leq \exp \left[
        n H^2(P_{\theta_j}, P_{\theta_k})/2
    \right]
\]</span>
To conduct <span class="math inline">\(M\)</span>-ary hypothesis, take the decoded value <span class="math inline">\(\hat \theta\)</span>
to be the winner of all pairwise tests (else error), then
<span class="math display">\[
    \mathrm{Pr}[\theta \neq \hat \theta] \leq M e^{-n\epsilon^2/2}
\]</span>
Suppose <span class="math inline">\(\mathrm{Pr}[\theta \neq \hat \theta]&lt;1/2\)</span>,
then use Fano’s inequality to lower-bound the mutual information
<span class="math display">\[
    C_n \geq I(\theta; X^n) \geq I(\theta; \hat \theta) \geq
    \dfrac 1 2 \log M - \log 2
\]</span></p>
</div>
</div>
<div id="dec-9.-strong-dpi" class="section level2 unnumbered hasAnchor">
<h2>Dec 9. Strong DPI<a href="lecture-notes.html#dec-9.-strong-dpi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Wrap-up relation between statistics and information theory.</li>
<li>Definition of SDPI.</li>
<li>Less noisy preorder.</li>
<li>Spiked Wigner model.</li>
</ol>
<div id="minimax-stats" class="section level3 unnumbered hasAnchor">
<h3>Minimax stats<a href="lecture-notes.html#minimax-stats" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that we wish to estimate <span class="math inline">\(\theta\in \Theta\)</span>
and observe <span class="math inline">\(X\sim P_\theta\)</span>.
The metric is <span class="math inline">\(\mathbb El(\theta, \hat \theta)\)</span> to be minimized.</p>
<p>Density estimation falls under this example: <span class="math inline">\(\theta\)</span> is a pdf,
the observation consists of i.i.d. samples, and the metric is
KL, <span class="math inline">\(H^2\)</span>, or TV.</p>
<p><strong>Lower bound:</strong> If there exists <span class="math inline">\(\epsilon\)</span>-estimator <span class="math inline">\(\hat \theta\)</span>
for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(I(\theta; \hat \theta)\leq I(\theta; X)\)</span>
implies that the metric entropy of <span class="math inline">\(\Theta \leq\)</span> the capacity of
<span class="math inline">\(\theta\to X\)</span> channel. This is called the M.I.M.</p>
<ul>
<li>The metric entropy of <span class="math inline">\(\Theta\)</span> can be
bounded by a packing of <span class="math inline">\(\Theta\)</span> in the <span class="math inline">\(l\)</span>-metric.</li>
<li>The capacity of <span class="math inline">\(\theta\to X\)</span> can be bounded by a covering in KL.</li>
</ul>
<p>The difficulty of learning (statistically estimating)
high-dimensional statistical classes may be captured by metric entropy.
The metric entropy gives us a rough estimate of “how hard” it is
to estimate a class.</p>
</div>
<div id="combinatorial-statistics" class="section level3 unnumbered hasAnchor">
<h3>Combinatorial statistics<a href="lecture-notes.html#combinatorial-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall the DPI graph <span class="math inline">\((P_X, Q_X)\xrightarrow {P_{Y|X}} (P_Y, Q_Y)\)</span>
and <span class="math inline">\(D(P_X\|Q_X) \geq D(P_Y \| Q_Y)\)</span>. Strong DPI aims to prove the
existence of <span class="math inline">\(\eta&lt;1\)</span> such that
<span class="math display">\[
    \eta\, D(P_X\|Q_X) \geq D(P_Y\|Q_Y)
\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-137" class="definition"><strong>Definition 12.6  (SDPI constant) </strong></span>Given <span class="math inline">\(P_{Y|X}\)</span>, define the (input-independent) SDPI constant
<span class="math display">\[
    \eta_f(P_{Y|X}) = \sup \dfrac{D_f(Q_Y\|P_Y)}{D_f(Q_X\|P_X)}
\]</span>
the supremum is taken over all <span class="math inline">\(Q_X, P_X\)</span>
such that the expression makes sense. The input-dependent SDPI
constant <span class="math inline">\(\eta_f(P_X, P_{Y|X})\)</span> is the supremum of the same
ratio over <span class="math inline">\(Q_X\)</span>.</p>
</div>
<p>Intuition: <span class="math inline">\(\eta_{\mathrm{KL}}\)</span> is basically the modified log-sobelev inequalities,
and <span class="math inline">\(\eta_{\chi^2}\)</span> is the spectral gap for Markov chains.
We’ll be mostly focused on the input-independent SDPI constant.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-138" class="proposition"><strong>Proposition 12.11  </strong></span>The SDPI constant for <span class="math inline">\(f\)</span>-divergence and <span class="math inline">\(f\)</span>-MI are equivalent.</p>
</div>
<p>How do we show that SDPI is not 1?</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-139" class="proposition"><strong>Proposition 12.12  (properties of SDPI) </strong></span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\eta_{\mathrm{TV}} = \mathrm{diam}_{\mathrm{TV}} = \sup_{x, x&#39;} \mathrm{TV}(P_{Y|X=x}, P_{Y|X=x&#39;})\)</span>.</li>
<li><span class="math inline">\(\eta_f \leq \eta_{\mathrm{TV}}\)</span>.</li>
<li><span class="math inline">\(\eta_{\mathrm{KL}} = \eta_{\chi^2} = \eta_{H^2} = \eta_f\)</span>
for <span class="math inline">\(f\)</span> if <span class="math inline">\(f\)</span> is twice-differentiable and operator-convex.</li>
<li><span class="math inline">\(\eta_{\chi^2} = \sup_{P_X, f, g} \rho^2(f(X), g(Y))\)</span>
i.e. maximum correlation coefficient squared.</li>
<li>For <span class="math inline">\(\mathcal X\)</span> is binary, let <span class="math inline">\(P_{Y|X=0} = P_0\)</span> and <span class="math inline">\(P_{Y|X=1}=P_1\)</span>, then
<span class="math display">\[
\eta_{\mathrm{KL}} = \sup_{0&lt;\beta&lt;1} LC_\beta(P_0\| P_1)
\]</span></li>
<li>Up to a coefficient of <span class="math inline">\(1/2\)</span>, <span class="math inline">\(\eta_{\mathrm{KL}}\)</span> is always given by Hellinger.
<span class="math display">\[
\dfrac 1 2 H^2(P_0, P_1)\leq \eta_{\mathrm{KL}} \leq H^2(P_0, P_1)
\]</span></li>
<li>For any <span class="math inline">\(\mathcal X\)</span>, the SDPI constant <span class="math inline">\(\eta_{\mathrm{KL}}\)</span> is equal to that
of the worst binary subchannel (Bernoulli on two-atom support).</li>
<li>Corollary: <span class="math inline">\(\dfrac 1 2 \mathrm{diam}_{H^2} \leq \eta_{\mathrm{KL}}(P_{Y|X}) \leq \mathrm{diam}_{H^2}\)</span>.</li>
</ol>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-140" class="proposition"><strong>Proposition 12.13  </strong></span><span class="math inline">\(\eta_{KL}(\mathrm{BSC}_\delta) = (1-2\delta)^2\)</span>.</p>
</div>
<p>To prove this, use <span class="math inline">\(\eta_{\mathrm{KL}} = 1 - (1 - \frac 1 2 H^2(\mathrm{Ber}_\delta,
\mathrm{Ber}_{\bar \delta}))\)</span>. Achievability is proven by taking <span class="math inline">\(\mathrm{BSC}_{\alpha\to 1/2}\)</span>
and comparing MI.</p>
<div class="definition">
<p><span id="def:unlabeled-div-141" class="definition"><strong>Definition 12.7  (less noisy comparison) </strong></span><span class="math inline">\(P_{Y|X}\leq_{ln} P_{Z|X}\)</span> if <span class="math inline">\(\forall P_{U, X}\)</span> in the chain
<span class="math inline">\(U\to X\to (Y, X)\)</span>, we have <span class="math inline">\(I(U; Y) \leq I(U; Z)\)</span>.</p>
</div>
<p>We also have the following result:
<span class="math inline">\(\eta_{\mathrm{KL}}(P_{Y|X}) = 1 - \tau\iff P_{Y|X} \leq_{ln} \mathrm{Erasure}_\tau\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-142" class="theorem"><strong>Theorem 12.14  (tensorization) </strong></span>The less-noisy definition tensorizes:
<span class="math display">\[
    P_{Y_j|X_j} \leq_{ln} P_{Z_j|X_j} \implies \prod P_{Y_j|X_j} \leq_{ln} \prod P_{Z_j|X_j}
\]</span>
i.e. given <span class="math inline">\(I(U; Y_j)\leq I(U; Z_j)\)</span>, we wish to show <span class="math inline">\(I(U; Y_1, Y_2)\leq I(U; Z_1, Z_2)\)</span>.</p>
</div>
<p>To show this, use the chain
<span class="math display">\[\begin{align}
    I(U; Y_1, Y_2)
    &amp;= I(U; Y_1) + I(U; Y_2|Y_1)
    \leq I(U; Y_1) + I(U; Z_2|Y_1) = I(U; Y_1, Z_2)
\end{align}\]</span></p>
</div>
</div>
<div id="dec-11-sdpi-distributed-estimation" class="section level2 unnumbered hasAnchor">
<h2>Dec 11: SDPI, Distributed Estimation<a href="lecture-notes.html#dec-11-sdpi-distributed-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Agenda:</p>
<ol style="list-style-type: decimal">
<li>Review SDPI.</li>
<li>Finish Spiked Wigner.</li>
<li>Correlation estimation (Gap Hamming problem in TCS)
and distributed mean estimation (machine learning).</li>
</ol>
<p>Recall the S-DPI constant
<span class="math display">\[
    \dfrac 1 2 \mathrm{diam}_{H^2}\leq \eta_{\mathrm{KL}}(P_{Y|X})
    \leq \mathrm{diam}_{H^2}(P_{Y|X})
\]</span>
Also recall the less-noisy definition:</p>
<ul>
<li>For all <span class="math inline">\(P_{UX}\)</span>, we have <span class="math inline">\(I(U; X)\leq I(U; Z)\implies P_{Y|X}\prec_{ln}P_{Z|X}\)</span>, so
<span class="math inline">\(Z\)</span> is less noisy than <span class="math inline">\(Y\)</span>.</li>
<li>Consider another “natural” definition: forall <span class="math inline">\(P_X\)</span>, we have <span class="math inline">\(I(X; Y) \leq I(X; Z)\)</span>;
this says that <span class="math inline">\(P_{Y|X}\prec_{mc}P_{Z|X}\)</span> i.e. <span class="math inline">\(Z|X\)</span> is more capable than <span class="math inline">\(Y|X\)</span>.</li>
</ul>
<p>Why is less noisy better than more capable? First, it’s stronger, and it additionally
ensures that not only is the whole input <span class="math inline">\(X\)</span> “better preseved,” but also any
subobject of <span class="math inline">\(X\)</span>. Properties of less noisy comparison:</p>
<ol style="list-style-type: decimal">
<li>Tensorization: <span class="math inline">\(P_{Y|X}\preceq_{ln}P_{Z|X} \iff
\forall n: P_{Y|X}^{\otimes n}\preceq_{ln}P_{Z|X}^{\otimes n}\)</span>.</li>
<li><span class="math inline">\(\eta_{KL}(P_{Y|X})\leq 1-\tau \iff P_{Y|X} \preceq_{ln} EC_\tau\)</span>.</li>
</ol>
<div id="spiked-wigner" class="section level3 unnumbered hasAnchor">
<h3>Spiked Wigner<a href="lecture-notes.html#spiked-wigner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider an ambient <span class="math inline">\(W_{jk}=W_{kj} \sim \mathcal N(0, 1)\)</span>, and
<span class="math inline">\(X_j\sim \mathrm{Unif}\{\pm 1\}\)</span> for some “signal”, we observe
<span class="math display">\[
    Y = \sqrt{\dfrac\lambda n} XX^T + W
\]</span>
and estimator <span class="math inline">\(\hat X(Y)\)</span>,</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\lambda \leq 1\)</span>, then <span class="math inline">\(\forall X: \dfrac 1 n \mathbb E|\hat X^TX|\to 0\)</span>.
i.e. if the signal is small, then asymptotically we cannot even learn about planted signals.</li>
<li>If <span class="math inline">\(\lambda&gt;1\)</span>, then for <span class="math inline">\(\hat X\)</span> being the top eigenvalue of <span class="math inline">\(Y\)</span>, the correlation is bounded
away from <span class="math inline">\(0\)</span>: <span class="math inline">\(\dfrac 1 n \mathbb E|\hat X^TX|\geq \epsilon_0(\lambda)&gt;0\)</span>.
<ul>
<li>This follows from random matrix theory.</li>
</ul></li>
</ol>
<p>Recall inference on graphs: consider a graph with vertices <span class="math inline">\(X_j\sim \mathrm{Unif}\pm 1\)</span>.
For every edge <span class="math inline">\(e=(u, v)\)</span>, we have access to a channel <span class="math inline">\(P_{Y_e|X_e}\)</span>.
For the spiked wigner problem, we have a complete graph and
<span class="math display">\[
    Y_e = \sqrt{\dfrac\lambda n} X_uX_v + \mathcal N(0, 1)
\]</span>
Consider the channel <span class="math inline">\(P_{Z_l|X_l} = EC_\tau(X_uX_v)\)</span>.
How to compute <span class="math inline">\(I(X_1; X_2|Z_E)\)</span>?</p>
<ul>
<li>If there is an open path (i.e. without erasure) between <span class="math inline">\(X_1, X_2\)</span>,
then we know <span class="math inline">\(X_1X_2\)</span> with certainty.</li>
</ul>
<p>Unrolling the definition yields
<span class="math display">\[
    I(X_1; X_2|Z_E) = \mathrm{Pr}[\exists\text{open path in } ER(1-\tau, n)]
\]</span>
Theorem (Erdos-Renyi random graph model):
<span class="math inline">\(ER(\lambda/n, n)\)</span> for <span class="math inline">\(\lambda&lt;1\)</span> are of size <span class="math inline">\(O(\log n)\)</span>; in this regime,
the probability that <span class="math inline">\(X_1, X_2\)</span> are connected goes as <span class="math inline">\(\log n / n \to 0\)</span>.</p>
<p>For <span class="math inline">\(\lambda&gt;1\)</span>, there exists a giant connected component of size <span class="math inline">\(\Omega(n)\)</span>.</p>
<p>Back to spiked Wigner, we compute the <span class="math inline">\(\eta_{KL}\)</span> of
<span class="math inline">\((X_1, X_2)\mapsto X_1X_2\mapsto Y=\sqrt{\lambda/n} X_1X_2 = W\)</span>,
apply less noisy reduction. Retracing the steps:</p>
<ol style="list-style-type: decimal">
<li>Start with random matrix theory,
we recognized the reduction to graph inference.</li>
<li>Graph inference is easily bounded for the <span class="math inline">\(EC_\tau\)</span> channel using ER theory.</li>
<li>Reduction to the relatively easy <span class="math inline">\(EC_\tau\)</span> is done using SDPI theory.</li>
</ol>
</div>
<div id="correlation-estimation" class="section level3 unnumbered hasAnchor">
<h3>Correlation estimation<a href="lecture-notes.html#correlation-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given i.i.d. pairs <span class="math inline">\((X_j, Y_j)\)</span> drawn from <span class="math inline">\(P_{XY}\)</span> with uniform marginal on <span class="math inline">\(\pm 1\)</span>
and correlation <span class="math inline">\(\rho\)</span>; equivalently,
<span class="math inline">\(X\xrightarrow{\mathrm{BSC}_\delta} Y\)</span> with <span class="math inline">\(1-2\delta = \rho\)</span>.</p>
<p>Suppose we wish to estimate <span class="math inline">\(\rho\)</span> between two different partieis
holding <span class="math inline">\(X, Y\)</span> respectively, using <span class="math inline">\(B\)</span> bits. Claim:
<span class="math display">\[
    \mathbb E(\rho - \hat \rho)^2 = \dfrac 1 {2\log 2} \cdot \dfrac{1+o(1)}{B}
\]</span></p>
<ul>
<li>Contraction coefficient tensorizes (invariant) assuming independent inputs.</li>
<li>Removing this constraint, the SDPI coefficient always goes to <span class="math inline">\(\infty\)</span> due to the
existence of error-correcting codes.</li>
</ul>
<p>Yury’s great quote: popularity is not quality;
Macdonalds produce billions of dollars of gross scales,
but the Michelin restaurants are not upset about it.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing-large-deviations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliography.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
