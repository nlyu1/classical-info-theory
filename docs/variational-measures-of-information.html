<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Variational Measures of Information | 6.7480 Notes</title>
  <meta name="description" content="5 Variational Measures of Information | 6.7480 Notes" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Variational Measures of Information | 6.7480 Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Variational Measures of Information | 6.7480 Notes" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2024-10-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mutual-information.html"/>
<link rel="next" href="extremization.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#administrative-trivia"><i class="fa fa-check"></i>Administrative trivia</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#todo-list"><i class="fa fa-check"></i>Todo list</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="entropy.html"><a href="entropy.html"><i class="fa fa-check"></i><b>1</b> Entropy</a>
<ul>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="" data-path="entropy.html"><a href="entropy.html#combinatorial-properties"><i class="fa fa-check"></i>Combinatorial properties</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html"><i class="fa fa-check"></i><b>2</b> Entropy method in combinatorics and geometry</a>
<ul>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#binary-vectors-of-average-weights"><i class="fa fa-check"></i>Binary vectors of average weights</a></li>
<li class="chapter" data-level="" data-path="entropy-method-in-combinatorics-and-geometry.html"><a href="entropy-method-in-combinatorics-and-geometry.html#counting-subgraphs"><i class="fa fa-check"></i>Counting subgraphs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="divergence.html"><a href="divergence.html"><i class="fa fa-check"></i><b>3</b> Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#kl-divergence"><i class="fa fa-check"></i>KL-Divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#differential-entropy"><i class="fa fa-check"></i>Differential entropy</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#channel-conditional-divergence"><i class="fa fa-check"></i>Channel, conditional divergence</a></li>
<li class="chapter" data-level="" data-path="divergence.html"><a href="divergence.html#chain-rule-dpi"><i class="fa fa-check"></i>Chain Rule, DPI</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mutual-information.html"><a href="mutual-information.html"><i class="fa fa-check"></i><b>4</b> Mutual Information</a>
<ul>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#definition-properties"><i class="fa fa-check"></i>Definition, properties</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#causal-graphs"><i class="fa fa-check"></i>Causal graphs</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#conditional-mi"><i class="fa fa-check"></i>Conditional MI</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#sufficient-statistic"><i class="fa fa-check"></i>Sufficient statistic</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#probability-of-error-fanos-inequality"><i class="fa fa-check"></i>Probability of error, Fano’s inequality</a></li>
<li class="chapter" data-level="" data-path="mutual-information.html"><a href="mutual-information.html#entropy-power-inequality"><i class="fa fa-check"></i>Entropy-power Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html"><i class="fa fa-check"></i><b>5</b> Variational Measures of Information</a>
<ul>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#geometric-interpretations-of-mi"><i class="fa fa-check"></i>Geometric interpretations of MI</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#lower-variational-bounds"><i class="fa fa-check"></i>Lower variational bounds</a></li>
<li class="chapter" data-level="" data-path="variational-measures-of-information.html"><a href="variational-measures-of-information.html#continuity"><i class="fa fa-check"></i>Continuity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extremization.html"><a href="extremization.html"><i class="fa fa-check"></i><b>6</b> Extremization</a>
<ul>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#extremizationConvexity"><i class="fa fa-check"></i>Convexity</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#minimax-and-saddle-point"><i class="fa fa-check"></i>Minimax and saddle-point</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-saddle-point-of-mi"><i class="fa fa-check"></i>Capacity, Saddle point of MI</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#capacity-as-information-radius"><i class="fa fa-check"></i>Capacity as information radius</a></li>
<li class="chapter" data-level="" data-path="extremization.html"><a href="extremization.html#gaussian-saddle-point"><i class="fa fa-check"></i>Gaussian saddle point</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="tensorization.html"><a href="tensorization.html"><i class="fa fa-check"></i><b>7</b> Tensorization</a></li>
<li class="chapter" data-level="8" data-path="f-divergence.html"><a href="f-divergence.html"><i class="fa fa-check"></i><b>8</b> f-Divergence</a>
<ul>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#definition"><i class="fa fa-check"></i>Definition</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#information-properties-mi"><i class="fa fa-check"></i>Information properties, MI</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#tv-and-hellinger-hypothesis-testing"><i class="fa fa-check"></i>TV and Hellinger, hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#joint-range"><i class="fa fa-check"></i>Joint range</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#rényi-divergence"><i class="fa fa-check"></i>Rényi divergence</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#variational-characterizations"><i class="fa fa-check"></i>Variational characterizations</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#empirical-distribution-and-χ²"><i class="fa fa-check"></i>Empirical distribution and χ²</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#fisher-information-location-family"><i class="fa fa-check"></i>Fisher information, location family</a></li>
<li class="chapter" data-level="" data-path="f-divergence.html"><a href="f-divergence.html#local-χ²-behavior"><i class="fa fa-check"></i>Local χ² behavior</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data-compression.html"><a href="data-compression.html"><i class="fa fa-check"></i><b>9</b> Data compression</a>
<ul>
<li class="chapter" data-level="" data-path="data-compression.html"><a href="data-compression.html#source-coding-theorems"><i class="fa fa-check"></i>Source coding theorems</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>10</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-2-fisher-information-classical-minimax-estimation"><i class="fa fa-check"></i>Oct 2: Fisher information, classical minimax estimation</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#main-content"><i class="fa fa-check"></i>Main content</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#one-parameter-families-minimax-rates"><i class="fa fa-check"></i>One-parameter families; minimax rates</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#hcr-inequality-fisher-information"><i class="fa fa-check"></i>HCR inequality; Fisher information</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-7-data-compression"><i class="fa fa-check"></i>Oct 7: Data compression</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#oct-9-data-compression-ii"><i class="fa fa-check"></i>Oct 9: data compression II</a>
<ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#review"><i class="fa fa-check"></i>Review</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#arithmetic-encoder"><i class="fa fa-check"></i>Arithmetic encoder</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lempel-ziv"><i class="fa fa-check"></i>Lempel-Ziv</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">6.7480 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variational-measures-of-information" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Variational Measures of Information<a href="variational-measures-of-information.html#variational-measures-of-information" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Key takeaways:</p>
<ol style="list-style-type: decimal">
<li>Variational characterizations are important because
they provide convenient bounds when we choose an easily-computable candidate.</li>
<li>For each convex quantity there is a dual variational characterization per
the Legendre transformation.</li>
<li>To analyze mutual information with a mixture distribution: add a latent
variable as an upper bound, then use the Kolmogorov identities to
analyze the gap / decompose in another way.</li>
<li>Mutual information is “average divergence” achieved
using center of gravity <a href="variational-measures-of-information.html#cor:centerOfGravityMI">5.1</a>.</li>
</ol>
<div id="geometric-interpretations-of-mi" class="section level2 unnumbered hasAnchor">
<h2>Geometric interpretations of MI<a href="variational-measures-of-information.html#geometric-interpretations-of-mi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:goldenFormula" class="theorem"><strong>Theorem 5.1  (golden formula) </strong></span>For any <span class="math inline">\(Q_Y\)</span> we have
<span class="math display">\[
    D(P_{Y|X} \| Q_Y|P_X) = I(X; Y) + D(P_Y\|Q_Y)
\]</span>
In particular, if <span class="math inline">\(P_Y \ll Q_Y\)</span>, then
<span class="math display">\[
    I(X; Y) = D(P_{Y|X} \| Q_Y|P_X) - D(P_Y\|Q_Y)
\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
The two sides expand to
<span class="math display">\[\begin{align}
    D(P_{Y|X} \| Q_Y|P_X) &amp;= D(P_{XY} \| P_XQ_Y) \\
    I(X; Y) + D(P_Y \|Q_Y) &amp;= D(P_{XY} \| P_XP_Y) + D(P_Y \|Q_Y)
\end{align}\]</span>
and are equal by the divergence chain rule <a href="divergence.html#prp:divergenceChainRule">3.2</a>.
</details>
<div class="corollary">
<p><span id="cor:centerOfGravityMI" class="corollary"><strong>Corollary 5.1  (center of gravity formula) </strong></span><span class="math inline">\(I(X; Y) = \min_{Q_Y} D(P_{Y|X} \| Q_Y|P_X)\)</span>, the
unique minimizer is <span class="math inline">\(Q_Y=P_Y\)</span>.</p>
</div>
<p><em>Proof:</em> Follows from the golden formula and information inequality.</p>
<p>To properly interpret this corollary as the center of gravity:
consider the simplex of conditionals <span class="math inline">\(\{P_{Y|X=x}\}\)</span>. Given
a candidate marginal <span class="math inline">\(Q_Y\)</span>, we can compute its divergence
from each conditional, weighted by their incidence probability
<span class="math display">\[
    D(P_{Y|X} \| Q_Y|P_X)
    = \mathbb E_{x\sim P_X} \left[D(P_{Y|X=x} \| Q_Y)\right]
\]</span>
The unique minimizer of this is <span class="math inline">\(P_Y\)</span>, with minimized value <span class="math inline">\(I(X; Y)\)</span>.</p>
<p>One can also see that mutual information is small if masses are concentrated:
this coincides with <span class="math inline">\(I(X; Y) = H(Y) - H(Y|X)\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-34" class="theorem"><strong>Theorem 5.2  (minimum distance from independence) </strong></span><span class="math inline">\(I(X; Y) = \min_{Q_X, Q_Y} D(P_{XY} \|Q_XQ_Y)\)</span>.
The unique minimizer is <span class="math inline">\(Q_X, Q_Y = P_X, P_Y\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Decompose the divergence on the RHS
<span class="math display">\[\begin{align}
    D(P_{XY} \|Q_XQ_Y)
    &amp;= D(P_{XY} \| Q_X P_Y) + D(P_Y \|Q_Y) \\
    &amp;= D(P_{XY} \| P_X P_Y) + D(P_Y \|Q_Y) + D(P_X \| Q_X) \\
    &amp;= I(X; Y) + D(P_Y \|Q_Y) + D(P_X \| Q_X)
\end{align}\]</span>
</details>
<div class="theorem">
<p><span id="thm:unlabeled-div-35" class="theorem"><strong>Theorem 5.3  (minimum distance from markov chain) </strong></span>Minimizing over all <span class="math inline">\(Q_{XYZ} = Q_XQ_{Y|X}Q_{Z|Y}\)</span>,
<span class="math display">\[
    I(X; Z|Y) = \min_{Q_{XYZ}:X\to Y\to Z} D(P_{XYZ} \| Q_{XYZ})
\]</span></p>
</div>
</div>
<div id="lower-variational-bounds" class="section level2 unnumbered hasAnchor">
<h2>Lower variational bounds<a href="variational-measures-of-information.html#lower-variational-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-36" class="theorem"><strong>Theorem 5.4  (lower variational bound of MI) </strong></span>For any Markov kernel <span class="math inline">\(Q_{X|Y}\)</span> such that <span class="math inline">\(Q_{X|Y=y} \ll P_X\)</span> for
<span class="math inline">\(P_Y\)</span> almost everywhere we have
<span class="math display">\[
    I(X; Y) \geq \mathbb E_{P_{X, Y}} \left[\log \dfrac{d Q_{X|Y}}{dP_X}\right]
\]</span>
if <span class="math inline">\(I(X; Y) &lt; \infty\)</span>, then
<span class="math display">\[
    I(X; Y) = \sup_{Q_{X|Y}} \mathbb E_{P_{XY}} \left[
        \log \dfrac{dQ_{X|Y}}{dP_X}
    \right]
\]</span>
equality is saturated when <span class="math inline">\(Q_{X|Y} = P_{X|Y}\)</span>.</p>
</div>
<details>
<summary>
Proof
</summary>
Without dealing with limit arguments,
<span class="math display">\[\begin{align}
    \mathbb E_{P_{X|Y=y}} \left[
        \log \dfrac{Q_{X|Y}(x|y)}{P_X(x)}
    \right]
    &amp;= \mathbb E_{P_{X|Y=y}} \left[
        \log \left(
            \dfrac{Q_{X|Y}(x|y)}{P_{X|Y}(x|y)}
            \dfrac{P_{X|Y}(x|y)}{P_X(x)}
        \right)
    \right] \\
    &amp;= \mathbb E_{P_{X|Y=y}} \left[
        -D(P_{X|Y=y} \| Q_{X|Y=y}) + D(P_{X|Y=y} \| P_X)
    \right]
\end{align}\]</span>
Take expectation over <span class="math inline">\(y\)</span> to obtain
<span class="math display">\[\begin{align}
    \mathbb E_{P_{XY}} \left[
        \log \dfrac{dQ_{X|Y}}{dP_X}
    \right]
    &amp;= D(P_{X|Y=y} \| P_X | P_Y) - D(P_{X|Y} \| Q_{X|Y} | P_Y) \\
    &amp;= I(X; Y) - D(P_{X|Y} \| Q_{X|Y} | P_Y)
\end{align}\]</span>
</details>
<div class="theorem">
<p><span id="thm:donskerVaradhan" class="theorem"><strong>Theorem 5.5  (Donsker-Varadhan) </strong></span>Given probability measures <span class="math inline">\(P, Q\)</span> over <span class="math inline">\(\mathcal X\)</span>,
denote a class of functions
<span class="math display">\[\begin{align}
    \mathcal C_Q &amp;= \{
        f:\mathcal X\to \mathbb R\cup \{-\infty\} |
        0 &lt; \mathbb E_Q[e^{f(X)}] &lt; \infty
    \} \\
    D(P \| Q) &amp;= \sup_{f\in \mathcal C_Q}
        \mathbb E_P f(X) - \log \mathbb E_Q\left[e^{f(X)}\right]
\end{align}\]</span>
If <span class="math inline">\(\mathcal X\)</span> is a metric space with the
Borel <span class="math inline">\(\sigma\)</span>-algebra, then the supremum can be
taken over the class of all bounded continuous functions.</p>
</div>
<details>
<summary>
Proof
</summary>
Again, for clarity we’re ignoring measurability
(or infinity) subtleties.
The key idea is, given <span class="math inline">\(f\in \mathcal C_Q\)</span>, to
define a tilted distribution <span class="math inline">\(Q^f\)</span> such that
<span class="math display">\[
    Q^f(dx) = e^{f(dx) - \psi_f} Q(dx), \quad
    \psi_f = \log \mathbb E_Q\left(e^{f(X)}\right)
\]</span>
Here <span class="math inline">\(\psi_f\)</span> is just a <span class="math inline">\(f\)</span>-dependent normalization constant.
Then
<span class="math display">\[\begin{align}
    \mathbb E_P[f(X) - \psi_f]
    &amp;= \mathbb E_P[\log e^{f(X) - \psi_f}]
    = \mathbb E_P\left[\log \dfrac{dQ^f}{dQ}\right]\\
    &amp;= \mathbb E_P\left[\log \dfrac{dP}{dQ}- \log \dfrac{dP}{dQ^f}\right]
    = D(P\|Q) - D(P\|Q^f) \leq D(P\|Q)
\end{align}\]</span>
Equality is saturated when <span class="math inline">\(Q^f = P \iff f = \log \dfrac{dP}{dQ}\)</span>.
</details>
<div class="corollary">
<p><span id="cor:unlabeled-div-37" class="corollary"><strong>Corollary 5.2  </strong></span>The supremum in Donsker-Varadhan is equivalent to,
<span class="math inline">\(\forall f\in \mathcal C_Q\)</span>
<span class="math display">\[
    \mathbb E_P f(X) \leq D(P\|Q) + \psi_f,
    \quad \psi_f = \log \mathbb E_Q\left(e^{f(X)}\right)
\]</span>
This allows us to upper-bound <span class="math inline">\(\mathbb E_P f(X)\)</span> for difficult <span class="math inline">\(P\)</span>
by substituting with an easier <span class="math inline">\(Q\)</span>.</p>
</div>
<p>Recall the Legendre transform: given a convex function <span class="math inline">\(f(x)\)</span>,
its Legendre transform is
<span class="math display">\[
    g(p) = \sup_x\left[\langle x, p\rangle- f(x)\right]
\]</span>
Here, <span class="math inline">\(g(p)\)</span> is the function <span class="math inline">\(P\mapsto D(P\|Q)\)</span>, <span class="math inline">\(x\)</span>
is a well-behaved function <span class="math inline">\(f\)</span>, and <span class="math inline">\(f(x)\)</span> corresponds
to the functional <span class="math inline">\(f\mapsto \psi_f\)</span>. This is a functional
Legendre transform. The following result will then not be too surprising.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-38" class="proposition"><strong>Proposition 5.1  (Gibbs variational principle) </strong></span>Given measurable <span class="math inline">\(\mathcal X:\mathbb R\cup \{-\infty\}\)</span>
and a probability measure <span class="math inline">\(Q\)</span> on <span class="math inline">\(\mathcal X\)</span>
<span class="math display">\[
    \log \mathbb E_Q\left[e^{f(X)}\right] = \sup_P \mathbb E_P [f(X)] - D(P\|Q)
\]</span>
If the LHS is finite, then the unique maximizer in the RHS is <span class="math inline">\(P=Q^f\)</span>.</p>
</div>
</div>
<div id="continuity" class="section level2 unnumbered hasAnchor">
<h2>Continuity<a href="variational-measures-of-information.html#continuity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="proposition">
<p><span id="prp:unlabeled-div-39" class="proposition"><strong>Proposition 5.2  (Single-argument continuity of divergence for finite alphabets) </strong></span>Fixing finite <span class="math inline">\(\mathcal X\)</span> with a strictly positive distribution <span class="math inline">\(Q\)</span> over <span class="math inline">\(\mathcal X\)</span>,
then <span class="math inline">\(P\mapsto D(P\|Q)\)</span> is continuous; in particular, <span class="math inline">\(P\mapsto H(P)\)</span> is continuous.</p>
</div>
<p><span style="color:green">
For finite alphabets, divergence is never continuous in the pair: consider
<span class="math display">\[
    \lim_{n\to \infty} d(1/n \| 2^{-n})
    = \dfrac 1 n \log \left(\dfrac{2^n} n\right) + \dfrac{n-1} n \log \left(\dfrac{1-1/n}{1-2^{-n}}\right)
    = \log 2 \neq 0
\]</span>
while the arguments converge to <span class="math inline">\(0\)</span>: <em>information can be destroyed at the limit.</em>
</span></p>
<div style="color:green">
<div class="remark">
<p><span id="unlabeled-div-40" class="remark"><em>Remark</em>. </span><span class="math inline">\(D(P\|Q)\)</span> is not continuous in either <span class="math inline">\(P\)</span> or <span class="math inline">\(Q\)</span> for general alphabets:
consider <span class="math inline">\(X_j\sim 2\mathrm{Ber}_{1/2}-1\)</span>, then <span class="math inline">\(\bar X_j\to \mathcal N(0, 1)\)</span>
but <span class="math inline">\(D(\bar X_j \|\mathcal N(0, 1))=\infty\)</span> because <span class="math inline">\(\bar X_j\)</span> is discrete for all <span class="math inline">\(n\)</span>.</p>
</div>
</div>
<div class="theorem">
<p><span id="thm:semiContinuity" class="theorem"><strong>Theorem 5.6  (Lower semicontinuity of divergence) </strong></span>Given a metric space <span class="math inline">\(\mathcal X\)</span> with Borel <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\(\mathcal H\)</span>.
If <span class="math inline">\(P_n, Q_n\)</span> converge weakly to <span class="math inline">\(P, Q\)</span>, respectively, then
<span class="math display">\[
    D(P\|Q) \leq \liminf_{n\to \infty} D(P_n\|Q_n)
\]</span>
Recall that weak convergence is pointwise convergence in distribution.</p>
</div>
<details>
<summary>
Proof
</summary>
This follows from Donsker-Varadhan by <span class="math inline">\(\mathbb E_{P_n}[f]\to \mathbb E_P[f]\)</span> and
<span class="math inline">\(\mathbb E_{Q_n}[e^f]\to \mathbb E_Q[e^f]\)</span>. To reason about <span class="math inline">\(\liminf\)</span>: choose a
subsequence such that <span class="math inline">\(D(P_n\|Q_n)\)</span> decreases monotonically,
then <span class="math inline">\(\liminf\mapsto \lim\)</span>; equality cannot be established by the previous example:
a more fundamental reason is because weak convergence guarantees
the convergence of expectations for each individual function but not
convergence for the supremum.
</details>
<p><span style="color:green">
Convergence in the weaker metric topology only implies weak
semi-continuity in the stronger Jensen metric.
</span></p>
<div class="proposition">
<p><span id="prp:unlabeled-div-41" class="proposition"><strong>Proposition 5.3  (continuity of MI) </strong></span></p>
<ol style="list-style-type: lower-alpha">
<li>Given finite alphabets <span class="math inline">\(\mathcal X, \mathcal Y\)</span>, <span class="math inline">\(P_{XY}\mapsto I(X; Y)\)</span> is continuous.</li>
<li>If <span class="math inline">\(\mathcal X\)</span> is finite, then <span class="math inline">\(P_X\mapsto I(X; Y)\)</span> is continuous.</li>
<li>For general <span class="math inline">\(\mathcal X, \mathcal Y\)</span>, let <span class="math inline">\(P_X\in \Pi = \mathrm{co}(P_1, \cdots, P_n)\)</span>
be in the convex hull of <span class="math inline">\((P_j\)</span>; if <span class="math inline">\(I(P_j; P_{Y|X}&lt;\infty\)</span> for each
<span class="math inline">\(P_j\)</span>, then the map <span class="math inline">\(P_X\mapsto I(X; Y)\)</span> is continuous.</li>
</ol>
</div>
<details>
<summary>
Proof
</summary>
For the first statement, <span class="math inline">\(I(X; Y)=H(X)+H(Y) - H(X, Y)\)</span> and entropy is
continuous on finite alphabets. For the second statement, define the
uniform mixture of conditionals <span class="math inline">\(Q_Y = |\mathcal X|^{-1}\sum P_{Y|X=x}\)</span>, then
<span class="math display">\[
    D(P_Y \|Q_Y) = \mathbb E_{Q_Y} \left[\varphi \left(\sum P_X(x)h_x(Y)\right)\right],
    \quad \varphi(t) = t\log t
\]</span>
Here <span class="math inline">\(h_x(y) = \dfrac{dP_{Y|X=x}}{dQ_Y}(y)\)</span> is nonnegative and bounded by <span class="math inline">\(|\mathcal X|\)</span>,
so using the bounded convergence theorem we have $P_X<span class="math inline">\(D(P_Y\|Q_Y)\)</span>
continuous. By the golden formula,
<span class="math display">\[
    I(X; Y) = D(P_{Y|X} \|Q_Y|P_X) - D(P_Y\|Q_Y)
\]</span>
the first term is linear in <span class="math inline">\(P_X\)</span>. For the third claim, form
a chain <span class="math inline">\(Z\to X\to Y\)</span> with <span class="math inline">\(Z\in [n]\)</span> mapping <span class="math inline">\(P_{X|Z=j}=P_j\)</span>. Then
<span class="math display">\[
    I(X; Y) = I(Z; Y) + I(X; Y|Z)
\]</span>
the first term is continuous in <span class="math inline">\(P_Z\)</span> while the second is linear in <span class="math inline">\(P_Z\)</span>.
Thus <span class="math inline">\(P_Z\mapsto I(X; Y)\)</span> is continuous, and so is <span class="math inline">\(P_X\mapsto I(X; Y)\)</span>.
</details>
<p><span style="color:green">
The bounded convergence theorem applies in proof of (b) because
we have pointwise convergence of <span class="math inline">\(\sum_{x} P_X(x)h_x(y)\)</span> as <span class="math inline">\(P_X\)</span>
approaches its limit, and <span class="math inline">\(\varphi\)</span> is continuous and bounded, so
this allows us to pass the limit through composition with <span class="math inline">\(\varphi\)</span>.
</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mutual-information.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="extremization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
