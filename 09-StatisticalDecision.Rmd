# Statistical Decision Applications 

Key takeaways:

1. A statistical model is a family $\{P_\theta\}$ of distributions indexed 
    by parameter values $\theta \in \Theta$. 
2. $R_\theta(\hat \theta)$ is the expectation of loss over $X\sim P_\theta$. 
3. There are two ways turn $R_\theta(\hat\theta)$ into a scalar: 
    - Eliminating $\theta$ with $\sup_\theta$, then taking $\inf$ 
    over all estimators leads to the minimax risk $R^*$. 
    - Replacing $\theta$ by averaging over a prior $\pi$, 
    taking $\inf$ over $\hat \theta$, then taking $\sup_\pi$ adversarially 
    over all priors $\pi$ leads to the Bayes risk. 
4. The minimax and Bayes risks are related by [duality](minimaxDual). 
5. Under mild assumptions, $R^* = R^*_{\mrm{Bayes}}$ 
    (proposition \@ref(prp:minimaxSeparation)). 
6. Sample complexity \@ref(def:sampleComplexity) is the number of samples 
    to achieve $R^*_n< \epsilon$. 
7. Tensor product experiments \@ref(def:tpExperiment) 
    have independent but not necessarily identically distributed observations; 
    theoreom \@ref(thm:tpMinimax) yields a bound on its minimax risk. 
8. Given i.i.d. observations, 
    an analogue of Cramer-Rao (theorem \@ref(thm:minimaxLB)) still holds 
    even for biased estimators.  


## Minimax and Bayes risks {-}

- A **statistical model** is a collection 
    $\mca P = \{P_\theta:\theta \in \Theta\}$ of distributions 
    over some measurable space $(\mca X, \Sigma)$. 
    Here $\Theta$ is the **parameter space**. 
- An **estimand** $T(\theta)$ has signature $T:\Theta\to \mca Y$; 
    in the trivial case $T$ is just the identity. 
- An **estimator** (decision rule) has type $\hat T:\Omega\to \hat {\mca Y}$. 
    The action space $\hat {\mca Y}$ does not have to be 
    the estimand space $\mca Y$ (e.g. when we're estimating a confidence interval). 
    - $\hat T$ can be deterministic or randomized. 
- To evaluate the estimator, the **loss function** $l:\mca Y\times \hat{\mca Y}\to \R$ 
    defines the **risk** of $\hat T$ for estimating $T$. 
- The (expected) risk of the estimator $\hat T$ for estimating $T$ is 
    <span style="color:green">a function of the true parameter $\theta$ 
    and estimator $\hat T$ </span>: 
\[ 
    R_\theta(\hat T) 
    = \Exp_{X\sim \theta}[l(T(\theta), \hat T(X)] 
    = \int P_\theta(dx) P_{\hat T|X}(\hat t|x) l(T(\theta), \hat t)
\] 

Two comments in order: 

- For the minimax risk, it is sometimes necessary to 
randomize. To minimize the average risk over a prior (Bayesian approach), 
though, it suffices to consider deterministic functions. 
- The space of randomized estimators (as Markov kernels) is convex. 
This is necessary for minimax theorems. 
- For $\hat T\mapsto l(T, \hat T)$ convex, we can derandomize 
    it by consider $\Exp[\hat T|X]$, then 
    \[ 
        R_\theta(\hat T) = \Exp_\theta l(T, \hat T) \geq 
        \Exp_\theta l(T, \Exp[\hat T|X])
    \] 

:::{.definition name="Bayes risk"}
Given a prior $\pi$, the average risk w.r.t. $\pi$ 
of an estimator is 
\[ 
    R_\pi(\hat \theta) 
    = \Exp_{\theta \sim \pi} R_\theta(\hat \theta) 
    = \Exp_{\theta \sim \pi, X\sim P_\theta} l(\theta, \hat \theta(X))
\] 
The **Bayes risk** of a prior is its minimal average risk 
$R_\pi^* = \inf_{\hat \theta} R_\pi(\hat \theta)$. 
The optimal $\hat \theta$ is the Bayes estimator. 
:::

:::{.definition name="minimax risk"}
Given a parameter family $P_{\theta\in \Theta}$ and 
loss function $l$, the minimax risk is  
\[ 
    R^* = \inf_{\hat \theta} \sup_{\theta \in \Theta} 
    R_\theta(\hat \theta) 
    = \inf_{\hat \theta} \sup_{\theta \in \Theta} 
    \Exp_{X\sim P_\theta} l(\hat \theta(X), \theta)
\] 
To prove minimax risk, one needs to establish for arbitrary $\epsilon$ 

1. an estimator $\hat \theta^*$ satisfying 
    $R_\theta(\hat \theta^*)\leq R^*+\epsilon$. 
2. For arbitrary $\hat \theta$, 
    $\sup_{\theta} R_\theta(\hat \theta)\geq R^*-\epsilon$. 
:::


:::{.theorem name="duality between minimax and Bayes risk"}
Let $\mca P(\Theta)$ denote the set of probability distributions 
on $\Theta$, then 
\[ 
    R^* = \inf_{\hat \theta}\sup_{\theta \in \Theta} R_\theta(\hat \theta)
    \geq R^*_{\mrm{Bayes}} 
    = \sup_{\pi \in \mca P(\theta)} \inf_{\hat \theta} 
    R_\pi(\hat \theta)
\] 
:::
_Proof:_ $R^* = \inf_{\hat \theta}\sup_{\theta \in \Theta} R_\theta(\hat \theta)
= \inf_{\hat \theta}\sup_{\pi \in \mca P(\Theta)} R_\pi(\hat \theta)
\geq R^*_{\mrm{Bayes}}$ by $\inf\sup \geq \sup\inf$. 

:::{.example name="strict inequality"}
Let $\theta, \hat \theta \in \{1, 2, \cdots\}$ and 
$l(\theta, \hat \theta) = 1_{\hat \theta < \theta}$, then 
$R^* = 1$ while $R^*_{\mrm{Bayes}} = 0$. 
:::

## A duality perspective {#minimaxDual -}

For simplicity, let $\Theta$ be a finite set and $l$ be convex, then 
\[ 
    R^* = \min_{P_{\hat \theta|X}} \max_{\theta \in \Theta} 
    \Exp_\theta \, l(\theta, \hat \theta)
\] 
This is a convex optimization problem since 
\[ 
    P_{\hat \theta|X}\mapsto \Exp_\theta \, l(\theta, \hat \theta)
    = \Exp_{X\sim P_\theta, \hat \theta\sim 
    P_{\hat \theta|X}}[l(\theta, \hat \theta)]
\] 
is linear and the pointwise supremum of convex functions is convex. 
Considering its dual, rewrite 
\[ 
    R^* = \min_{P_{\hat \theta|X}, t} \, t\quad \text{s.t. } 
    \Exp_\theta\, l(\theta, \hat \theta) \leq t, \quad \forall \theta \in \Theta 
\] 
Let $\pi_\theta\geq 0$ for each inequality constraint. 
The Lagrangian is 
\[ 
    \mca L(P_{\hat \theta|X}, t, \pi) 
    = t + \sum_{\theta \in \Theta} \pi_\theta\cdot \left(
        \Exp_\theta\, l(\theta, \hat \theta) - t 
    \right) = \left(
        1 - \sum_{\theta \in \Theta} \pi_\theta 
    \right)t + \sum_{\theta \in \Theta} \pi_\theta \Exp_\theta\, l(\theta, \hat \theta)
\] 
The first term implies that $\pi$ must be a probability measure, 
yielding the dual problem 
\[ 
    \max_\pi \min_{P_{\hat \theta|X}} 
    \sum_{\theta \in \Theta} \pi_\theta \Exp_\theta\, l(\theta, \hat \theta)
    = \max_{\pi \in \mca P(\Theta)} R_\pi^*
\] 

:::{.theorem name="general minimax equality"}
$R^* = R^*_{\mrm{Bayes}}$ if the following conditions hold: 

1. The experiment is dominated: $P_{\forall \theta} \ll \nu$ for some $\nu$. 
2. The action space $\hat \Theta$ (codomain of the estimator) 
    is a locally compact topological space with a countable basis. 
3. The loss function is level compact: for each $\theta\in \Theta, l(\theta, \cdot)$
    is bounded from below and $\{\hat \theta:l(\theta, \hat \theta)\leq a\}$ 
    is compact for each $a$. 
:::

We will prove the following special case for demonstration. 

:::{.proposition #minimaxSeparation name="special minimax equality"}
$R^* = R^*_{\mrm{Bayes}}$ if 
$\Theta$ is finite and $l$ is bounded from below (e.g. quadratic). 
:::
<span style="color:green">
Proof ideas: work in the vector space of risk vectors 
with inner product given by average risk. Next 
separate the convex sets of (1) all risk vectors, and (2) 
all vectors component-wise less than $R^*$.  
</span>
<details>
<summary>Proof</summary>
- First consider the edge case $R^*=\infty \iff R^*_{\mrm{Bayes}} = \infty$; 
this is established by considering the uniform prior $\pi$ on $\Theta$. 
- Next consider $R^*<\infty$. Given an estimator $\hat \theta$, denote its 
risk vector $R(\hat \theta)_\theta = \Exp_{X\sim P_\theta} l(\theta, \hat \theta)$ 
with components in $\theta$. 
- The average risk is given by the inner product $\la R(\hat \theta), \pi\ra$. 
- Define the set $S$ of all possible risk vectors for randomized estimators; 
Note that $S$ is convex because linear interpolations of risk vectors 
are given by mixtures of their corresponding random estimators. 
- The set $T=\{t\in R^\Theta: t_{\forall \theta} < R^*\}$ 
is convex, and $S\cap T=\emptyset$ since for every valid estimator at 
least one component achieves $R^*$. 
- By the hyperplane separation theorem, there exists $\pi \in R^\Theta$ 
    and $c\in \R$ such that \inf_{s\in S}\la \pi, s\ra \geq c\geq \sup_{t\in T}\la \pi, t\ra$. 
    Now $\pi$ must be componentwise positive else $\sup_{t\in T}\la \pi, t\ra = \infty$, 
    so w.l.o.g. $\pi$ is a probability vector. 
- Thus we have established $R^*_{\mrm{Bayes}} \geq R_\pi^* \geq R^*$. 
</details>

## Sample complexity, tensor products {-}

Given an experiment $P_{\theta\in \Theta}$, the _independent sampling model_ 
refers to the experiment 
\[ 
    \mca P_n = \{P_{\theta \in \Theta}^{\otimes n}\}, \quad n\geq 1 
\] 
Define $R_n^*(\Theta) = \inf_\hat \theta \sup_{\theta \in \Theta} \Exp_\theta\, l(\theta, \hat \theta)$
to be the minimax risk when $\hat \theta$ consumes 
$X=(X_1, \cdots, X_n)$ consisting of independent observations. Note that: 

- $n\mapsto R_n^*(\Theta)$ is non-increasing since we can always discard extra observations. 
- We typically expect $R_n^*(\Theta)\to 0$ as $n\to \infty$. 

:::{.definition #sampleComplexity name="sample complexity"}
The sample complexity is the minimum sample size required to obtain a prescribed 
error $\epsilon$ in the worst case (of actual parameter): 
\[ 
    n^*(\epsilon) = \min\{n\in \mbb N: R^*_n(\Theta)\leq \epsilon\} 
\] 
:::

:::{.definition #tpExperiment name="tensor product experiment"}
Given statistical experiments $\mca P_i = \{P_{\theta_i \in \Theta_i}\}$ 
and loss $l_i$ for each $i\in [d]$, the tensor product experiment 
\[ 
    \mca P = \left\{\prod P_{\theta_j}, \quad 
    (\theta_j)\in \Theta = \prod \Theta_j\right\}
\] 
the loss function is $l(\theta, \hat \theta) = \sum_j l_j(\theta_j, \hat \theta_j)$. 
In this model, the observation $(X_1, \cdots, X_d)$ are independent but not 
necessarily identically distributed. 
:::

:::{.theorem #tpMinimax name="minimax risk of tensor product"}
\[ 
    \sum_{j=1}^d R^*_{\mrm{Bayes}}(\mca P_j) \leq R^*(\mca P) 
    \leq \sum_{j=1}^d R^*(\mca P_j)
\] 
If the minimax theorem $R^*(\mca P_i) = R^*_{\mrm{Bayes}}(\mca P_i)$, 
then it holds for the product experiment and the minimax risk additively 
decomposes. 
:::
<span style="color:green">
Proof idea: right inequality follows by unrolling definitions. 
For the left inequality, choose $\theta_j$'s independent from a 
product prior and argue that each component of the estimator only 
obtain information from $X_j$ alone. 
</span>
<details>
<summary>Proof</summary>
For the right inequality, choose $\hat \theta^* = (\hat \theta_j^*)$ 
to be the component minimax estimators without considering any other observations. 
For the left inequality, consider for each $(\pi_j)$ a product prior 
$\pi = \prod \pi_j$; under this prior, _both $\theta_j$'s and $X_j$'s are independent._
For any component $\hat \theta_j = \hat \theta_j(X, U_j)$ of any randomized 
estimator $\hat \theta$, the non-$X_j$ components $(U_j, X_{\bar j})$ 
by independence can be viewed as a randomized estimator based on $X_i$ alone 
(all other $X_{\bar j}$'s do not yield any information about $\theta_j$ by independence 
of $\theta$'s). Thus its average risk satisfies 
$R_{\pi_j}(\hat \theta_j) \geq R^*_{\pi_j}$. 
Take supremum over all $\pi_j$ and sum over $j$ to obtain the left inequality. 
</details>


## HCR lower bound {-}

:::{.theorem #hcrBound name="HCR lower bound"}
The quadratic loss of any estimator $\hat \theta$ at 
$\theta\in \Theta\subset \R^d$ satisfies 
\[ 
    R_\theta(\hat \theta) = \Exp_{
        \theta, X\sim P_\theta 
    } [\hat \theta(X) - \theta]^2 
    \geq \Var_\theta(\hat \theta) \geq 
    \sup_{\theta'\neq \theta} \df{
        (\Exp_\theta \hat \theta - \Exp_{\theta'} \hat \theta)^2
    }{\chi^2(P_{\theta'} \| P_\theta)}
\] 
:::
<span style="color:green">
_Proof idea_: fix $\theta' \neq \theta$ 
and treat $X\mapsto \hat \theta(X)$ as a data-processor; 
apply the variational characterization of $\chi^2$. 
</span>
<details>
<summary>Proof</summary>
Recall the variational characterization of $\chi^2$ 
(proposition \@ref(prp:chiVariation)): for all $g:\mca X\to \R$ 
\[ 
    \Var_Q[g^2] \geq \df{(\Exp_P g - \Exp_Q g)^2}{\chi^2(P\|Q)}
\] 
Given two different underlying parameters $\theta, \theta'$, 
let $P_X=P_\theta, Q_X=P_{\theta'}$, then applying the 
data processor $X\mapsto \hat \theta(X)$ 
and the variational characterization 
\[ 
    \chi^2(P_X \| Q_X) \geq \chi^2(P_{\hat \theta} \| Q_{\hat \theta}) 
    \geq \df{(\Exp_\theta\hat \theta - \Exp_{\theta'} \hat \theta)^2}
    {\Var_\theta(\hat \theta)}
\]
</details>


:::{.corollary #cramerRao name="Cramer-Rau lower-bound"}
For an unbiased estimator $\hat \theta$ satisfying 
$\Exp_{\forall \theta}[\hat \theta] = \theta$, we obtain 
\[ 
    \Var_\theta(\hat \theta) \geq \mca J_F(\theta)^{-1}
\]  
:::
<details>
<summary>Proof</summary>
Apply the unbiased condition to theorem \@ref(thm:hcrBound), 
take $\theta = \theta + \xi_{\to 0}$ and apply the local Fisher 
behavior of divergence theorem \@ref(thm:localFisherDiv) 
\begin{align}
    \Var_\theta(\hat \theta) 
    &\geq \df{
        \xi^2
    }{\chi^2(P_{\theta + \xi} \| P_\theta)}
    = \lim_{\xi\to 0} \xi\df{
        \xi^2 
    }{\mca J_F(0) \xi^2 + \cdots} = \mca J_F(\theta)^{-1}
\end{align}
</details>

To generalize to $\theta \in \Theta\subset \R^d$, assume 
$\hat \theta$ _is unbiased and 
\[ 
    \Cov_\theta(\hat \theta)
    = \Exp_\theta[(\hat \theta - \theta)(\hat \theta - \theta)^T]
\] 
is positive-semidefinite, 
apply theorem \@ref(thm:hcrBound) 
to each data-processor $\la a, \hat \theta(X)\ra, 
a\in \R^d$ to obtain 
\[ 
    \chi^2(P_\theta \| P_{\theta'}) \geq 
    \df{\la a, \theta - \theta'\ra^2}{a^T \Cov_\theta(\hat \theta)a}
\] 
Optimizing over $a$ and apply, for $0\preceq \Sigma$ 
\[ 
    \sup_{x\neq 0} \df{\la x, y\ra^2}{x^T\Sigma x} = y^T \Sigma^{-1} y, \quad 
    x = \Sigma^{-1} y 
\] 
yields $\chi^2(P_\theta \| P_{\theta'}) \geq 
(\theta - \theta')^T \Cov_\theta(\hat \theta)^{-1}
(\theta - \theta')$. Applying the multivariate local 
approximation again yields $\mca J_F^{-1}(\theta) \preceq \Cov_\theta(\hat \theta)$. 

Further recall that the Fisher information is additive 
under i.i.d. tensorization; taking the trace on both 
sides, we obtain, for unbiased estimators, 
\[ 
    \Exp_\theta \|\hat \theta - \theta\|_2^2 
    \geq \df 1 2 \tr \mca J_F^{-1}(\theta)
\] 

## Bayesian perspective {-}

In minimax settings, it is often wise to 
trade bias with variance to achieve a smaller 
overall risk. Fixing a prior $\pi \in \mca P(\Theta)$,  
consider the following joint distributions for 
$\theta, X)$: 

- Under $Q$, $\theta\sim \pi$ and $X\sim P_\theta$ after 
    sampling $\theta$. 
- Under $P, \theta\sim T_\delta\pi$ where $T_\delta\pi$ 
    is the displaced prior $T_\delta \pi(A) = \pi(A - \delta)$, and 
    $X\sim P_{\theta - \delta}$ conditioning on $\theta$. 

:::{.theorem #vanTrees name="van Trees inequality"}
Fixing a differentiable prior $\pi$ 
and using the setup for $P_{X\theta}, Q_{X\theta}$ 
above, under regularity conditions 
\[ 
    R^* \geq R^*_\pi = \inf_{\hat \theta} \Exp_\pi[(\hat \theta - \theta)^2]
    \geq \df 1 {J(\pi) + \Exp_{\theta\sim \pi} \mca J_F(\theta)}
\] 
Note that $\Exp_\pi[(\hat \theta - \theta)^2]$ is 
in fact expectation over $\theta\sim \pi$, then $X\sim P_\theta$, 
and $J(\pi)$ is the Fisher information 
of the location family $\pi$. The regularity conditions are: 

1. $\pi$ is differentiable, supported on $[\theta_0, \theta_1]$ and 
    $\pi(\theta_0) = \pi(\theta_1) = 0$. 
2. The location-family Fisher information of the prior is finite 
\[ 
    J(\pi) = \int_{\theta_0}^{\theta_1} \df{\pi'(\theta)^2}{\pi(\theta)}\, d\theta < \infty
\] 
3. The family has a dominating measure: $P_\theta = P_\theta\mu$, and $p_\theta(x)$ 
    is differentiable in $\theta$ a.e. 
4. For $\pi$-almost every $\theta$: $\int \pd \theta p_\theta(x)\, d\mu = 0$.  
:::
<details>
<summary>Proof by data-processing</summary>
Fix any (possibly randomized) estimator $\hat \theta$, 
the marginals (but not the joint!) 
$P_X = Q_X$ so $\Exp_P\hat \theta = \Exp_Q\hat \theta$ 
while $\Exp_P \theta = \Exp_Q \theta + \delta$. Then 
\begin{align}
    \chi^2(P_{\theta X} \| Q_{\theta X}) 
    &\geq \chi^2(P_{\theta \hat \theta} \| Q_{\theta\hat \theta}) 
    \geq \chi^2(P_{\theta - \hat \theta} \| Q_{\theta - \hat \theta}) 
     \\ 
     &\geq \df{(\Exp_P[\theta - \hat \theta] - 
    \Exp_Q[\theta - \hat \theta])^2}{\Var_Q(\hat \theta - \theta)}
    = \df{\delta^2}{\Var_Q(\hat \theta - \theta)} \geq 
    \df{\delta^2}{\Exp_Q[\hat \theta - \theta]^2}
\end{align}
To obtain the second inequality, first consider 
the local expansions: 

- $\chi^2(P_\theta \| Q_\theta) = \chi^2(T_\delta \pi \| \pi) 
    \approx J(\pi) \delta^2$. 
- $\chi^2(P_{X|\theta} \| Q_{X|\theta}) \approx \mca J_F(\theta) \delta^2$. 
Next, apply the $\chi^2$ chain rule (proposition \@ref(prp:chiSqChain))
to obtain 
\begin{align}
    \chi^2(P_{X\theta} \| Q_{X\theta}) 
    &= \chi^2(P_\theta \| Q_{\theta}) + \Exp_Q \left[
        \chi^2(P_{X|\theta} \| Q_{X|\theta}) 
        \left(\df{dP_\theta}{dQ_\theta}\right)^2
    \right] \\ 
    R_\pi^* 
    &\geq \df{\delta^2}{J(\pi) \delta^2 + 
    \Exp_Q \left[\mca J_F(\theta) \delta^2 (dP_\theta/dQ_\theta)^2\right]}\\ 
    &\geq \df 1 {J(\pi) + \Exp_{\theta \sim \pi} \mca J_F(\theta)}
\end{align}
In the last step, we can also take $dP_\theta/dQ_\theta\to 1$ 
by continuity of $\pi$. 
</details>

<details>
<summary>Direct proof</summary>
For the Bayes setting, w.l.o.g. assume that the estimator is deterministic. 
For each $x$, integrate by parts to obtain 
\[ 
    \int_{\theta_0}^{\theta_1} (\hat \theta - \theta) 
    \pd \theta[p_\theta \pi(\theta)] 
    = \int_{\theta_0}^{\theta_1} p_\theta \pi(\theta)\, d\theta 
\] 
Next integrate both sides over $\mu(dx)$ to obtain 
\[ 
    \Exp_{\theta, X}[(\hat \theta - \theta)V(\theta, X)] = 1, \quad 
    V(\theta, x) = \pd \theta \log[p_\theta(x) \pi(\theta)]
\] 
Next apply Cauchy-Schwarz to obtain 
$\Exp[(\hat \theta - \theta)^2]\Exp[V(\theta, X)^2]\geq 1$ and 
\[ 
    \Exp[V(\theta, X)^2] 
    = \Exp[V(\theta, X)^2] = \Exp[(\pd \theta \log P_\theta(X))^2] 
    + \Exp[(\pd \theta \log \pi(\theta))^2] = \Exp_\theta \mca J_F(\theta) 
    + J(\pi)
\] 
</details>

- We assume a prior density that vanishes at the boundary. 
    A uniform prior yields the Chernoff-Rubin-Stein inequality. 
- To obtain the tightest lower bound, one shouold use regular prior 
    with minimum Fisher information, which is known to be 
    $g(u) = \cos^2(\pi u/2)$ supported on $[-1, 1]$ with Fisher information $\pi^2$. 

We only state the multivariate version of the theorem above. 

:::{.theorem name="multivariate BCR"}
Given a product prior density $\pi(\theta) = \prod_{j=1}^d \pi_j(\theta_j)$ 
such that each $\pi_j$ is compactly supported and vanishes 
on the boundary; suppose that for $\pi$-almost every $\theta$ 
\[ 
    \int \mu(dx) \nabla_\theta p_\theta(x) = 0
\] 
Then, for $J(\pi) = \mrm{diag}(\{J(\pi_j)\})$ we have (first inverse 
then trace) 
\[ 
    R_\pi^* = \inf_{\hat \theta} \Exp_\pi \|\theta - \hat \theta\|_2^2 
    \geq \tr \left(
        \Exp_{\theta \sim \pi} \mca J_F(\theta) + J(\pi)
    \right)^{-1}
\] 
:::

:::{.theorem #minimaxLB name="lower bound on asymptotic minimax risk"}
Assuming $\theta \mapsto \mca J_F(\theta)$ continuous; 
let $X_1, \cdots X_n\sim P_\theta$ i.i.d. and define 
the minimax risk $R_n^* = \inf_{\hat \theta} \sup_{\theta \in \Theta} 
\Exp_\theta \|\hat \theta - \theta\|_2^2$, then 
\[ 
    R_n^* \geq \df{1+o(1)}{n} \sup_{\theta \in \Theta} \tr \mca J_F^{-1}(\theta)
\] 
:::
<details>
<summary>Proof</summary>
Fix $\theta\in \Theta$ and choose a small $\delta$ 
such that $\theta + [-\delta, \delta]^d \subset \Theta$ 
and let $\pi_j(\theta_j)$ be the compact minimal-Fisher information 
cosine prior. Then 
\[ 
    J(\pi_j) = \df 1 {\delta^2} J(g) = \df{\pi^2}{\delta^2}\implies 
    J(\pi) = \df{\pi^2}{\delta^2}I_d 
\] 
Next, continuity $\theta \mapsto \mca J_F(\theta)$ guarantees 
the hypothesis of the HCR bound, then applying 
the additivity of Fisher information, we obtain 
\[ 
    R_n^* \geq \tr \left(
        n \Exp_{\theta \sim \pi} \mca J_F(\theta) + J(\pi)
    \right)^{-1} 
    = \df 1 n \tr \left(
        \Exp_{\theta \sim \pi} \mca J_F(\theta) + \df{\pi^2}{n\delta^2}I_d 
    \right)^{-1}
\] 
Choose $\delta = n^{-1/4}$ and apply the continuity 
of $\mca J_F(\theta)$ in at $\theta$. 
</details>

## Miscellany: MLE {-}

Consider $X^n=(X_j)\sim P_{\theta_0}$ i.i.d., the 
maximum likelihood estimator (MLE) is 
\[ 
    \hat \theta_{\mrm{MLE}} 
    = \mrm{arg}\max_{\theta\in \Theta} L_\theta(X^n), \quad 
    L_\theta(X^n) = \sum_{j=1}^n \log p_\theta(X_j)
\] 
For discrete distributions, we also have 
$\hat \theta_{\mrm{MLE}} \in 
\mrm{arg}\min_{\theta \sin \Theta} D(\hat P_n \| P_\theta)$. 
This implies that the expected log-likelihood is maximized 
at the true parameter value $\theta_0$. 
- Suppose $\theta \mapsto P_\theta$ is injective, then 
\[ 
    \Exp_{\theta_0}[L_\theta - L_{\theta_0}] 
    = \Exp_{\theta_0} \left[
        -\sum \log \df{p_{\theta_0}(X_j)}{p_\theta(X_j)}
    \right] = -nD(P_{\theta_0} \| P_\theta) < 0 
\] 
Assuming regularity conditions and together 
with l.l.n, this ensures 
consistency $\hat \theta_{n\to \infty}\to \theta_0$. 


- Assuming more regularity conditions, we can approximate 
\[ 
    L_\theta = L_{\theta_0} + 
    (\theta - \theta_0)^T \sum_j V(\theta_0, X_j) 
    + \df 1 2 (\theta - \theta_0^T \left(
        \sum H(\theta_0, X_j)
    \right)(\theta - \theta_0) + \cdots 
\] 
Under regularity conditions, we have 
$\sum H(\theta_0, X_j) \to -n \mca J_F(\theta_0)$ 
and the linear term vanishing. This yields the stochastic 
approximation 
\[ 
    L_\theta \approx L_{\theta_0} 
    + \la \sqrt{n\mca J_F(\theta_0)} Z, \theta - \theta_0\ra 
    - \df n 2 (\theta - \theta_0)^T \mca J_F(\theta_0) (\theta - \theta_0)
\] 
Maximizing the RHS yields the asymptotic normal expression 
\[ 
    \hat \theta_{\mrm{MLE}} \approx \theta_0 
    + \df 1 {\sqrt n} \mca J_F(\theta_0)^{-1/2} Z
\] 