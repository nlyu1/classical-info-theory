# Lecture notes 

## Oct 2: Fisher information, classical minimax estimation {-}

Agenda: 

1. $\chi^2$ variational characterization. 
2. $1$-parameter families; minimax rate. 
3. HCR, and Fisher information. 
4. Cramer-Rau; van Trees inequality. 


Key takeaways: 

1. $R_n^*$ is the minimax rate of parameterization. 
2. LeCam-Hajek theory: $R_n^* = \df{1+o(1)}{n \min_\theta I_F(\theta)}$. 
3. To obtain more well-behaved inequalities, expand a single extremum 
    into a nested one (e.g. scalar multiple) then solve the closed form 
    of the inner optimization. 
4. All $f$-divergences are locally quadratic in parameteric families 
    with Hessian given by Fisher information. 
5. Cramer-Rau is an application of the variational characterization 
    of $\chi^2$. 

### Main content {-}

Recalling the variational characterization: 
\[ 
    D_f(P\|Q) = \sup_g \Exp_p g - \Exp_Q f^*\circ g
\] 
where the convex conjugate is given by 
\[ 
    f^*(h) = \sup_{t\in \R} th - f(t), \quad 
    f(t) = \sup_{h\in \R} th - f^*(h)
\] 
Recall that Donsker-varadhan is not linear in $\Exp_Q$, 
but there is a standard trick to rewrite the expectation. 

We now apply the variational characterization to $\chi^2$; 

:::{.proposition name="variational characterization of χ²"}
$\chi^2(P \|Q) 
= \sup_{h:\mca X\to \R} \Exp_p h(X) - \Exp_Q \left[
    h(X) + \df{h(X)^2}{4}
\right]$
:::
Expanding this out, the first term is very useful; 
but the second is not. The first step is to break a single 
extrema into two: 
\begin{align}
    \chi^2(P \|Q) 
    &= \sup_h \left[
        \Exp_p h - \Exp_Q h 
    \right] - \df 1 4 \Exp_Q h^2 \\ 
    &= \sup_g \sup_{\lambda\in \R} \lambda \left(
        \Exp_P g - \Exp_Q g 
    \right) - \df{\lambda^2}{4} \Exp_Q g^2  
    = \sup_g \df{(\Exp_p g - \Exp_Q g)^2}{\Exp_q g^2} 
\end{align}
As a consequence, we obtain 
\[ 
    \left(
        \Exp_P g - \Exp_Q g 
    \right)^2 \leq \chi^2(P \|Q) \Exp_Q g^2 
\] 
In fact, this equation is invariant under $g\mapsto g + c$, yielding 
\[ 
    \left(
        \Exp_P g - \Exp_Q g 
    \right)^2 \leq \chi^2(P \|Q) \Var_Q g^2 
\] 

<span style="color:green">
Exercise: $\forall g>0$, we have 
$\Exp_P g \leq 2\Exp_Q g + 2_? H^2(P, Q)$. 
</span>

### One-parameter families; minimax rates {-}

Statisticians care about sub-manifolds 
of the probability simplex. 

For one-parameter families, we typically have 
\[ 
    P^\theta(dx) = p^\theta(x) \mu(dx) 
\] 
For discrete, $\mu$ is counting; 
for real-valued, $\mu$ is Lebesgue. 


**Parameter estimation**: given $X_1, \cdots, X_n \sim P^\theta$; 
find function $\hat \theta(X_1, \cdots, X_n)$ such that 
$\hat \theta \approx \theta$; here $\approx$ is defined w.r.t. 
a risk function 
\[ 
    R(\hat \theta, \theta) = \Exp_{X^n} (\theta - \hat \theta(X^n))^2
\] 
Note that the first argument is a function, while the second is a 
parameter coordinate. 

Example: $\hat \theta_0 = \df 1 n \sum X_j$ for Bernoulli 
parameter estimation; we can compute 
\[ 
    R(\hat \theta, \theta) = \df{\theta \bar \theta}{n}
\] 
Recall that it's MLE and unbiased. To get mid of the 
$\theta$-dependence, we can consider 
\[ 
    R^{\mrm{max}}(\hat \theta) 
    = \sup_\theta R(\hat \theta_1, \theta)
\] 

Consider a naive estimator $\hat \theta_1 = 1/2$ 
(this has definite bias but not variance) and 
\[ 
    \hat \theta_\lambda = \lambda \hat \theta_1 
    + \bar \lambda \hat \theta_{\mrm{MLE}}
\] 
One can compute risk = bias^2 +variance^2 
and choose an optimal $\lambda_n^*= \df 1 {1 + \sqrt n}$; 
this allows one to optimize the risk everywhere. 

**Theorem** The optimal shrinkage estimator 
$\hat \lambda_{\lambda_n^*}$ saturates the minimax risk 
\[ 
    \inf_{\hat \theta} R^{\mrm{max}}(\hat \theta) 
    = \df 1 {4(1 + \sqrt n)^2}
\] 
_Key idea:_ obtain MLE; identify points of worst 
performance (minimum Fisher information), then bias 
towards it. 

The optimal minimax risk profile (minimax rate) is 
\[ 
    R_n^* = \inf_{\hat \theta} \sup_\theta R(\hat \theta, \theta) 
\] 

### HCR inequality; Fisher information{-}

:::{.proposition}
$\forall \hat \theta$ and $\theta, \theta_0$ we have
\[ 
    R_n^* \geq \Var_{P^\theta} \geq \df{
        \left(\Exp_{P^\theta} \hat \theta 
        - \Exp_{P^{\theta_0}} \hat \theta\right)^2
    }{
    \chi^2(P^{\theta \otimes n}\| P^{\theta_0 \otimes n})
    }
\] 
:::
_Proof:_ The first inequality follows from 
bias-variance decomposition. For the second, take 
$g(X^n) = \hat \theta(X^n)$ in the variational characterization 
of $\chi^2$. 

To bring Fisher information into the picture, consider 
\[ 
    \chi^2 (P^\theta \| P^{\theta_0}) 
    = \sum \df{P^\theta(x)^2}{P^{\theta_0}(x)} - 1
\] 
Take $\theta = \theta_0 + \epsilon$ for small $\epsilon$; 
Taylor-expand to obtain 
\begin{align}
    P^\theta(x) = P^{\theta_0}(x) 
    + \pd \theta P^\theta \big|_{\theta = \theta_0} (x)\epsilon + 
    \cdots 
\end{align}
Substitute this into the expression for $\chi^2$ to obtain 
\[ 
    \chi^2 = \sum_x \df{\left[
        P^{\theta_0}(x) + \epsilon \pd \theta P^\theta(x)
    \right]^2}{P^{\theta_0}(x)} - 1 + o(\epsilon^2)
\] 
Expanding the term, the linear term is $0$ (pull $\pd \theta$ out): 
\[ 
    \sum_x \pd \theta P^{\theta}(x) \big|_{\theta = \theta_0} 
    = 0 
\] 
The null term sums to $1$, cancelling the $-1$, yielding the 
local definition of $\chi^2$ 
\[ 
    \chi^2 = \epsilon^2 \sum_x 
    \df{\left[\pd \theta P^{\theta_0}(x)\right]^2}
    {P^{\theta_0}(x)} + o(\epsilon^2)
\] 

Recall that the **Fisher information** is given by 
\[ 
    \mca J_F(\theta; \{P^\theta\}_{\theta \in \Theta}) 
    = \int \df{\left[\pd \theta P^\theta(x)\right]^2}{P^\theta(x)} \, \mu(dx)
\] 
Fisher information is just the second-order derivative 
of $\chi^2$. 


There are two notions of distance: 
Fisher information tells us, when we nudge 
$\theta \mapsto \theta + \epsilon$, 
how much is the statistical distance between $P^\theta$ 
and $P^{\theta + \epsilon}$. 

Moreover, Fisher information is additive under 
tensorization: KL is additive under tensorization 
and apply the locality principle. 

Theorems about Fisher information require a lot 
more assumptions. 

## Oct 7: Data compression {-}

:::{.theorem name="local expansion of χ"}
For $\theta_0 = 0$, when the following conditions hold: 

1. $\theta \in [0, \tau)$. 
2. There exists $\dot p^\theta(x)$ satisfying 
\[ 
    P^\theta(x) = P^0(x) + \int_0^\theta \dot p^t(x)\, dt 
\] 
3. The fisher information is defined _everywhere_ 
    in a connected interval open interval about $\theta_0$: 
\[ 
    \int dx\, \sup_{0\leq t < \tau} 
    \df{\dot p^t(x)^2}{p^0(x)} < \infty 
\] 

Then $\chi^2(P^{\theta_0 + \delta} \| P^{\theta_0}) 
    + \delta^2 \mca J_F(\theta_0) + o(\delta^2)$. 
:::
The quadratic local behavior of $\chi^2$ requires 
not only finite Fisher information at $\theta_0$; 
it also requires $\mca J_F$ to be finite almost everywhere 
on a nontrivial interval. 

<span style="color:green">
In summary, $\chi^2$ is nice for the "good" cases 
where $\mca J$ is finite a.e. However, for some 
irregular cases it's best to use 
$H^2(P^{\theta_0 + \delta}, P^\theta)$, whose local 
behavior is always determined by the local Fisher information. 
</span>

A location family is a one-parameter family for which 
the parameter controls the displacement of a fixed density $\nu$, 
e.g. $\mca N(\theta, 1)$; for this family, 
$\mca J(\theta) = \mca J(0)$, which we can abbreviate as 
\[ 
    \mca J(\nu) = \int \df{(\nu')^2}{\nu} \, dx 
\] 
Note that Fisher information $\mca J_F$ is originally defined for 
a one-parameter family; with the notation $\mca J(\nu)$, 
we're assuming the one-parameter family to be the location family 
of the given density. 

:::{.theorem name="van Trees inequality"}
Under regularity assumptions on $P^\theta$, 
for every estimator $\hat \theta(X)$ and prior 
$\pi$ 
\[ 
    \Exp_{\theta \sim \pi} \Exp_{X\sim P^\theta} \left[
        [\theta - \hat \theta(X)]^2
    \right] \geq \df 1 {\mca J(\pi) + 
    \Exp_{\theta \sim \pi} \mca J_F(\theta)}
\] 
:::

:::{.corollary}
For $n$ i.i.d. observations, for every $\pi$ 
\[ 
    R_n^* \geq \df 1 {\mca J(\pi) + 
    n\Exp_{\theta \sim \pi} \mca J_F(\theta)}
\] 
:::
van Trees is important important because it provides 
a lower bound on minimax risk based on 
local quantities $\mca J_F(\theta)$. 

Two takeaways for information theorists: 

1. In Bayesian setting, apply information theory techniques 
    to the joint P_{\tilde \theta, X}$. 
2. In local neighborhoods, $\chi^2$ satisfies an additive 
    chain rule (since it's close to KL). 

Van Trees replaced $\mca J_F$ with the expectation of 
$\mca J_F$, and it applies to every (instead of unbiased) estimators. 

## Oct 9: data compression II {-}

Overview: 

1. Review 
2. Distribution of fixed length
3. Arithmetic encoder 
4. Stationary, ergodic data sources
5. Lempel-Ziv (adaptive, universal compression)

Main takeaways: 

1. AEP. 
2. Optimal compression is possible using (1) arithmetic encoding 
    and (2) optimal next-token prediction. 

Information theory is about bounding 
hard, intractable operational quantities with mathematically 
analyzable quantities. 

### Review {-}

Assume $P_X$ known and ordered $P_X(1)\geq P_X(2)\geq \cdots$. 
The optimum compressor $f^*(x)$ satisfies 
\[ 
    l(f^*(X)) = \lceil \log_2 X\rceil 
\] 
We also proved that the expected 
optimum compression length satisfies 
\[ 
    \Exp l(f^*(X)) \approx H(X)
\] 
In fact, this is upper-bounded by $H(X)$. 
_But this is not an algorithm!_ 

1. Sorting the distribution is intractible. 
2. It's variable-length but not prefix-free!
    We're assuming one-shot compression 
    (comma is for free). Howevver, prefix free code, 
    the optimal expected bound is lower-bounded by $H(X)$. 
3. Proof idea: typicality; break region into typical (tractable) 
    and atypical regions with vanishing probability. 
4. Asymptotic equipartition property: i.i.d. implies 
    law of large numbers (in log-space). 

Distribution of compression length is distributed (up to constants) 
as the entropy density. This is great because entropy density tensorizes. 

:::{.proposition name="distribution of compression length"}
Define random variable $L^* = l(f^*(X))$. 
Denote the entropy density $i_X(a) = -\log P_X(a)$; 
the optimal compressor should compress symbol $a$ to roughly 
this length: 
\[ 
    \Pr[i(X) \leq k] \leq \Pr[L^* \leq K] \leq \Pr[i(X) \leq k + \tau]
    + 2^{1-\tau}
\] 
This holds for every $k\in \mbb Z_+, \tau>0$. 
:::
_Proof:_ Left-bound is easy: 
\[ 
    L^*(x) = \lceil \log_2 X\rceil \leq \log 2 |X| 
    \leq -\log P_X(x) = i_X(x)
\] 
To bound the second term, decompose the probability 
\begin{align}
    \Pr[L^* \leq k] 
    &= \Pr[L^* \leq k, i(x) \leq k+\tau] + \Pr[L^* \leq k, i(x) > k+\tau] \\ 
    &\leq \Pr[i_X(x) \leq k+\tau] + (\cdots)
\end{align}
The second term is bounded by the number of strings 
which achieves this $2^{k+1}$ times the maximum probability 
they're obtaining $2^{-k-\tau}$, yielding $2^{1-\tau}$. 

Corrolary: if, for some sequence of r.v., the normalized entropy 
rate converges in distribution to $U$, then the normalized optimal 
compression length (for asymptotically large block length) 
also converges to $U$. 

Another corollary: if $S_j\sim P_S$ i.i.d., then 
\[ 
    \df 1 n i_{S_1^n}(S_1^n) = -\df 1 n \log P_{S^n}(S^n) 
    = -\df 1 n \sum_{j=1}^n \log P_S(s_j) \to H(S)
\] 
This implies that the expectation of the optimal compression length 
for i.i.d. source $X$ converges to $H(X)$ in the limit of asymptotically 
large block lengths. 

<u> This is a nontrivial result, because the optimal compressor 
is a very freely-specified object, but we are able to bound its 
behavior very neatly. </u>

In particular, recall that the optimal-compressor maps 
highest-probability atoms to the empty set, but we see from the 
asymptotic Gaussian distribution of the compression length that 
they have almost negligible density. This is a demonstration of the 
**asymptotic equipartition property**, which states that 
for i.i.d. sources, the overwhelming number of sequences have 
the same probability given by $e^{H(X)}$. 

### Arithmetic encoder {-}

Given a general $\{X_j\}$-process and wishing to compress $X_1^n$. 

First consider i.i.d process. Order the alphabet and 
recursively partition the interval $[0, 1]$ so that 
each interval has length equal to its probability. 
The trick is to find the largest dyadic interval (recursive 
binary partition) that fits inside the interval of the message. 
For example: 
\[ 
    [0, 1]\to \emptyset, \quad [6\cdot 2^{-3}, 7\cdot 2^{-3}]\to 110
\] 
In the limit that the codestring goes to infinity, 
the distribution of binary expansion will be uniform. 

Fact for the compression length of arithmetic encoder for i.i.d. terms: 
\[ 
    \log \df 1 {P_{X^n}(x^n)} \leq l(f_{\mrm{ae}}(x^n) 
    \leq \log_2 \df 1 {P_{X^n}(x^n)} + 2
\] 
The arithmetic encoder is additionally sequential: it does not 
need to consume the full string to start outputting compression; 
the same holds for the decompressor. 

Implementing the arithmetic encoder for general non-i.i.d. 
distributions just replace subsequent intervals by the marginals 
$P_{X_n \| X^{(n-1)}}$. 

<u>
This means that, if we can sequentially predict the marginal 
$P_{X_n \|X^{(n-1)}}$ very well (e.g. next-token prediction LLM), 
then we can close-to-optimal compress a non-i.i.d. distribution 
by combining this with the arithmetic encoder. 
</u>


:::{.theorem name="Shannon-McMillan-Breimen"}
A.e.p. holds w.r.t. the entropy rate 
if $\{X_j\}$ is a stationary (ensures 
existence of the entropy rate) ergodic process. 
:::

Shannon's proof: 
every stationary ergodic process can be arbitrarily 
approximated by a $m\to \infty$-order Markov chain. 

### Lempel-Ziv {-}

Central question: how to compress well without $P_{X^n}$? 

:::{.lemma name="Katz's lemma"}
Given a stationary ergodic process $(X_{j\in \mbb Z})$. 
Define $L = \inf\{t>0: X_{-t}=X_0\}$, then 
\[ 
    \Exp[L | X_0=u] = \Pr[X_0=u]^{-1}
\] 
:::
_Proof:_ consider the probability that we don't see $u$ 
when we look back for $k$ steps, using stationarity: 
\begin{align}
    \Pr[L>K, X_0=u]
    &= \Pr[X_0=u, X_{-1}\neq U, \cdots, X_{-k}\neq u]  \\ 
    &= \Pr[X_k=u, X_{k-1}\neq U, \cdots, X_0\neq u]
    = \Pr[E_k]
\end{align}
_todo_ Another key point is that for stationary ergodic 
processes, $\Pr[\bigcup E_k] = 1$. 

Using Katz's lemma, we can do an unbiased estimation of $\Pr[X_0=u]$ 
by looking back. 


<span style="color:green">
Do probability mixtures satisfy all of the local Fisher 
regularity conditions?
</span>

## Oct 28: Binary hypothesis testing {-}

Agenda: 

1. Finish universal compression. 
2. Define binary hypothesis testing; define $R(P, Q)$. 
3. Stein's lemma. 

### More on universal compression {-}

Recall that universal compression wants to compress 
$P_{X^n}$ to entropy, where $P_{X^n}$ is unknown 
and drawn from a class $\Pi$. 

The **fundamental limit** of compression is 
_the redundancy of class $\Pi$_, defined by 
\[ 
    R_n^*(\Pi) = \inf_{Q_{X^n}} 
    \sup_{P_{X^n}\in \Pi} \Exp_{X^n\sim P_{X^n}} 
    \log \df 1 {Q_{X^n}(X^n)} - H(X^n)
\] 
Note the order of $\inf$ and $\sup$! Else 
the "redundancy" would be trivial. 

It turns out that for the uniform mixture of 
all Bernoullis approximately, for all $\theta$. 
\[ 
    Q_{X^n} = \int_0^1 d\theta\, \mrm{Ber}(\theta)^{\otimes n} 
\] 
satisfies $D(\mrm{Ber}(\theta)^{\otimes n} \| Q_{X^n}) \leq \log n$ 
To see this, for non-extremal $\theta, \theta'$ we have 
\[ 
    d(\theta \| \theta') \approx \df 1 2 (\theta - \theta')^2
\] 
The distribution $Q_{X^n}$ is the _universal probability assignment._
(or maximally ignorant 

**Universal data compression (probability assignment) $\approx$ 
choosing the best prior.**

The most important theorem: redundancy is equal to the following capacity: 
\[ 
    R_n^*(\Pi) = \sup_{\pi \in \mca P(\Pi)} I(\theta; X^n)
\] 

<u>Jeffrey's (objective) prior</u>: given smoothness conditions for a family $\Theta$, 
we have 
\begin{align}
    R_n^*(\Theta) 
    &= \df d 2 \log \df{n}{2\pi e} + \log \int_{\Theta} 
    \sqrt{\det \mca J_F(\theta)}\, d\theta + o(1) \\ 
    \pi_n^* &\to \pi_\infty^* \text{ weakly } 
    = \df 1 Z \sqrt{\det \mca J_F(\theta)}\, d\theta 
\end{align}
The objective prior is invariant to 
reparameterizations. Importantly, for Bernoullis the Jeffreys prior is 
\begin{align}
    \pi^*(\theta) = \df 1 {\theta(1 - \theta)}\quad k=2, \text{ else }
    \mrm{Direchlet}(1/2, \dots, 1/2). 
\end{align}
The marginal estimator $Q_{X_t|X_1^{t-1}}$ turns out to correspond 
to the add-half estimator (KT code). 

_Connection between universal compression and sequential prediction_: 
Recalling the setup of sequential prediction, for each step the predictor 
is penalized by the negative log likelihood (with minimizer given by the true 
distribution). From learning theory, the natural metric for this problem 
(of a complexity of a class $\Pi$) is 
\[ 
    \sup_{P_X\in \Pi} \Exp \sum_{t=1}^n 
    \log \df 1 {\tilde P_t(X_t; X_1^{t-1})}
    - \log \df 1 {P_{X_t|X_1^{t-1}}}
\] 
If some estimator achieves $o(n)$ for the preceding quantity, 
then the class is learnable since the estimator is asymptotically as 
good as the oracle. It turns out that 
\[ 
    \inf_{\tilde P} \text{regret} = R_n^*(\Pi).
\] 
Relation to _in-context learning_: training phase allows for representation 
for different generating procedures (e.g. Jeffreys prior), 
and context chooses the posterior which specializes to the suitable 
data-generating process. 

**Every regret-minimizing estimator is an in-context learner.** 
This is related to a theorem for density estimation (related to the 
Young-Barron bound): 
given $X_1, \cdots X_n\sim P\in \Pi$ i.i.d, we wish to find $\tilde P$ 
which will estimate $P$ very well. 

:::{.theorem}
For every i.i.d. class $\Pi$ as above, there exists estimator such that 
\[ 
    \Exp \mrm{TV}^2 \leq \Exp D(P\|\tilde P_{\text{this is random}}) 
    \leq \df{R_{n+1}^*(\Pi)}{n+1}
\] 
:::

<div style="color:blue">
This is a very insightful theorem since the right-hand side quantity 
can be upper-bounded by the redundancy theorem. 
Universal data compression (information)
reduces to the problem of universal probability assignment, 
which is simultaneously the core of density estimation (statistics)
and sequential prediction (learning). 

**Minimum description length principle**: the more likely 
(class) of data-generating process is the one under whose universal 
compressor compresses the data down to a smaller length; this 
is equivalent to point-point hypothesis testing after collapsing 
the classes down to their universal prior. 
</div>

### Binary hypothesis testing {-}
We are interested in simple v. simple hypothesis testing i.e. 
distinguishing beween $\theta_1\neq \theta_2\in \Theta$ (Neyman-Pearson); 
equivalently, distinguishing between $P_{X^n}$ and $Q_{X^n}$. 
Our goal is to design a binary estimator $Z=Z(X^n)\in \{0, 1\}$, with 
$0$ denoting $X^n\sim P$ and $1$ denoting $X^n\sim Q$ 
(the first serious treatment originated with radars detecting incoming planes). 
Two associated quantities: 

1. $\alpha = P[Z=0]$: probability of success under $P$. 
2. $\beta = Q[Z=0]$: probability of failure under $Q$. 

:::{.theorem name="Stein's lemma"}
Among all tests with $\alpha \geq 1 - \epsilon$, the smallest possible 
$\beta = e^{-nD(P\|Q) + o_\epsilon(n)}$ assuming $X_j^n\sim P$ or $Q$. 
:::

Several comments: 

1. This theorem is not symmetric w.r.t. $P\leftrightarrow Q$. 
2. The dependence upon $\epsilon$ is captured in $o_\epsilon(n)$. 

To reason about binary hypothesis testing, define 
\[ 
    \mca R(P, Q) = \{(P[Z=0], Q[Z=0])\}
\] 

:::{.proposition}
$\mca R\subset [0, 1]^2$ is closed, convex, symmetric 
w.r.t. $(1/2, 1/2)$, and 
\[ 
    \mca R = \overline{\mrm{co}\, 
        \mca R_{\mrm{det}}(P, Q)
    }
\] 
where $\mca R_{\mrm{det}}$ denotes the set of deterministic 
joint ranges. 
:::

As a corollary, the problem can be simplified by noting that 
\[ 
    \mca R_{\mrm{det}} = \{(P[E], Q[E]), \quad E\in \mca X\}
\] 
For the Bernoulli case, $E$ takes values in $\{\emptyset, 0, 1, \mca X\}$. 
The first and last cases correspond to $(0, 0)$ and $(1, 1)$. 

Perspective on the proof of the NP-lemma: 
to understand $\mca R_{\mrm{det}}$, it suffices to understand 
its extremal points, which corresponds to the problem 
\[ 
    \sup_Z P[Z=0]_{=\alpha} - e^\gamma Q[Z=0]_{=\beta}
\] 
The parameterization $e^\gamma$ simply assures positivity. 
We "scan" over the supporting hyperplanes with a certain offset. 
The upshot is that BHT boils down to computing $P[\log P/Q > \gamma]$ 
and $Q[\log P/Q < \gamma]$ for a range of $\gamma$'s. 

The "fatter" this joint range is, the more distinguishable the 
two distributions are; identical distributions correspond to the 
line $\alpha=\beta$ with zero measure. This motivates the 
following results: 

:::{.proposition}
$d(\alpha \| \beta) \leq D(P\|Q)$ for $\alpha, \beta \in \mca R$. 
This results from applying DPI to the estimator $Z$. 
:::

## Nov 4. Channel coding {-}

Agenda: 

1. Finish Sanov's theorem, large deviation theory, and I-projection. 
2. Define error-correction codes; example using BSC. 
3. Weak converse bound. 

### BHT, large deviations theory {-}

To properly understand hypothesis testing, we need to 
understand the large-deviations question: 
to compute $E_0, E_1$, we need to compute $E$ in 
\[ 
    \Pr[\bar T < \gamma] \sim e^{-nE(\gamma)}
\] 

:::{.theorem name="Sanov principle"}
under regularity conditions on $\mca E$, we have 
$\Pr[\hat P_n\in \mca E] = e^{-nE + o(n)}$, where 
\[ 
    E = \inf_{R\in \mca E} D(R\|P)
\] 
:::

<span style="color:green">
Extremely important proof below!
</span>

:::{.lemma}
If $\mca X$ is finite and $\mca E$ is _convex_ with 
nonempty interior, then Sanov holds. 
:::

First consider an upper bound: $\Pr[\hat P_n\in \mca E] \leq e^{-nE}$. 
By $P_{X^n} = P^{\otimes n}$, define $\tilde P_{X^n} = P_{X^n | E_n}$. 
Note that the KL divergence 
\[ 
    D(\tilde P_{X^n} \| P_{X^n}) 
    = \Exp_{P_{X^n}} \log \df{P_{X^n|E_n}(x)}{P_{X^n}(x)}
    = \log \df 1 {P_{X^n}(E_n)}
\] 
This is the extension of Theorem 11.9. This implies a lot of bounds! 
Chernoff, Berstein, Benett, Okamota, etc. In particular, 
\[ 
    \forall k, \quad  \Pr[\mrm{Bin}(n, p)\geq k] \leq \exp \left[-n d(d/n\|p)\right]
\] 

<span style="color:green">
Idea to proving lower bounds: prove for the easy case, 
then lift to the general case using information measures. 
</span>

To prove the lower bound, if $P\in \mrm{int}(E)$,
then using the law of large numbers yield $\Pr[\hat P_n\in \mca E]\to 1$. 
Otherwise, take $R\in \mrm{int}(\mca E)$, then 
$R^{\otimes n}[\hat P_n\in \mca E]\to 1$. Apply DPI on the indicator $1_{E_n}$. 

### I-projections {-}

Take $X\in \R^d$ samples from $X\sim P$; define the information projection by 
\[ 
    I(\gamma) = \inf D(R\|P)
\] 
where infimum is over $\Exp_P[X]=\gamma$. 

:::{.proposition name="tilting is optimal if exists"}
Suppose there exists $\lambda \in \R^d$ such that 
$\Exp_{X\sim P_\lambda}[X]=\gamma$, then 
for every $R$ satisfying $\Exp_R[X]=\gamma$, we have 
\[ 
    D(R\|P) = D(R\|P_\lambda) + D(P_\lambda \| P) \geq D(P_\lambda \| P). 
\] 
:::
_Proof:_ $D(R\|P) = \Exp_R \log \df{dR}{dP} \df{dR}{dP_\lambda}$ and regroup 
by cross-terms. Recognize $\Exp_R \log dP_\lambda/dP = \Exp_R[\lambda^T X - \psi(\lambda)]$. 

:::{.theorem name="I-projection on a hyperplane"} 

1. If $\exists R: \Exp_R[X]=\gamma$ and $D(R\|P)<\infty$, 
then $\gamma\in \mrm{csupp}(P)$. 
2. $\exists P_\lambda$ with $\Exp_R[X]=\gamma \iff \gamma\in \mrm{int}(\mrm{csupp}\, P)$. 
:::

Here the convex support of $P$ is the intersection of all sets $S$ 
such that $P[S]=1$ and $S$ is closed and convex; to visualize this, imagine 
the mixture of two disjointly supported distributions. 

In other words, information projection is solvable iff $\gamma$ is in 
csupp, and it is solvable by tilting if it $\gamma$ is in the interior. 

:::{.theorem name="tradeoff in error exponents"}
$E_1(E_0) = \inf_{D(R\|P)<E_0} D(R\|Q)$. 
:::

Relation to Wald's SPRT principle by slightly 
modifying the problem formulation: commit to an expected 
instead of exact number of samples. For composite v.s. 
composite tests, Hellinger computation becomes important. 

Hypothesis testing is still a thriving topic! 
Instead of specifying the exact generating distributions 
$P, Q$, only samples are provided (**likelihood-free inference**).

### Channel coding {-}

We started with information measures, next found 
in compression that _compression_ is described by entropy; 
consequently, the optical error decay in _hypothesis testing_
is provided by divergence. The problem of _channel coding_
is solved by mutual information. 

Given a channel $P_{Y|X}:\mca X\to \mca Y$, we wish to 
construct $f:[M]\to \mca X$ and $g:\mca Y\to [M]\cup \{e\}$ 
such that the maximal error rate (across all $j$) $\leq \epsilon$. 
For generality, allow randomized $f, g$. 
\[ 
    W\xrightarrow f X\xrightarrow{P_{Y|X}} Y\xrightarrow g \hat W
\] 
This is a maximal probability of error code, in comparison to 
average probability of error codes. 

1. If $f$ is given, the design of $g$ is essentially $M$-ary H. T. 
    For $M>2$ there is no uniquely optimal criteria. 
    However, if we postulate $j\sim [M]$ uniformly, then 
    the optimal solution is given by maximum-likelihood decoder. 

Note that the maximum-likelihood decoder tesselates the 
decoder-input space according to 1-nearest neighbor. 

_In information theory, almost all of the work is designing a great distribution._