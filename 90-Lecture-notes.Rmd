# Lecture notes 

## Oct 2: Fisher information, classical minimax estimation {-}

Agenda: 

1. $\chi^2$ variational characterization. 
2. $1$-parameter families; minimax rate. 
3. HCR, and Fisher information. 
4. Cramer-Rau; van Trees inequality. 


Key takeaways: 

1. $R_n^*$ is the minimax rate of parameterization. 
2. LeCam-Hajek theory: $R_n^* = \df{1+o(1)}{n \min_\theta I_F(\theta)}$. 
3. To obtain more well-behaved inequalities, expand a single extremum 
    into a nested one (e.g. scalar multiple) then solve the closed form 
    of the inner optimization. 
4. All $f$-divergences are locally quadratic in parameteric families 
    with Hessian given by Fisher information. 
5. Cramer-Rau is an application of the variational characterization 
    of $\chi^2$. 

### Main content {-}

Recalling the variational characterization: 
\[ 
    D_f(P\|Q) = \sup_g \Exp_p g - \Exp_Q f^*\circ g
\] 
where the convex conjugate is given by 
\[ 
    f^*(h) = \sup_{t\in \R} th - f(t), \quad 
    f(t) = \sup_{h\in \R} th - f^*(h)
\] 
Recall that Donsker-varadhan is not linear in $\Exp_Q$, 
but there is a standard trick to rewrite the expectation. 

We now apply the variational characterization to $\chi^2$; 

:::{.proposition name="variational characterization of χ²"}
$\chi^2(P \|Q) 
= \sup_{h:\mca X\to \R} \Exp_p h(X) - \Exp_Q \left[
    h(X) + \df{h(X)^2}{4}
\right]$
:::
Expanding this out, the first term is very useful; 
but the second is not. The first step is to break a single 
extrema into two: 
\begin{align}
    \chi^2(P \|Q) 
    &= \sup_h \left[
        \Exp_p h - \Exp_Q h 
    \right] - \df 1 4 \Exp_Q h^2 \\ 
    &= \sup_g \sup_{\lambda\in \R} \lambda \left(
        \Exp_P g - \Exp_Q g 
    \right) - \df{\lambda^2}{4} \Exp_Q g^2  
    = \sup_g \df{(\Exp_p g - \Exp_Q g)^2}{\Exp_q g^2} 
\end{align}
As a consequence, we obtain 
\[ 
    \left(
        \Exp_P g - \Exp_Q g 
    \right)^2 \leq \chi^2(P \|Q) \Exp_Q g^2 
\] 
In fact, this equation is invariant under $g\mapsto g + c$, yielding 
\[ 
    \left(
        \Exp_P g - \Exp_Q g 
    \right)^2 \leq \chi^2(P \|Q) \Var_Q g^2 
\] 

<span style="color:green">
Exercise: $\forall g>0$, we have 
$\Exp_P g \leq 2\Exp_Q g + 2_? H^2(P, Q)$. 
</span>

### One-parameter families; minimax rates {-}

Statisticians care about sub-manifolds 
of the probability simplex. 

For one-parameter families, we typically have 
\[ 
    P^\theta(dx) = p^\theta(x) \mu(dx) 
\] 
For discrete, $\mu$ is counting; 
for real-valued, $\mu$ is Lebesgue. 


**Parameter estimation**: given $X_1, \cdots, X_n \sim P^\theta$; 
find function $\hat \theta(X_1, \cdots, X_n)$ such that 
$\hat \theta \approx \theta$; here $\approx$ is defined w.r.t. 
a risk function 
\[ 
    R(\hat \theta, \theta) = \Exp_{X^n} (\theta - \hat \theta(X^n))^2
\] 
Note that the first argument is a function, while the second is a 
parameter coordinate. 

Example: $\hat \theta_0 = \df 1 n \sum X_j$ for Bernoulli 
parameter estimation; we can compute 
\[ 
    R(\hat \theta, \theta) = \df{\theta \bar \theta}{n}
\] 
Recall that it's MLE and unbiased. To get mid of the 
$\theta$-dependence, we can consider 
\[ 
    R^{\mrm{max}}(\hat \theta) 
    = \sup_\theta R(\hat \theta_1, \theta)
\] 

Consider a naive estimator $\hat \theta_1 = 1/2$ 
(this has definite bias but not variance) and 
\[ 
    \hat \theta_\lambda = \lambda \hat \theta_1 
    + \bar \lambda \hat \theta_{\mrm{MLE}}
\] 
One can compute risk = bias^2 +variance^2 
and choose an optimal $\lambda_n^*= \df 1 {1 + \sqrt n}$; 
this allows one to optimize the risk everywhere. 

**Theorem** The optimal shrinkage estimator 
$\hat \lambda_{\lambda_n^*}$ saturates the minimax risk 
\[ 
    \inf_{\hat \theta} R^{\mrm{max}}(\hat \theta) 
    = \df 1 {4(1 + \sqrt n)^2}
\] 
_Key idea:_ obtain MLE; identify points of worst 
performance (minimum Fisher information), then bias 
towards it. 

The optimal minimax risk profile (minimax rate) is 
\[ 
    R_n^* = \inf_{\hat \theta} \sup_\theta R(\hat \theta, \theta) 
\] 

### HCR inequality; Fisher information{-}

:::{.proposition}
$\forall \hat \theta$ and $\theta, \theta_0$ we have
\[ 
    R_n^* \geq \Var_{P^\theta} \geq \df{
        \left(\Exp_{P^\theta} \hat \theta 
        - \Exp_{P^{\theta_0}} \hat \theta\right)^2
    }{
    \chi^2(P^{\theta \otimes n}\| P^{\theta_0 \otimes n})
    }
\] 
:::
_Proof:_ The first inequality follows from 
bias-variance decomposition. For the second, take 
$g(X^n) = \hat \theta(X^n)$ in the variational characterization 
of $\chi^2$. 

To bring Fisher information into the picture, consider 
\[ 
    \chi^2 (P^\theta \| P^{\theta_0}) 
    = \sum \df{P^\theta(x)^2}{P^{\theta_0}(x)} - 1
\] 
Take $\theta = \theta_0 + \epsilon$ for small $\epsilon$; 
Taylor-expand to obtain 
\begin{align}
    P^\theta(x) = P^{\theta_0}(x) 
    + \pd \theta P^\theta \big|_{\theta = \theta_0} (x)\epsilon + 
    \cdots 
\end{align}
Substitute this into the expression for $\chi^2$ to obtain 
\[ 
    \chi^2 = \sum_x \df{\left[
        P^{\theta_0}(x) + \epsilon \pd \theta P^\theta(x)
    \right]^2}{P^{\theta_0}(x)} - 1 + o(\epsilon^2)
\] 
Expanding the term, the linear term is $0$ (pull $\pd \theta$ out): 
\[ 
    \sum_x \pd \theta P^{\theta}(x) \big|_{\theta = \theta_0} 
    = 0 
\] 
The null term sums to $1$, cancelling the $-1$, yielding the 
local definition of $\chi^2$ 
\[ 
    \chi^2 = \epsilon^2 \sum_x 
    \df{\left[\pd \theta P^{\theta_0}(x)\right]^2}
    {P^{\theta_0}(x)} + o(\epsilon^2)
\] 

Recall that the **Fisher information** is given by 
\[ 
    \mca J_F(\theta; \{P^\theta\}_{\theta \in \Theta}) 
    = \int \df{\left[\pd \theta P^\theta(x)\right]^2}{P^\theta(x)} \, \mu(dx)
\] 
Fisher information is just the second-order derivative 
of $\chi^2$. 


There are two notions of distance: 
Fisher information tells us, when we nudge 
$\theta \mapsto \theta + \epsilon$, 
how much is the statistical distance between $P^\theta$ 
and $P^{\theta + \epsilon}$. 

Moreover, Fisher information is additive under 
tensorization: KL is additive under tensorization 
and apply the locality principle. 

Theorems about Fisher information require a lot 
more assumptions. 

## Oct 7: Data compression {-}

:::{.theorem name="local expansion of χ"}
For $\theta_0 = 0$, when the following conditions hold: 

1. $\theta \in [0, \tau)$. 
2. There exists $\dot p^\theta(x)$ satisfying 
\[ 
    P^\theta(x) = P^0(x) + \int_0^\theta \dot p^t(x)\, dt 
\] 
3. The fisher information is defined _everywhere_ 
    in a connected interval open interval about $\theta_0$: 
\[ 
    \int dx\, \sup_{0\leq t < \tau} 
    \df{\dot p^t(x)^2}{p^0(x)} < \infty 
\] 

Then $\chi^2(P^{\theta_0 + \delta} \| P^{\theta_0}) 
    + \delta^2 \mca J_F(\theta_0) + o(\delta^2)$. 
:::
The quadratic local behavior of $\chi^2$ requires 
not only finite Fisher information at $\theta_0$; 
it also requires $\mca J_F$ to be finite almost everywhere 
on a nontrivial interval. 

<span style="color:green">
In summary, $\chi^2$ is nice for the "good" cases 
where $\mca J$ is finite a.e. However, for some 
irregular cases it's best to use 
$H^2(P^{\theta_0 + \delta}, P^\theta)$, whose local 
behavior is always determined by the local Fisher information. 
</span>

A location family is a one-parameter family for which 
the parameter controls the displacement of a fixed density $\nu$, 
e.g. $\mca N(\theta, 1)$; for this family, 
$\mca J(\theta) = \mca J(0)$, which we can abbreviate as 
\[ 
    \mca J(\nu) = \int \df{(\nu')^2}{\nu} \, dx 
\] 
Note that Fisher information $\mca J_F$ is originally defined for 
a one-parameter family; with the notation $\mca J(\nu)$, 
we're assuming the one-parameter family to be the location family 
of the given density. 

:::{.theorem name="van Trees inequality"}
Under regularity assumptions on $P^\theta$, 
for every estimator $\hat \theta(X)$ and prior 
$\pi$ 
\[ 
    \Exp_{\theta \sim \pi} \Exp_{X\sim P^\theta} \left[
        [\theta - \hat \theta(X)]^2
    \right] \geq \df 1 {\mca J(\pi) + 
    \Exp_{\theta \sim \pi} \mca J_F(\theta)}
\] 
:::

:::{.corollary}
For $n$ i.i.d. observations, for every $\pi$ 
\[ 
    R_n^* \geq \df 1 {\mca J(\pi) + 
    n\Exp_{\theta \sim \pi} \mca J_F(\theta)}
\] 
:::
van Trees is important important because it provides 
a lower bound on minimax risk based on 
local quantities $\mca J_F(\theta)$. 

Two takeaways for information theorists: 

1. In Bayesian setting, apply information theory techniques 
    to the joint P_{\tilde \theta, X}$. 
2. In local neighborhoods, $\chi^2$ satisfies an additive 
    chain rule (since it's close to KL). 

Van Trees replaced $\mca J_F$ with the expectation of 
$\mca J_F$, and it applies to every (instead of unbiased) estimators. 

## Oct 9: data compression II {-}

Overview: 

1. Review 
2. Distribution of fixed length
3. Arithmetic encoder 
4. Stationary, ergodic data sources
5. Lempel-Ziv (adaptive, universal compression)

Main takeaways: 

1. AEP. 
2. Optimal compression is possible using (1) arithmetic encoding 
    and (2) optimal next-token prediction. 

Information theory is about bounding 
hard, intractable operational quantities with mathematically 
analyzable quantities. 

### Review {-}

Assume $P_X$ known and ordered $P_X(1)\geq P_X(2)\geq \cdots$. 
The optimum compressor $f^*(x)$ satisfies 
\[ 
    l(f^*(X)) = \lceil \log_2 X\rceil 
\] 
We also proved that the expected 
optimum compression length satisfies 
\[ 
    \Exp l(f^*(X)) \approx H(X)
\] 
In fact, this is upper-bounded by $H(X)$. 
_But this is not an algorithm!_ 

1. Sorting the distribution is intractible. 
2. It's variable-length but not prefix-free!
    We're assuming one-shot compression 
    (comma is for free). Howevver, prefix free code, 
    the optimal expected bound is lower-bounded by $H(X)$. 
3. Proof idea: typicality; break region into typical (tractable) 
    and atypical regions with vanishing probability. 
4. Asymptotic equipartition property: i.i.d. implies 
    law of large numbers (in log-space). 

Distribution of compression length is distributed (up to constants) 
as the entropy density. This is great because entropy density tensorizes. 

:::{.proposition name="distribution of compression length"}
Define random variable $L^* = l(f^*(X))$. 
Denote the entropy density $i_X(a) = -\log P_X(a)$; 
the optimal compressor should compress symbol $a$ to roughly 
this length: 
\[ 
    \Pr[i(X) \leq k] \leq \Pr[L^* \leq K] \leq \Pr[i(X) \leq k + \tau]
    + 2^{1-\tau}
\] 
This holds for every $k\in \mbb Z_+, \tau>0$. 
:::
_Proof:_ Left-bound is easy: 
\[ 
    L^*(x) = \lceil \log_2 X\rceil \leq \log 2 |X| 
    \leq -\log P_X(x) = i_X(x)
\] 
To bound the second term, decompose the probability 
\begin{align}
    \Pr[L^* \leq k] 
    &= \Pr[L^* \leq k, i(x) \leq k+\tau] + \Pr[L^* \leq k, i(x) > k+\tau] \\ 
    &\leq \Pr[i_X(x) \leq k+\tau] + (\cdots)
\end{align}
The second term is bounded by the number of strings 
which achieves this $2^{k+1}$ times the maximum probability 
they're obtaining $2^{-k-\tau}$, yielding $2^{1-\tau}$. 

Corrolary: if, for some sequence of r.v., the normalized entropy 
rate converges in distribution to $U$, then the normalized optimal 
compression length (for asymptotically large block length) 
also converges to $U$. 

Another corollary: if $S_j\sim P_S$ i.i.d., then 
\[ 
    \df 1 n i_{S_1^n}(S_1^n) = -\df 1 n \log P_{S^n}(S^n) 
    = -\df 1 n \sum_{j=1}^n \log P_S(s_j) \to H(S)
\] 
This implies that the expectation of the optimal compression length 
for i.i.d. source $X$ converges to $H(X)$ in the limit of asymptotically 
large block lengths. 

<u> This is a nontrivial result, because the optimal compressor 
is a very freely-specified object, but we are able to bound its 
behavior very neatly. </u>

In particular, recall that the optimal-compressor maps 
highest-probability atoms to the empty set, but we see from the 
asymptotic Gaussian distribution of the compression length that 
they have almost negligible density. This is a demonstration of the 
**asymptotic equipartition property**, which states that 
for i.i.d. sources, the overwhelming number of sequences have 
the same probability given by $e^{H(X)}$. 

### Arithmetic encoder {-}

Given a general $\{X_j\}$-process and wishing to compress $X_1^n$. 

First consider i.i.d process. Order the alphabet and 
recursively partition the interval $[0, 1]$ so that 
each interval has length equal to its probability. 
The trick is to find the largest dyadic interval (recursive 
binary partition) that fits inside the interval of the message. 
For example: 
\[ 
    [0, 1]\to \emptyset, \quad [6\cdot 2^{-3}, 7\cdot 2^{-3}]\to 110
\] 
In the limit that the codestring goes to infinity, 
the distribution of binary expansion will be uniform. 

Fact for the compression length of arithmetic encoder for i.i.d. terms: 
\[ 
    \log \df 1 {P_{X^n}(x^n)} \leq l(f_{\mrm{ae}}(x^n) 
    \leq \log_2 \df 1 {P_{X^n}(x^n)} + 2
\] 
The arithmetic encoder is additionally sequential: it does not 
need to consume the full string to start outputting compression; 
the same holds for the decompressor. 

Implementing the arithmetic encoder for general non-i.i.d. 
distributions just replace subsequent intervals by the marginals 
$P_{X_n \| X^{(n-1)}}$. 

<u>
This means that, if we can sequentially predict the marginal 
$P_{X_n \|X^{(n-1)}}$ very well (e.g. next-token prediction LLM), 
then we can close-to-optimal compress a non-i.i.d. distribution 
by combining this with the arithmetic encoder. 
</u>


:::{.theorem name="Shannon-McMillan-Breimen"}
A.e.p. holds w.r.t. the entropy rate 
if $\{X_j\}$ is a stationary (ensures 
existence of the entropy rate) ergodic process. 
:::

Shannon's proof: 
every stationary ergodic process can be arbitrarily 
approximated by a $m\to \infty$-order Markov chain. 

### Lempel-Ziv {-}

Central question: how to compress well without $P_{X^n}$? 

:::{.lemma name="Katz's lemma"}
Given a stationary ergodic process $(X_{j\in \mbb Z})$. 
Define $L = \inf\{t>0: X_{-t}=X_0\}$, then 
\[ 
    \Exp[L | X_0=u] = \Pr[X_0=u]^{-1}
\] 
:::
_Proof:_ consider the probability that we don't see $u$ 
when we look back for $k$ steps, using stationarity: 
\begin{align}
    \Pr[L>K, X_0=u]
    &= \Pr[X_0=u, X_{-1}\neq U, \cdots, X_{-k}\neq u]  \\ 
    &= \Pr[X_k=u, X_{k-1}\neq U, \cdots, X_0\neq u]
    = \Pr[E_k]
\end{align}
_todo_ Another key point is that for stationary ergodic 
processes, $\Pr[\bigcup E_k] = 1$. 

Using Katz's lemma, we can do an unbiased estimation of $\Pr[X_0=u]$ 
by looking back. 