---
title: "6.7480 Notes"
author: "Nicholas Lyu"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [refs.bib]
biblio-style: "numeric"
split_bib: yes
link-citations: true
---

\usepackage{cancel}
\usepackage{amsmath}
\usepackage{bm}
\newcommand{\pd}[1]{\partial_{#1}}

\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\mb}{\boldsymbol}
\newcommand{\mrm}{\mathrm}
\newcommand{\mca}{\mathcal}
\newcommand{\mfk}{\mathfrak}
\newcommand{\tr}{\mrm{tr}} 
\newcommand{\df}{\dfrac}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\dag}{\dagger}

\newcommand{\Cl}{\mca C}
\newcommand{\Gr}{\mca G}
\newcommand{\Pf}{\mrm{Pf}}
\newcommand{\Pa}{\mca P}
\newcommand{\R}{\mbb R}

\newcommand{\poly}{\mrm{poly}}
\newcommand{\Exp}{\mbb E}
\newcommand{\Var}{\mrm{Var}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\Pr}{\mrm{Pr}}
\newcommand{\TV}{\mrm{TV}}
\newcommand{\LC}{\mrm{LC}}
\newcommand{\JS}{\mrm{JS}}
\newcommand{\ot}{\otimes}



# Preface {-}

Fundamentally, information and entropy captures 
how our uncertainty in a quantity changes after 
observations. Some recurring themes in the book are: 

1. Fundamental properties of information measures (e.g. entropy, mutual information, divergence, 
    Fisher information) are: 
    - Nonnegativity. 
    - Monotonicity: joint provides more information than marginals. 
    For especially well-behaved ones (entropy, KL, Fisher information) we expect 
    additive decomposition, i.e. chain rule. 
    - Data-processing inequality. 
2. Divergence as a fundamental concept in information theory. 
    a. Quantifies difference between distributions; 
    the choice of this quantification depends on our problem. 
3. **Convexity** of $f$ for $f$-divergence is mathematically equivalent 
    to our information intuitions: data-processing inequality, monotonicity, etc. 
    a. KL-divergence (and Rényi) is special because its $\log$ form 
    admits additive decomposition of joint divergence. 
4. Convexity of information measures correspond to **variational characterizations**, 
    which are extremely useful because: 
    a. They provide bounds: choose the varied quantity to be our friend! 
    b. Provide tractable variational approximations 
    using numerical optimization methods (e.g. VAE, GAN). 
5. Mutual information as information radius, or center of gravity; capacity as a minimax saddle point. 
6. Divergence measures are extremely useful for bounding events: 
    bound the event for a tractable distribution, bound the divergence between the tractable 
    and given distribution, and we can bound the event for the given distribution. 

The foundational results in this book include:

1. Additive decomposition of KL. 
2. <u>Golden formula</u>: variational characterization of mutual information. 
3. <u>Saddle point characterization of mutual information</u>. 
4. <u>Donsker-Varadhan</u> (theorem \@ref(thm:donskerVaradhan)): 
    variational characterization of KL. 
5. Finite-partition approximation theorem for $f$-divergence. 
6. <u>Harremoës-Vadja</u> (theorem \@ref(thm:harremoesVajda)): 
    one theorem to rule them all for $f$-divergence inequalities. 
7. Most $f$-divergences are locally $\chi^2$-like about 
    $\lim_{\lambda \to 0^+} D_f(\lambda P + \bar \lambda Q \| Q)$ 
    and decays quadratically (theorem \@ref(thm:localChiExpansion)). 

Useful proof ideas:

1. Asymptotic analysis proofs: rewrite a difference as an integral, then 
    apply the monotone convergence theorem. 
    - Theorems \@ref(thm:localFisherDiv), \@ref(thm:localChiExpansion). 
2. Unique extremality proofs: reduce the desired inequality to 
    the information inequality (theorem \@ref(thm:infoInequality)) 
    or a nonnegative information quantity (e.g. entropy). 
    - Corollary \@ref(cor:minimalCE), theorem \@ref(thm:gaussianCovExtremality), 
        lemma \@ref(lem:finiteExpectationExtremality); the latter two 
        characterized Gaussian and geometric distributions. 
    - Occasionally we need to introduce a tilted distribution based on 
    both inputs, see Donsker-Varadhan (\@ref(thm:donskerVaradhan)) or 
3. Bounds on conditional quantities: consider the joint quantity 
    and decompose in $2$ different ways. 
    - KL-DPI theorem \@ref(thm:klDPI). 
4. Meta-converse: instead of studying $P_{X\hat X}$ for 
    arbitrary $\hat X$, introduce a tractable $Q_{X\hat X}$, 
    to bound an event, then use DPI (or divergence inequalities like Donsker-Varadhan) 
    to bound the same event for $P_{X\hat X}$ together with some 
    divergence quantity.  
    - Fano's inequality \@ref(thm:simpleFano). 
5. Convexity proofs: introduce an additional parameter 
    $\theta \sim \mrm{Ber}(\lambda)$ to rewrite 
    $\lambda f(x)+\bar \lambda f(y)$ as a conditional quantity, 
    then use decomposition.
    - See section on [convexity](#extremizationConvexity). 
6. Counting bounds: to bound $|\mca C|$, draw an element uniformly from $\mca C$, 
    then upper-bound the joint-entropy by marginal bound, Han's inequality, or chain rule. 
7. Typicality methods (compression): break region into tractable region of 
    large probability and difficult region with low probability. 


## Administrative trivia {-}
Professor email: `yp@mit.edu`. 

Office Hours: LIDS ($6$th floor of $D$-tower, lobby). 

## Motivation {-}

1. Why is training GPT $\iff$ building a great text compressor? 
2. Given $2$ families of distributions 
$P_\theta, \theta\in \Theta_1, \Theta_2$. 
To have $P_{\mrm{err}} \leq \delta$, the number of samples 
is asymptotically 
$\left(\min D(P_{\theta_1}\|P_{\theta_2})\right)^{-1} \log(1/\delta)$ 
3. Noisy channel coding. 
4. Cramer-Rau bound
\[ 
    \min_{\hat \theta} 
    \max_{\theta\in \Theta} \Exp[(\theta - \hat \theta^2]
    \asymp \df 1 {n\min \mca I_F(\theta)}
\] 
5. How many bits to represent a random function / signal / picture 
with fidelity $\epsilon$. 

Key concepts: entropy, KL-divergence, mutual information, 
fisher information, mutual information. 

### Todo list {-}

Mandatory:

1. Gaussian saddle-point. 
2. Variational characterizations of $f$-divergences. 
3. Refine TV and Hellinger. 

Optional: 

1. Entropy-power inequality
2. Sufficient statistic. 