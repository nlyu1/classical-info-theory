# Extremization 

Key perspectives: 

1. _Capacity:_ Given channel $P_{Y|X}$, maximize $I(X; Y)$ 
    over a convex set of inputs $P_X$. 
2. _Rate-distortion:_ Given $P_X$, minimize $I(X; Y)$ 
    over a convex set of $P_{Y|X}$. 
3. _Maximum likelihood:_ Given $P$, minimize $D(P\|Q)$ 
    over a class of $Q$. 
4. _Information projection:_ Given $Q$, minimize $D(P\|Q)$ 
    over a convex class of $P$. 

The first two minimax objectives correspond to the 
concavity of $I(P_X; P_{Y|X})$ in the first argument and 
convexity in the second argument. The last two objectives 
correspond to the convexity of $D(P\|Q)$ in both arguments. 

Key mathematical ideas: 

1. Convexity of $D(P\|Q)$ is equivalent to "conditioning increases 
    divergence" (proposition \@ref(prp:conditionIncDiv)) 
    $\iff$ "mixing decreases divergence." 
2. For convexity proofs, introduce a Bernoulli latent variable, 
    then use chain rule to decompose the quantity (MI, KL, entropy, etc) 
    into the two compared components. 
3. The saddle point of mutual information yields a game-theoretic 
    perspective to the duality between achieving capacity and 
    rate distortion: corollary \@ref(cor:MIMinimax). 
4. The capacity of a channel is the radius of posteriors under 
    divergence. 

## Convexity {-}

:::{.theorem name="KL convexity"}
The map $(P, Q)\mapsto D(P\|Q)$ is convex: 
\[ 
    \forall \lambda \in [0, 1]: 
    \lambda D(P_0 \| Q_0) + \bar \lambda D(P_1\|Q_1) 
    \geq D(\lambda P_0 + \bar \lambda P_1 \| 
    \lambda Q_0 + \bar \lambda Q_1) 
\] 
:::
_Proof:_ The first quantity can be seen as an expected divergence 
over $R\sim \mrm{Ber}(\lambda)$: 
\[ 
    \lambda D(P_0 \| Q_0) + \bar \lambda D(P_1\|Q_1) 
    = D(P\|Q | R), \quad (P, Q)_{R=j} = (P_j, Q_j)
\] 
The second quantity is simply the divergence of the marginal, 
then by chain rule we have 
\begin{align}
    D(P\|Q | R) 
    = D(PR \|QR) \geq D(P\|Q)
\end{align} 


:::{.theorem name="concavity of entropy"}
$P_X\mapsto H(X)$ is concave. Fixing channel 
$P_{Y|X}$, $P_X\mapsto H(X|Y)$ is concave 
and continuous if $\mca X$ finite. 
:::
_Proof:_ The first proof is complete by the 
KL-entropy relation $H(X) = \log |\mca X| - D(P_X\|U_X)$. 
The second proof follows by a similarl latent 
variable argument: consider $U\sim \mrm{Ber}(\lambda)$ and 
$U\to X\to Y$; let $f(P_X) = H(X|Y)$, then 
\begin{align} 
    f(\lambda P_0 + \bar \lambda P_1) = H(X|Y), \quad 
    \lambda f(P_0) + \bar \lambda f(P_1) = H(X|Y, U)
\end{align}
concavity follows from $H(X|Y) \leq H(X|Y, U)$. Continuity 
follows $H(Y|X) = H(Y) - I(X; Y)$ both continuous. 


:::{.theorem name="extremality of MI"}

1. Fixing channel $P_{Y|X}$, $P_X\mapsto I(P_X, P_{Y|X})$ is concave. 
2. Fixing input distribution $P_X$, $P_{Y|X} \mapsto I(P_X, P_{Y|X})$ 
    is convex. 
:::
_Proof_ Consider 3 proofs for the first statement: 
\[ 
    \lambda I(P^0_X; P_{Y|X} \circ P^0_X) +
    \bar \lambda I(P^1_X; P_{Y|X} \circ P^1_X) 
    \geq 
    I(P_X; P_{Y|X}\circ P_X), \quad 
    P_X = \lambda P^0_X + \bar \lambda P^1_X
\] 

1. _Standard latent variable proof:_ Consider the standard 
    latent variable proof $Z\to X\to Y$ with 
    $Z\sim \mrm{Ber}(\lambda)\indep Y$, 
    then the RHS is $I(X; Y)$ while the LHS is 
    \[ 
        I(Y; X, Z) = I(Y; X | Z) + I(Y; Z)_{=0}
    \] 
    the result follows from the fact that mutual information increases 
    with more deta. 
2. _Center of gravity formula:_ 
    Use corollary \@ref(cor:centerOfGravityMI): 
    $I(X; Y) = \min_{Q_Y} D(P_{Y|X} \|Q_Y | P_X)$; this is 
    the pointwise minimum of affine functions in $P_X$ 
    hence concave. 
3. _Golden formula:_ Since 
    $I(X; Y) = D(P_{Y|X} \|Q_Y|P_X) - D(P_Y\|Q_Y)$, 
    the map $P_X\mapsto D(P_{Y|X} \circ P_X \|Q_Y)$ 
    is convex and the first term is affine, so the 
    combination is concave. 

To prove the second argument, note that 
$I(X; Y) = D(P_{Y|X} \| P_Y|P_X)$; here $D(P_{Y|X=x} \| P_Y)$ 
is jointly convex, and $P_Y$ is linear function of $P_{Y|X}$. 


## Minimax and saddle-point {-}

:::{.proposition name="minimax inequality"}
\[ 
    \inf_y \sup_x f(x, y) \geq \sup_x \inf_y f(x, y)
\] 
Whichever operation acts first strictly dominates. 
:::
_Proof:_ Fixing $y=y_0$ on the LHS, 
$\sup_x f(x, y_0) \geq \sup_x \inf_y f(x, y)$ by 
$f(x, y_0)\geq  \inf_y f(x, y)$. 


1. Minimax equality is implied by the existence of a saddle 
    point $(x^*, y^*)$ such that 
    \[ 
        f(x, y^*) \leq f(x^*, y^*) \leq f(x^*, y), \quad \forall x, y
    \] 
    here $x^*$ is the dominant strategy given $y^*$, and 
    $y^*$ is the dominant strategy given $x^*$. 
2. If $\inf, \sup\mapsto \min, \max$, then equality implies 
    the existence of a saddle point. 
3. _von Neumann_: Given $A, B$ with finite alphabets and $g(A, B)$ 
    arbitrary, then 
    \[ 
        \min_{P_A} \max_{P_B} \Exp[g(A, B)] 
        = \max_{P_B} \min_{P_A} \Exp[g(A, B)]
    \] 
    this is a special case of minimax with 
    $f(x, y) = \sum_{ab} P_A(a)P_B(b) g(a, b)$. 

:::{.theorem #minimaxEquality name="minimax theorem"}
If $\mca X, \mca Y$ are compact domains in $\R^n$, and 
$f(x, y)$ is continuous in $(x, y)$, concave in $x$ and 
convex in $y$, then 
\[ 
    \max_x \min_y f(x, y) = \min_y \max_x f(x, y)
\] 
In particular, this implies the existence of a saddle point. 
:::

## Capacity, Saddle point of MI {-}

:::{.definition name="capacity, caid, caod"}
The capacity of a channel $P_{Y|X}$ over a set $\mca P$ 
of (usually convex) input distributions is 
\[ 
    C = \sup_{P_X\in \mca P} I(P_X, P_{Y|X}) 
    = \sup_{P_X} D(P_{Y|X}P_X \| (P_{Y|X}\circ P_X)P_X)
\] 
If equality is saturated by $P_X^*\in\mca P$, then 
$P_X^*$ is a capacity-achieving input distribution 
(caid) and $P_Y^* = P_{Y|X}\circ P_X^*$ is the capacity-achieving 
output distribution (caod). 
:::

:::{.theorem #MISaddleInequality name="implications of MI saddle point"}
Fixing a convex $\mca P$ on $\mca X$. The existence of a caid 
implies, for every $P_X\in \mca P, P_Y\in P_{Y|X} \circ \mca P$, 
\[ 
    D(P_{Y|X} \| P_Y^* |P_X) \leq D(P_{Y|X} \| P_Y^* | P_X^*) 
    \leq D(P_{Y|X} \| Q_Y | P_X^*)
\] 
:::
_Proof:_ The second inequality is simply center of gravity 
formula \@ref(cor:centerOfGravityMI) applied to the 
middle quantity $C=I(X^*, Y^*) = D(P_{Y|X} \| P_Y^* | P_X^*)$. 
For the first inequality, let $C<\infty$ and let 
\[ 
    P_{X_\lambda} = \lambda P_X + \bar \lambda P_X^*, \quad 
    P_{Y_\lambda} = P_{Y|X} \circ P_{X_\lambda} \implies 
    P_{Y_\lambda} = \lambda P_Y + \bar \lambda P_Y^* 
\] 
The following chain yields (the third line applies the 
center of gravity characterization again) 
\begin{align}
    C 
    &\geq I(X_\lambda; Y_\lambda) 
    = D(P_{Y|X} \| P_{Y_\lambda} | P_{X_\lambda}) \\ 
    &= \lambda D(P_{Y|X} \| P_{Y_\lambda} | P_X) + 
    \bar \lambda D(P_{Y|X} \| P_{Y_\lambda} | P_{X^*}) \\ 
    &\geq \lambda D(P_{Y|X} \| P_{Y_\lambda} | P_X) + \bar \lambda C \\ 
    C &\geq D(P_{Y|X} \| P_{Y_\lambda} | P_X) 
    = D(P_{Y|X} P_X \| P_{Y_\lambda} P_X) 
\end{align}
Apply lower semi-continuity \@ref(thm:semiContinuity) 
by taking $\liminf_{\lambda\to 0}$ to obtain the desired quantity 
\[ 
    \liminf_{\lambda\to 0} P_{Y_\lambda} P_X = P_{Y^*}P_X \implies 
    D(P_{Y|X} \| P_{Y^*} | P_X) \leq 
    \liminf_{\lambda\to 0} D(P_{Y|X} P_X \| P_{Y_\lambda} P_X)  \leq C 
\] 

:::{.corollary #caodUniqueness name="uniqueness of caod"}
In addition to the assumptions in theorem \@ref(thm:MISaddleInequality), 
if capacity is finite, then the caod $P_Y^*$ is unique and satisfies 
\[ 
    D(P_{Y|X} \circ P_X \| P_Y^*) \leq C < \infty, 
    \quad \forall P_X\in \mca P 
\] 
In particular, KL-finite implies $P_Y \ll P_Y^*$. 
:::
_Proof:_ Recognize the capacity as a decomposed component 
in the divergence chain rule \@ref(prp:divergenceChainRule):
\begin{align}
    C = D(P_{X^*Y^*} \| P_{Y^*}P_{X^*})
    &= D(P_{X^*Y^*} \| P_{Y^*} P_X) - D(P_{Y^*} \| P_Y) \\ 
    &= D(P_{X|Y} \| P_X | P_{Y^*}) - D(P_{Y^*} \| P_Y) \\ 
    &\geq D(P_{X|Y} \| P_{X^*} | P_{Y^*})_{=C} - D(P_{Y^*} \| P_Y)
\end{align}
Saturated equality implies $D(P_{Y^*} \| P_Y)=0\iff P_Y=P_{Y^*}$. 

Note that the caid need not be unique.  

:::{.corollary #MIMinimax name="minimax MI"}
Under saddle point assumptions, we additionally have 
\begin{align}
    C = \max_{P_X\in \mca P} I(X; Y) 
    &= \max_{P_X\in \mca P} \min_{Q_Y} D(P_{Y|X} \| Q_Y | P_X) \\ 
    &= \min_{Q_Y} \sup_{P_X\in \mca P} D(P_{Y|X} \| Q_Y|P_X) 
\end{align}
:::
_Proof:_ The first $\max\min$ equation comes from the 
center of gravity characterization \@ref(cor:centerOfGravityMI). 
The second equation comes from applying center of gravity 
to the left inequality of theorem \@ref(thm:MISaddleInequality) 
\[ 
    C = D(P_{Y|X} \| P_Y^* | P_X^*) 
    = \max_{P_X\in \mca P} D(P_{Y|X} \| P_Y^* | P_X) 
    = \min_{Q_Y} \sup_{P_X\in \mca P} D(P_{Y|X} \| P_Y^* | P_X) 
\] 

## Capacity as information radius {-}

Some review: given metric space $(X, d)$ and bounded set $A$. 

:::{.definition name="radius, diameter"}
The (Chebyshev) radius is the radius of the smallest ball 
that covers $A$. 
\[ 
    \mrm{rad}(A) = \inf_{y\in X}\sup_{x\in A} d(x, y)
\] 
The diameter of $A$ is the least upper bound on 
two distances in the set 
\[ 
    \mrm{diam}(A) = \sup_{x, y\in A} d(x, y)
\] 
:::

<span style="color:green">
Compare the radius to the second equation in corollary 
\@ref(cor:MIMinimax): channel capacity is the information 
radius of the conditionals. 
</span>

:::{.corollary}
Given finite $\mca X$ and channel $P_{Y|X}$, the 
maximal mutual information over all input distributions 
satisfies 
\[ 
    \max_{P_X} I(P_X; P_{Y|X}) 
    = \max_{x\in \mca X} D(P_{Y|x=x} \| P_{Y^*})
\] 
:::
_Proof:_ This follows from the left equality 
$C=\max_{P_X\in \mca P} D(P_{Y|X} \| P_Y^* | P_X)$: for finite 
alphabets, concentrate weight in the single $x\in \mca X$ 
which maximizes $D(P_{Y|X=x} \| P_Y^*)$. 

## Gaussian saddle point {-}

:::{.theorem name="extremality of Gaussian channels"}
Let $X_g\sim \mca N(0, \sigma_X^2), 
N_g\sim \mca N(0, \sigma_N^2)\indep X_g$, then 

1. _Gaussian capacity:_ 
\[ 
    C = I(X_g; X_g + N_g) = \df 1 2 \log \left(
        1 + \df{\sigma_X^2}{\sigma_N^2}
    \right)
\] 
2. _Gaussian input is the caid for Gaussian noise:_ under 
    the power constraint $\Var(X) \leq \sigma_X^2$ and $X\indep N_g$ 
\[ 
    I(X; X+N_g) \leq I(X_g; X_g+N_g) 
\] 
with equality saturated iff $X=X_g$ (in distribution). 
3. _Gaussian noise is the worst for Gaussian input:_ under 
    the power constraint $\Exp[N^2] \leq \sigma_N^2$ and $\Exp[X_gN] = 0$ 
\[ 
    I(X_g; X_g+N) \geq I(X_g; X_g+N_g) 
\] 
with equality iff $N=N_g$ (in distribution and $N\indep X_g$. 
:::
_Proof:_ Placeholder. 